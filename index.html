<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="zhengchenyu">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<meta name="generator" content="Hexo 8.1.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-ErasuceCode算法实现" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/02/07/ErasuceCode%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/" class="article-date">
  <time class="dt-published" datetime="2025-02-07T08:06:18.198Z" itemprop="datePublished">2025-02-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <blockquote>
<p>本文侧重于如何从依据理论指导实践，在不适用任何第三方库的情况下实现ErasuceCode算法。本文仅会对用到的理论做简单的解释并标记引用，不会做详细解释。因此阅读本文前请熟读文献[1]第四章内容。<br>本文代码存储于<a target="_blank" rel="noopener" href="https://github.com/zhengchenyu/SimpleErasureCode">https://github.com/zhengchenyu/SimpleErasureCode</a>, SimpleErasureCode只是为了便于方便理解，为了保持与文章保持一致，因此没有优化。部分关键流程文章中会有链接到对应的代码。</p>
</blockquote>
<p>EC算法在存储领域和通信领域都有广泛的应用。在分布式存储领域，为了避免机器宕机，需要存储3份冗余副本，这3台机器最多允许2台宕机。如果使用EC RS-6-3的话，可以实现使用9个副本冗余6份数据，这9台机器最多允许3台宕机，而且存储量却减少了一半。分布式存储使用EC在几乎不降低可用性的前提下，降低了冗余副本数，大大节约了存储资源。</p>
<p>实现EC算法是检验是否理解EC算法的重要手段。对EC算法的理解对工程上也有很大的帮助，笔者实现EC算法的初衷也是为了解决HADOOP-19180提出的问题。</p>
<h1 id="1-算法概述"><a href="#1-算法概述" class="headerlink" title="1 算法概述"></a>1 算法概述</h1><p>本文以RS-6-3算法为例, 存在6个数据块和3个校验块。EC算法的两个基本问题:</p>
<ul>
<li>如何通过数据块生成校验块?</li>
<li>如何通过部分数据块和校验块恢复丢失的数据块?</li>
</ul>
<h2 id="1-1-生成校验块"><a href="#1-1-生成校验块" class="headerlink" title="1.1 生成校验块"></a>1.1 生成校验块</h2><p>生成校验块即编码过程(<a target="_blank" rel="noopener" href="https://github.com/zhengchenyu/SimpleErasureCode/blob/24be76083a0c3172f1d2fe7af8e1ad972935657f/src/main/java/zcy/ec/coder/ErasureCoder.java#L29">encode</a>)。对6个数据块依次取出byte, 即d<sub>0</sub>,d<sub>1</sub>,d<sub>2</sub>,d<sub>3</sub>,d<sub>4</sub>,d<sub>5</sub>。如下图所示，用编码矩阵(<a target="_blank" rel="noopener" href="https://github.com/zhengchenyu/SimpleErasureCode/blob/24be76083a0c3172f1d2fe7af8e1ad972935657f/src/main/java/zcy/ec/coder/ErasureCoder.java#L14C24-L14C36">encodeMatrix</a>)乘以对应的数据块,即得到原有的数据d<sub>0</sub>,d<sub>1</sub>,d<sub>2</sub>,d<sub>3</sub>,d<sub>4</sub>,d<sub>5</sub>和对应的校验字节c<sub>0</sub>,c<sub>1</sub>,c<sub>2</sub>(<a target="_blank" rel="noopener" href="https://github.com/zhengchenyu/SimpleErasureCode/blob/24be76083a0c3172f1d2fe7af8e1ad972935657f/src/main/java/zcy/ec/coder/ErasureCoder.java#L50">编码过程</a>)。对数据块的所有字节依次执行上述的操作就得到了校验块。</p>
<img src="/images/ec/ec编码矩阵.png" width=50% height=50% text-align=center/>

<h2 id="1-2-恢复数据块"><a href="#1-2-恢复数据块" class="headerlink" title="1.2 恢复数据块"></a>1.2 恢复数据块</h2><p>恢复数据块的过程即解码过程(<a target="_blank" rel="noopener" href="https://github.com/zhengchenyu/SimpleErasureCode/blob/24be76083a0c3172f1d2fe7af8e1ad972935657f/src/main/java/zcy/ec/coder/ErasureCoder.java#L65">decode</a>)。假如d2,d3,d4丢失了，我们需要通过d<sub>0</sub>,d<sub>1</sub>,d<sub>5</sub>,c<sub>0</sub>,c<sub>1</sub>,c<sub>2</sub>恢复d<sub>2</sub>,d<sub>3</sub>,d<sub>4</sub>。在上面公式中删除对应的行，公式中用d<sub>2?</sub>,d<sub>3?</sub>,d<sub>4?</sub>表示数据块丢失。有如下公式:</p>
<img src="/images/ec/ec解码过程1.png" width=50% height=50% text-align=center/>

<p>对上面的公式两侧都乘以裁剪后的矩阵的逆矩阵，这个逆矩阵即解码矩阵(<a target="_blank" rel="noopener" href="https://github.com/zhengchenyu/SimpleErasureCode/blob/24be76083a0c3172f1d2fe7af8e1ad972935657f/src/main/java/zcy/ec/coder/ErasureCoder.java#L117C18-L117C30">decodeMatrix</a>)。可以得到如下的公式:</p>
<img src="/images/ec/ec解码过程2.png" width=50% height=50% text-align=center/>

<p>得到解码矩阵后其实计算过程与编码类似，只是输入为为d<sub>0</sub>,d<sub>1</sub>,d<sub>5</sub>,c<sub>0</sub>,c<sub>1</sub>,c<sub>2</sub>，输出为d<sub>0</sub>,d<sub>1</sub>,d<sub>2</sub>,d<sub>3</sub>,d<sub>4</sub>,d<sub>5</sub>。</p>
<h1 id="2-关于矩阵"><a href="#2-关于矩阵" class="headerlink" title="2 关于矩阵"></a>2 关于矩阵</h1><p>上一章节已经介绍了EC编解码的基本过程，但工程实现上让然会有一些问题需要解决。这一小节主要介绍一下关于矩阵方面的问题。</p>
<h2 id="2-1-如何选择矩阵"><a href="#2-1-如何选择矩阵" class="headerlink" title="2.1 如何选择矩阵"></a>2.1 如何选择矩阵</h2><p>根据第一小节的分析，编码矩阵是一个9 * 6的矩阵。而解码矩阵是在编码矩阵的基础上删除任意三行，然后再求逆的。因此我们定义编码矩阵的时候要保证，对于这个9 * 6的编码矩阵，任意删除三行得到的6 * 6的矩阵是一个可逆矩阵。<br>为了便于计算，编码矩阵的上半部分使用的是6 * 6 的单位矩阵，单位矩阵是可逆的，是满足条件的。接下来就是给下半部分的3 * 6的找到合理的矩阵。本文使用的是范德蒙矩阵。</p>
<img src="/images/ec/ec范德蒙矩阵.png" width=50% height=50% text-align=center/>

<blockquote>
<p>由于MathType不支持新版Mac, 而且markdown经常因为调整不好乱码，这里的共识还是贴图吧…</p>
</blockquote>
<p>如下为范德蒙矩阵的行列式:</p>
<img src="/images/ec/ec范德蒙矩阵行列式.png" width=50% height=50% text-align=center/>

<p>只要ai各不相等且不为0，则范德蒙矩阵一定可逆，意味着任意挑出3行向量，一定是线性无关的。那我们就挑出前三行，令a<sub>1</sub>&#x3D;1,a<sub>2</sub>&#x3D;2,a<sub>3</sub>&#x3D;3，便可以得到如下编码矩阵:</p>
<img src="/images/ec/ec真实编码矩阵.png" width=50% height=50% text-align=center/>

<p>前面已经说明了前6行向量是互为线性无关，后3行向量也是互为线性无关的。如果在前6行中选取n行，在后三行中去6-n行，那么这6行向量还是线性无关的吗？假设n为5，由于后三行的没有元素为0，因此肯定是缺一个维度来保证线性相关。如果n为4和3也是同样的道理。因此这个9 * 6的矩阵随意挑出6行组成的6 * 6阶矩阵一定是可逆的。</p>
<h2 id="2-2-高斯消元求逆"><a href="#2-2-高斯消元求逆" class="headerlink" title="2.2 高斯消元求逆"></a>2.2 高斯消元求逆</h2><p>本文使用了容易理解的高斯消元法求逆(<a target="_blank" rel="noopener" href="https://github.com/zhengchenyu/SimpleErasureCode/blob/24be76083a0c3172f1d2fe7af8e1ad972935657f/src/main/java/zcy/ec/coder/math/Matrix.java#L73">inverse</a>)。假设当前6 * 6矩阵为A,  在右侧再拼接6 * 6的单位矩阵E，得到矩阵[A | E]。如果使用A<sup>-1</sup>乘以这个矩阵，会得到[E|A<sup>-1</sup>]。这样我们只需将A矩阵转化为单元矩阵，E矩阵自然就变成了A<sup>-1</sup>。(<a target="_blank" rel="noopener" href="https://github.com/zhengchenyu/SimpleErasureCode/blob/24be76083a0c3172f1d2fe7af8e1ad972935657f/src/main/java/zcy/ec/coder/math/Matrix.java#L73C24-L73C31">高斯消元求逆</a>)<br>对每一行行依次执行如下的过程:</p>
<ul>
<li>(1) 对第i行, 找到第i行第i列的值。(<a target="_blank" rel="noopener" href="https://github.com/zhengchenyu/SimpleErasureCode/blob/24be76083a0c3172f1d2fe7af8e1ad972935657f/src/main/java/zcy/ec/coder/math/Matrix.java#L120">Step1</a>)</li>
<li>(2) 然后计算该值的乘法逆元。然后让该行的每个元素均乘以这个乘法逆元。(<a target="_blank" rel="noopener" href="https://github.com/zhengchenyu/SimpleErasureCode/blob/24be76083a0c3172f1d2fe7af8e1ad972935657f/src/main/java/zcy/ec/coder/math/Matrix.java#L126">Step2</a>)</li>
<li>(3) 对其他行j，使用第i行经过线性变换将地地j行第i列的值消为0。</li>
</ul>
<p>经过这三步变换为，对于第i列，有且只有地i行的数据为1，其他均为0。对每一行均执行如下操作，左侧变得到了单位矩阵，右侧的结果也即A<sup>-1</sup>。</p>
<blockquote>
<p>对于有理数域中，乘以乘法逆元即除法，加上加法逆元即减法。这样描述对伽罗华域中的数学运算的描述更准确。</p>
</blockquote>
<p>对于第0行计算的过程如下，其他行同理。</p>
<img src="/images/ec/ec高斯消元处理第0行.png" width=50% height=50% text-align=center/>

<p>还有一种特殊的情况，譬如上述矩阵如果处理第2行。第2行和第2列的元素值为0，0是没有乘法逆元的。所以这时候就需要找到第2行下面的行中第2列不为0的情况，把该行加到第2行上，这样可以保证算法可运行(<a target="_blank" rel="noopener" href="https://github.com/zhengchenyu/SimpleErasureCode/blob/24be76083a0c3172f1d2fe7af8e1ad972935657f/src/main/java/zcy/ec/coder/math/Matrix.java#L105">setup for step1</a>)。其实移位可能更高效，但是为了便于理解，采用这样的方式。</p>
<h1 id="3-伽罗华域"><a href="#3-伽罗华域" class="headerlink" title="3 伽罗华域"></a>3 伽罗华域</h1><p>矩阵运算还存在一个问题，数据都是byte存储的，是一个0-255的数值。经过有理数运算后，数值是很容易越界的。因此我们需要一套新的计算体系同时满足以下两个条件:</p>
<ul>
<li>该算法体系的值和计算得到的值只能是在有限的集合范围内，即不越界。</li>
<li>可以通过有限的运算恢复得到原有值，即可解码。</li>
</ul>
<p>这就需要使用伽罗华有限域。伽罗华有限域可以保证任何计算得到的结果均在有限集合内，这满足了不越界的要求。另外伽罗华有限域的运算都有对应的逆运算。譬如对a，如果a乘以b，再乘以b的加法逆元，结果仍然是a。这满足了可解码的要求。</p>
<p>这里为了便于理解先简单的介绍GF(7)，然后再解释实际使用的GF(2<sup>8</sup>)。</p>
<blockquote>
<p>本章并没有详细的证明。譬如将7扩张到无穷大的质数的时候，如何证明相应的定理为什么成立。事实上笔者也不知道，参考文献上也没有严谨的证明。也只是简单的说明了合数不能作为模数。但对于工程上有限的集合内，很容易通过穷举法来证明算法的可行性。</p>
</blockquote>
<h2 id="3-1-GF-7"><a href="#3-1-GF-7" class="headerlink" title="3.1  GF(7)"></a>3.1  GF(7)</h2><p>首先给出如下定义:</p>
<ul>
<li>加法逆元: 给定x，如果存在x’，使得x+x’&#x3D;x’+x&#x3D;0，则称x’是x的加法逆元。</li>
<li>乘法逆元: 给定x，如果存在x’，使得x * x’&#x3D;x’ * x&#x3D;e，则称x’为x的乘法逆元。其中e为该群的单位元。对于GF(7)，e为1。</li>
</ul>
<p>我们定义一种新的运算，即模7运算。对于加法和乘法运算，我们会将结果然后模7。譬如, 8 + 3 &#x3D; 11 mod 7 &#x3D; 4。8 * 3 &#x3D; 24 mod  7 &#x3D; 3。<br>对于加法逆元和乘法逆元，根据定义我们可以穷举加法和乘法计算，根据结果得到对应的逆元。譬如, 4 + 3 &#x3D; 7 mod 7 &#x3D; 0，我们就说4和3互为加法逆元。2 * 4 &#x3D; 8 mod 7 &#x3D; 1，我们就说2和4互为乘法逆元。<br>下表穷举了GF7的所有加法和乘法运算，根据码表也得到了GF(7)下所有元素对应的加法逆元和乘法逆元。</p>
<img src="/images/ec/ecgf7table.png" width=50% height=50% text-align=center/>

<p>来看下是否满足我们的要求，不越界在定义上就满足了。可解码的特点，我们随机指定数组假设为5，用5乘以3在乘以3的乘法逆元5, 得到的值为5 * 3 * 5 &#x3D; 75 mod 1 &#x3D; 5。事实上根据乘法交换律很容易证明这个。对于加法逆元的解码同理。</p>
<h2 id="3-2-GF-28"><a href="#3-2-GF-28" class="headerlink" title="3.2 GF(28)"></a>3.2 GF(2<sup>8</sup>)</h2><h3 id="3-2-1-基本原理"><a href="#3-2-1-基本原理" class="headerlink" title="3.2.1 基本原理"></a>3.2.1 基本原理</h3><p>实际的数字存储的值是0-255, 那么是否意味着我们可以直接使用GF(256)呢? 答案是不可以的。因为256是合数，譬如16 * 16 &#x3D; 256 mod 256 &#x3D; 0，那么16就不存在乘法逆元，也就难以进行边界吗。<br>因此需要引入多项式的运算，且要求同指数幂下遵循GF(2)。模数为不可约多项式。对于不可约多项式a, 无法找到两个不为1的多项式b和c使得b * c &#x3D; a。可以使用穷举法得到每个多项式的加法逆元和乘法逆元。事实上只要集合内的每个元素都有1对1对应的乘法逆元，就可以满足我们的要求。文献1的表4.6也通过穷举证明了GF(2<sup>3</sup>)的有效性。<br>多项式的计算与对byte的编码有什么关系呢？由于多项式的同指数幂是GF(2)，也就意味着对于多项式a<sub>0</sub> + a<sub>1</sub>x + a<sub>2</sub>x<sup>2</sup> + a<sub>3</sub>x<sup>3</sup> + a<sub>4</sub>x<sup>4</sup> + a<sub>5</sub>x<sup>5</sup>  +  a<sub>6</sub>x<sup>6</sup> +  a<sub>7</sub>x<sup>7</sup> ，a<sub>i</sub> 为0和1。如果a<sub>0</sub>为第0位，a<sub>1</sub>为第1位，依次类推，这组系数就是一个byte。这样就把要存储的byte与多项式运算结合了。</p>
<h3 id="3-2-2-计算"><a href="#3-2-2-计算" class="headerlink" title="3.2.2 计算"></a>3.2.2 计算</h3><p>本章主要介绍如何计算GF(2<sup>8</sup>)。对于加法和乘法，我们直接使用多项式运算。对于加法逆元和乘法逆元，我们穷举加法和乘法运算，然后通过码表来得到加法和乘法逆元。<br>本文的算法的不可约多项式为x<sup>8</sup> + x<sup>4</sup>+ x<sup>3</sup> + 1。<br>f(x) &#x3D; x<sup>6</sup> + x<sup>4</sup>+ x<sup>2</sup> + x + 1, g(x) &#x3D;  x<sup>7</sup> + x + 1<br>对于加法, 指数幂执行GF(2)运算，G(2)运算实际上就是异或。得到f(x)+g(x)&#x3D; x<sup>7</sup> + x<sup>6</sup>  +x<sup>4</sup>+ x<sup>2</sup> + 1。实际可以理解为f(x)对应的二进制0b01010111与g(x)对应的二进制0b10000011进行按位的异或计算。(<a target="_blank" rel="noopener" href="https://github.com/zhengchenyu/SimpleErasureCode/blob/24be76083a0c3172f1d2fe7af8e1ad972935657f/src/main/java/zcy/ec/coder/operator/GaloisFieldComputation.java#L101">GF(2<sup>8</sup>)</a>)<br>对于乘法, 先考虑h(x) &#x3D; a<sub>0</sub> + a<sub>1</sub>x + a<sub>2</sub>x<sup>2</sup> + a<sub>3</sub>x<sup>3</sup> + a<sub>4</sub>x<sup>4</sup> + a<sub>5</sub>x<sup>5</sup>  +  a<sub>6</sub>x<sup>6</sup> +  a<sub>7</sub>x<sup>7</sup>, 计算 h(x) * x &#x3D; (a<sub>0</sub>x + a<sub>1</sub>x<sup>2</sup> + a<sub>2</sub>x<sup>3</sup> + a<sub>3</sub>x<sup>4</sup> + a<sub>4</sub>x<sup>5</sup> + a<sub>5</sub>x<sup>6</sup>  +  a<sub>6</sub>x<sup>7</sup> +  a<sub>7</sub>x<sup>8</sup> ) mod (x<sup>8</sup> + x<sup>4</sup>+ x<sup>3</sup> + x + 1)。存在两种情况</p>
<ul>
<li>(1) a<sub>7</sub> 等于 0<br>那么该式已经不可约，因此h(x) * x&#x3D; a<sub>0</sub>x + a<sub>1</sub>x<sup>2</sup> + a<sub>2</sub>x<sup>3</sup> + a<sub>3</sub>x<sup>4</sup> + a<sub>4</sub>x<sup>5</sup> + a<sub>5</sub>x<sup>6</sup>  +  a<sub>6</sub>x<sup>7</sup>。也就意味着[a<sub>7</sub>a<sub>6</sub>a<sub>5</sub>a<sub>4</sub>a<sub>3</sub>a<sub>2</sub>a<sub>1</sub>a<sub>0</sub>] * [00000010] &#x3D; [a<sub>6</sub>a<sub>5</sub>a<sub>4</sub>a<sub>3</sub>a<sub>2</sub>a<sub>1</sub>a<sub>0</sub>0]，即向左移动一位。(<a target="_blank" rel="noopener" href="https://github.com/zhengchenyu/SimpleErasureCode/blob/24be76083a0c3172f1d2fe7af8e1ad972935657f/src/main/java/zcy/ec/coder/operator/GaloisFieldComputation.java#L85">a<sub>7</sub> 等于 0</a>)</li>
<li>(1) a<sub>7</sub> 不等于0<br>h(x) * x &#x3D; (a<sub>0</sub>x + a<sub>1</sub>x<sup>2</sup> + a<sub>2</sub>x<sup>3</sup> + a<sub>3</sub>x<sup>4</sup> + a<sub>4</sub>x<sup>5</sup> + a<sub>5</sub>x<sup>6</sup>  +  a<sub>6</sub>x<sup>7</sup>  + x<sup>8</sup>) mod (x<sup>8</sup> + x<sup>4</sup>+ x<sup>3</sup> +x+ 1) &#x3D; a<sub>0</sub>x + a<sub>1</sub>x<sup>2</sup> + a<sub>2</sub>x<sup>3</sup> + a<sub>3</sub>x<sup>4</sup> + a<sub>4</sub>x<sup>5</sup> + a<sub>5</sub>x<sup>6</sup>  +  a<sub>6</sub>x<sup>7</sup>  + x<sup>8</sup> - x<sup>8</sup> -  x<sup>4</sup> -  x<sup>3</sup> -1 &#x3D; 1 + (a<sub>0</sub>+1)x + a<sub>1</sub>x<sup>2</sup> + (a<sub>2</sub>+1)x<sup>3</sup> + (a<sub>3</sub>+1)x<sup>4</sup> + a<sub>4</sub>x<sup>5</sup> + a<sub>5</sub>x<sup>6</sup>  +  a<sub>6</sub>x<sup>7</sup>。以为意味着[a<sub>7</sub>a<sub>6</sub>a<sub>5</sub>a<sub>4</sub>a<sub>3</sub>a<sub>2</sub>a<sub>1</sub>a<sub>0</sub>] * [00000010] &#x3D; [a<sub>6</sub>a<sub>5</sub>a<sub>4</sub>a<sub>3</sub>a<sub>2</sub>a<sub>1</sub>a<sub>0</sub>0] ^ [00011011]。即左移一位后，在于不包含最高位系统的字节进行异或。(<a target="_blank" rel="noopener" href="https://github.com/zhengchenyu/SimpleErasureCode/blob/24be76083a0c3172f1d2fe7af8e1ad972935657f/src/main/java/zcy/ec/coder/operator/GaloisFieldComputation.java#L88">a<sub>7</sub> 不等于 0</a>)</li>
</ul>
<blockquote>
<p>这里减法为加法逆元，对于GF(2)，加法逆元就是自己。</p>
</blockquote>
<p>f(x) * g(x) &#x3D; ((x<sup>6</sup> + x<sup>4</sup>+ x<sup>2</sup> + x + 1) * ( x<sup>7</sup> + x + 1))。我们可以把他分解为一下三个运算之和。</p>
<ul>
<li>(1)  (x<sup>6</sup> + x<sup>4</sup>+ x<sup>2</sup> + x + 1) * x<sup>7</sup></li>
<li>(2) (x<sup>6</sup> + x<sup>4</sup>+ x<sup>2</sup> + x + 1) * x</li>
<li>(3) (x<sup>6</sup> + x<sup>4</sup>+ x<sup>2</sup> + x + 1) * 1<br>对于(2)在前面已经介绍了，对于(3)实质上不用计算。对于(1), 我们可以使用(2)的计算方法进行递归计算(<a target="_blank" rel="noopener" href="https://github.com/zhengchenyu/SimpleErasureCode/blob/24be76083a0c3172f1d2fe7af8e1ad972935657f/src/main/java/zcy/ec/coder/operator/GaloisFieldComputation.java#L73">fxxn</a>)。然后分别计算的三个值进行相加便得到了最终的结果(<a target="_blank" rel="noopener" href="https://github.com/zhengchenyu/SimpleErasureCode/blob/24be76083a0c3172f1d2fe7af8e1ad972935657f/src/main/java/zcy/ec/coder/operator/GaloisFieldComputation.java#L63">_mul</a>)。</li>
</ul>
<p>至此，ErasureCode算法的所有问题都已经得到了解释。本文以自顶向下的方式描述了如何从理论到实践来实现ErasureCode算法。完整的代码见<a target="_blank" rel="noopener" href="https://github.com/zhengchenyu/SimpleErasureCode">SimpleErasureCode</a>。</p>
<p>参考文献:</p>
<ul>
<li>[1]William Stallings. 密码编码学与网络安全 原理与实践 第六版[M] 第四章</li>
<li>[2]<a target="_blank" rel="noopener" href="https://drmingdrmer.github.io/tech/distributed/2017/02/01/ec.html">https://drmingdrmer.github.io/tech/distributed/2017/02/01/ec.html</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/02/07/ErasuceCode%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/" data-id="cuidOXba6tP7aVxbRj-Hrpsbd" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-RSS-远程Merge的设计" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/12/25/RSS-%E8%BF%9C%E7%A8%8BMerge%E7%9A%84%E8%AE%BE%E8%AE%A1/" class="article-date">
  <time class="dt-published" datetime="2023-12-25T10:58:15.000Z" itemprop="datePublished">2023-12-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/12/25/RSS-%E8%BF%9C%E7%A8%8BMerge%E7%9A%84%E8%AE%BE%E8%AE%A1/">RSS-远程Merge的设计</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="1-默认的shuffle"><a href="#1-默认的shuffle" class="headerlink" title="1 默认的shuffle"></a>1 默认的shuffle</h1><blockquote>
<p>注: 第一节简单介绍了主流框架默认的shuffle的原理，目的为了那里会使用本地磁盘，从而设计远程Merge。如果足够了解，可以忽略这部分内容。</p>
</blockquote>
<p>我们依次对MapReduce, Tez, Spark的shuffle进行分析。</p>
<h2 id="1-1-MapReduce"><a href="#1-1-MapReduce" class="headerlink" title="1.1 MapReduce"></a>1.1 MapReduce</h2><p>Map将Record写入到内存，当内存超过阈值的时候，会将内存的数据spill到磁盘文件中，按照partitionid+key的顺序依次将Record写入到磁盘文件。当Map处理完所有Record后, 会将当前内存中的数据spill到磁盘文件。然后读取所有的spill到磁盘的文件，并按照partitionid+key的顺序进行merge，得到排序的Records。</p>
<blockquote>
<p>注: 按照partitionid排序的目的是Reduce端从Map端获取的数据的时候，尽可能顺序读。对于MR、Tez、Spark, 无论是否排序，只要有分区，都需要按照partitionid进行排序。</p>
</blockquote>
<p>Reduce端会从Map端以远程或本地的方式拉取对应分区的Records，称之为MapOutput。正常情况下会直接使用内存，如果内存超过阈值会将这些Records写入到磁盘。然后Reduce端会对所有MapOutput使用最小堆K路归并排序进行一些Merge操作，得到全局排序的Reccords。Merge的过程中，可能会因为内存超过阈值，会将临时的结果spill到磁盘。另外如果spill到磁盘的文件过多，也会触发额外的merge。</p>
<h2 id="1-2-Tez"><a href="#1-2-Tez" class="headerlink" title="1.2 Tez"></a>1.2 Tez</h2><p>Tez的情况略微复杂。Tez分为ordered io和unordered io。</p>
<p>ordered io与MapReduce相同，不再展开分析。</p>
<p>unordered io一般用于hashjoin等不需要key进行排序的情况。非排序的io采用来之即用方案。Map直接将Record写入文件或者通过缓存在写入文件。Reduce端也是读数据的时候也是读之即用。</p>
<h2 id="1-3-Spark"><a href="#1-3-Spark" class="headerlink" title="1.3 Spark"></a>1.3 Spark</h2><p>spark的shuffle更复杂，也更合理。部分任务是不需要sort和combine的，因此spark用户可以需求决定shuffle的逻辑。</p>
<h3 id="1-3-1-Shuffle写操作"><a href="#1-3-1-Shuffle写操作" class="headerlink" title="1.3.1 Shuffle写操作"></a>1.3.1 Shuffle写操作</h3><p>写shuffle数据的时候，支持三种writer:</p>
<ul>
<li>(1) BypassMergeSortShuffleWriter</li>
</ul>
<p>为每个partition都生成一个临时文件。在写Record的时候，找到对应的分区直接写入对应的临时文件。然后当所有数据都处理完成后，将这些临时的文件按照分区的顺序依次写入到一个最终的文件，并删除临时文件。</p>
<ul>
<li>(2) UnsafeShuffleWriter</li>
</ul>
<p>UnsafeShuffleWriter主要通过ShuffleExternalSorter来实现具体的逻辑。当写Record的时候，直接序列化操作，并将序列化的字节拷贝到申请的内存中。同时也会将Record的地址和分区记录到内存中(inMemSorter)。</p>
<p>当内存的Record达到阈值的时候，会进行spill操作。根据内存(inMemSorter)中的信息，我们很容易得到一个按照分区排序的Record，并写到文件中。</p>
<p>当处理完所有Record，我们会把当前内存中的Records spill到文件中。最后对所有spilled文件进行一次聚合。由于之前spilled的文件已经是按照分区排序的，所以我们可以按照分区的顺序依次将所有spill的文件的对应自己拷贝到最终文件。这样得到的最终文件即为分区排序的文件。</p>
<ul>
<li>(3) SortShuffleWriter</li>
</ul>
<p>SortShuffleWriter主要通过ExternalSorter来实现具体逻辑。ExternalSorter根据用户的需求决定是否combine和sort。</p>
<p>当写Record的时候，会直接插入到内存中。如果需要combine，内存架构是map，否则是buffer。</p>
<p>如果当前评估内存大于阈值会触发spill操作。spill操作的时候，会将Record，然后spill到磁盘。这个过程是需要进行排序的。而具体的比较器会根据用户需求的不同使用不同的值。如果设置了keyordering会按照key进行排序。如果没有设置keyordering，但是设置了aggregator(即combine)，则按照key的hashcode进行排序，这样保证相同的key组织一起，便于combine操作。如果keyordering和aggregator都没有设置，则会按照partiton进行排序。</p>
<p>所有Record都写完时，需要读取spill的文件，并合并成一个全局有序的文件。</p>
<p>三种writer的比较</p>
<table>
<thead>
<tr>
<th>writer</th>
<th>优点</th>
<th>缺点</th>
<th>场景</th>
</tr>
</thead>
<tbody><tr>
<td>BypassMergeSortShuffleWriter</td>
<td>(1) 只经过一次序列化。<br>(2) 采用类hashmap的数据结构，插入数据快。</td>
<td>(1) 不支持combine <br>(2) 每个分区都要对应生成一个临时文件，会产生过多的临时文件。</td>
<td>适合分区数较少(默认小于等于200)且没有combine的的情况.</td>
</tr>
<tr>
<td>UnsafeShuffleWriter</td>
<td>(1) 只经过一次序列化。<br>(2) spill到磁盘的文件数目有限，不再基于分区数，可以支持更大的分区。</td>
<td>(1) 不支持combine <br>(2) 写入顺序Record顺序会打乱，要求supportsRelocationOfSerializedObjects。</td>
<td>适用于没有combine的情况，且支持supportsRelocationOfSerializedObjects，并且支持最大支持分区数为16777216。</td>
</tr>
<tr>
<td>SortShuffleWriter</td>
<td>(1) 支持combine <br> (2) 适合于所有场景 <br> (3)  spill到磁盘的文件数目有限</td>
<td>(1) 需要进行多次序列化</td>
<td>适用于所有场景。</td>
</tr>
</tbody></table>
<h3 id="1-3-2-shuffle读"><a href="#1-3-2-shuffle读" class="headerlink" title="1.3.2 shuffle读"></a>1.3.2 shuffle读</h3><p>当前只有BlockStoreShuffleReader一个实现。实现与MapReduce类似。<br>Reduce端会从Map端以远程或本地的方式拉取对应分区的Records。正常情况下会直接写到内存中，但如果要获取的block大小超过阈值则会使用磁盘。<br>然后会根据用户的需求决定是否进行combine或sort，最终形成一个用户要求的record iterator。<br>combine和sort分别使用了ExternalAppendOnlyMap和ExternalSorter，当内存超过阈值后，会将数据spill到本地磁盘中。</p>
<h2 id="1-4-总结"><a href="#1-4-总结" class="headerlink" title="1.4 总结"></a>1.4 总结</h2><p>(1) 关于各个框架语义</p>
<p>对于MapReduce和Tez的ordered io, 实质就是spark的排序的特例。对于Tez的unordered io，实质上就是spark的非排序的特例。实质各个框架上语义是相同的, spark更加泛化。</p>
<p>(2) 哪里会产生本地磁盘文件? </p>
<p>分析三种计算框架后，我们得知如下过程会使用磁盘:</p>
<ul>
<li>(1) Map由于内存超越阈值，可能会产生中间的临时文件。</li>
<li>(2) Map端最终必然会产生磁盘文件，用于提供shuffle服务。</li>
<li>(3) Reduce拉取records时候，可能因为超越阈值，产生磁盘文件。</li>
<li>(4) Reduce端Merge的时候，可能会产生临时的磁盘文件，用于全局排序。</li>
</ul>
<p>而事实上, uniffle已经解决(1), (2)。对于(3), 如果有效的调整参数，是很难产生磁盘文件的。事实上只有(4)是本文需要讨论的。</p>
<h1 id="2-方案的选择"><a href="#2-方案的选择" class="headerlink" title="2 方案的选择"></a>2 方案的选择</h1><p>为了解决在Reduce端Merge可能会spill到磁盘的问题，主要有两个方案:</p>
<ul>
<li>(1) Shuffle Server端进行Merge</li>
<li>(2) Reduce端按需Merge</li>
</ul>
<h2 id="2-1-方案1-ShuffleServer端进行Merge"><a href="#2-1-方案1-ShuffleServer端进行Merge" class="headerlink" title="2.1 方案1: ShuffleServer端进行Merge"></a>2.1 方案1: ShuffleServer端进行Merge</h2><p>将Reduce的Merge过程移到ShuffleServer端，ShuffleServer会对Map端发来的局部排序后的Records进行Merge，合并成一个全局排序的Records序列。Reduce端直接按照哦Records序列的顺序读取。</p>
<ul>
<li>优点: 不需要过多内存和网络RPC。</li>
<li>缺点: Shuffle Server端需要解析Key, Value和Comparator。Shuffle端不能combine。</li>
</ul>
<h2 id="2-2-方案2-Reduce端按需Merge"><a href="#2-2-方案2-Reduce端按需Merge" class="headerlink" title="2.2 方案2: Reduce端按需Merge"></a>2.2 方案2: Reduce端按需Merge</h2><img src="/images/rss/on_demand_merge.png" width=50% height=50% text-align=center/>
由于Reduce端内存有限，为了避免在Reduce端进行Merge的时候spill数据到磁盘。Reduce在获取Segment只能读取每个segment的部分buffer，然后对所有buffer进行Merge。然后对然后当某一个segment的部分buffer读取完成，会继续读取这个segment的下一块buffer，将这块buffer继续加到merge过程中。
这样有一个问题，Reduce端从ShuffleServer读取数据的次数大约为为segments_num * (segment_size / buffer_size)，对于大任务这是一个很大的值。过多的RPC意味着性能的下降。

<blockquote>
<p>这里的segment是指排序后record集合，可以理解为record已经按照key排序后的block。</p>
</blockquote>
<ul>
<li>优点: Shuffle Server不需要做额外的任何事情。</li>
<li>缺点: 过多的RPC。</li>
</ul>
<p><strong>本文选择方案1，接下来的内容主要针对于方案1进行讨论。</strong></p>
<h1 id="3-需求分析"><a href="#3-需求分析" class="headerlink" title="3 需求分析"></a>3 需求分析</h1><h2 id="3-1-哪类任务需要远程merge"><a href="#3-1-哪类任务需要远程merge" class="headerlink" title="3.1 哪类任务需要远程merge?"></a>3.1 哪类任务需要远程merge?</h2><p>当前uniffle的map端操作已经不再需要磁盘操作。本文主要考虑reduce端的情况。主要分如下几种情况:</p>
<ul>
<li>(1) 对于spark的非排序且非聚集、tez unordered io，Record是来之即用的，不需要有任何的全局的聚合和排序操作，只需要非常少的内存。当前版本的uniffle在内存设置合理的情况下是不会使用磁盘的。使用当前uniffle的方案即可。本文不会讨论这方面的内容。</li>
<li>(2) 对于spark的排序或聚集任务、tez ordered io、mapreduce，由于需要全局排序或聚集，内存可能不够用，可能会将Record spill到磁盘。本文主要讨论这种情况。<br><strong>综上可知，remote merge仅用于需要排序或聚集的shuffle。</strong></li>
</ul>
<h2 id="3-2-ShuffleServer如何进行排序"><a href="#3-2-ShuffleServer如何进行排序" class="headerlink" title="3.2 ShuffleServer如何进行排序?"></a>3.2 ShuffleServer如何进行排序?</h2><p>对于排序类的操作，一般会在Map进行排序得到一组局部排序的记录，这里称之为segment。然后Reduce会获取所有的segment, 并进行归并，Spark, MR, Tez都是用了最小堆K路归并排序。远程排序依旧可以使用这种方式。</p>
<p>BufferManager和FlushManager维护着block在内存和磁盘中的信息。我们只需要在需要在ShuffleServer中新增MergeManager，并将同一个Shuffle下的block进行Merge，得到全局排序的Records即可。</p>
<p>在ShuffleServer端引入排序后产生一个副作用: 即需要将该Shuffle的KeyClass和ValueClass以及KeyComparator传递给ShuffleServer。</p>
<h2 id="3-3-ShuffleServer禁止combine"><a href="#3-3-ShuffleServer禁止combine" class="headerlink" title="3.3 ShuffleServer禁止combine"></a>3.3 ShuffleServer禁止combine</h2><p>Combine一般都是用户自定义的操作，因此禁止ShuffleServer端进行Combine操作。</p>
<h1 id="4-架构设计"><a href="#4-架构设计" class="headerlink" title="4 架构设计"></a>4 架构设计</h1><h2 id="4-1-RemoteMerge的基本流程"><a href="#4-1-RemoteMerge的基本流程" class="headerlink" title="4.1 RemoteMerge的基本流程"></a>4.1 RemoteMerge的基本流程</h2><img src="/images/rss/remote_merge_structure.png" width=50% height=50% text-align=center/>


<p>下面介绍一下Remote Merge的流程:</p>
<ul>
<li>(1) 注册<br>AM&#x2F;Driver调用registerShuffle方法，会额外注册keyClass, valueClass和keyComparator. 这些信息主要用于ShuffleServer在合并的时候对Record进行解析和排序。</li>
<li>(2) sendShuffleData<br>sendShuffleData逻辑与现有的RSS任务基本保持一致。唯一区别是使用统一的序列化器和反序列化器，这样可以保证无论是哪一种框架，ShuffleServer都可以正常的解析Record.</li>
<li>(3) buffer and flush<br> shuffle server端会将数据存在缓存中，或者通过flush manager缓存到本地文件系统或远程文件系统。这里还是复用原来的ShuffleServer的逻辑。</li>
<li>(4) reportUniqueBlocks<br>提供了一个新的API, 即reportUniqueBlocks。Reduce端会根据map产生的block进行去重，然后将得到有效block集合通过reportUniqueBlocks发送给ShuffleServer。ShuffleServer收到有效的blocks集合后，会触发Remote Merge。Remote Merge的结果会像普通的block一样写入到bufferPool, 避免的时候会flush到磁盘中。RemoteMerge产生的结果即为普通的block，但是为了方便说明，这里称之为merged block。merged block记录的是按照key排序后的结果，因此读取merged block的时候，需要按照blockid的顺序依次递增读取。</li>
<li>(5) getSortedShuffleData<br>Reduce会按照block序号的顺序读取merged block，然后根据一定的条件选择何时为reduce计算使用。</li>
</ul>
<h2 id="4-2-从Record的视角分析流程"><a href="#4-2-从Record的视角分析流程" class="headerlink" title="4.2 从Record的视角分析流程"></a>4.2 从Record的视角分析流程</h2><p>我们可以WordCount为例子解释Record在整个过程中的流转。本例子中有两个分区以及一个Reduce，即一个Reduce处理两个分区的数据。</p>
<img src="/images/rss/remote_merge_from_record_perspective.jpg" width=50% height=50% text-align=center/>

<ul>
<li>(1) MAP端<br>Map端处理文档数据后，会进行排序。对于Map1, 由于存在两个分区，以奇数为key的record会写入到block101中，以偶数为key的record会写入到block102中。Map2同理。注意这里block中的Record都是已经排序后的。</li>
<li>(2) Map端发送数据<br>Map端通过sendShuffleData将block发送给ShuffleServer， ShuffleServer会将其存储到bufferPool中。<br>这里指的注意的是，在注册的时候会会注册APP1名字的app的同时，也会注册APP1@RemoteMerge的app，稍后会介绍它。</li>
<li>(3) ShuffleServer端Merge<br>Reduce启动后，会调用reportUniqueBlocks汇报可用的block集合，同时触发ShuffleServer中对应的partition进行Merge。Merge的结果在这个分区下全局排序的Record集合。<br>然后的问题是Merge的结果存在那里？Merge过程是在内存中发生的，每当Merge一定数量的Record后，会将这些结果写到一个新的block中。为了与原来的appid区分，这里会将这组block放在一个以”@RemoteMerge”结尾的appid进行管理。这组新的block的blockid是从1开始递增的，而且是经过全局排序的。即每个block内部的record是排序的，blockid&#x3D;1的records一定小于等于blockid&#x3D;2的records。</li>
<li>(4) Reduce端读<br>根据前面的分析，Reduce端只要读取以”@RemoteMerge“结尾的appid管理的block即可。Reduce读取block的时候从blockid&#x3D;1的block开始，按照blockid顺序读取即可。我们知道Reduce进行计算的时候，是按照顺序计算的。由于我们在ShuffleServer端获取的数据已经是排序后的，所以每次只需要从ShuffleServer端获取少量的数据即可，这样就实现了从ShuffleServer端按需读取，大大降低了使用内存。<br>这里还存在两种特殊的情况，详细5.5</li>
</ul>
<h1 id="5-计划"><a href="#5-计划" class="headerlink" title="5 计划"></a>5 计划</h1><h2 id="5-1-统一序列化器"><a href="#5-1-统一序列化器" class="headerlink" title="5.1 统一序列化器"></a>5.1 统一序列化器</h2><p>由于需要在ShuffleServer端进行Merge, 需要提取出独立于计算框架的统一序列化器。这里提炼出两类序列化器: (1) Writable (2) Kryo。Writable序列化用于处理org.apache.hadoop.io.Writable接口的类，用于MR和TEZ框架。Kryo可以序列化绝大多数的类，一般用于Spark框架。</p>
<h2 id="5-2-RecordsFileWriter-RecordFileReader"><a href="#5-2-RecordsFileWriter-RecordFileReader" class="headerlink" title="5.2 RecordsFileWriter&#x2F;RecordFileReader"></a>5.2 RecordsFileWriter&#x2F;RecordFileReader</h2><p>提供关于处理Record的抽象方法</p>
<h2 id="5-3-Merger"><a href="#5-3-Merger" class="headerlink" title="5.3 Merger"></a>5.3 Merger</h2><p>提供基础的Merger服务，对多个数据流按照key进行merge。采用最小堆K路归并排序，对已经进行局部排序的数据流进行归并排序。</p>
<h2 id="5-4-MergeManager"><a href="#5-4-MergeManager" class="headerlink" title="5.4 MergeManager"></a>5.4 MergeManager</h2><p>用于在服务端对Records进行Merge。</p>
<h2 id="5-5-RMRecordsReader"><a href="#5-5-RMRecordsReader" class="headerlink" title="5.5 RMRecordsReader"></a>5.5 RMRecordsReader</h2><p>一般来讲Reduce端在读取数据的情况，直接发给下游计算即可。但是存在两种特殊的情况:<br>(1) 对于存在需要在Merge进行combine的情况，我们需要等待所有相同的key都达到后进行combine，然后再发给下游。<br>(2) 对于spark和tez, reduce端可以会读取多个分区的数据。因此我们需要对多个分区的数据在reduce端再进行一次merge，然后在发给下游。<br>RMRecordsReader是用于读取排序后数据的工具。大致的架构如下:</p>
<img src="/images/rss/rm_records_reader.png" width=50% height=50% text-align=center/>

<p>图中描述了单个Reduce处理两个分区的情况。RecordsFetcher线程会读取对应分区的block，然后解析成Records。然后发送到combineBuffers中。RecordCombiner从combineBuffer中读取Records，当某个key的所有records都收集完成，会进行combine操作。结果会发送给mergedBuffer。RecordMerge会获取所有mergedBuffer，然后在内存中再进行一次归并排序。最终得到全局排序的结果给下游使用。</p>
<h2 id="5-6-框架适配"><a href="#5-6-框架适配" class="headerlink" title="5.6 框架适配"></a>5.6 框架适配</h2><p>适配MR,Tez,Spark三种架构。</p>
<blockquote>
<p>笔者已使用线上任务对MR和Tez进行了大规模压测。Spark目前仅进行了一些基础的examples的测试，仍需要大量测试。</p>
</blockquote>
<h2 id="5-7-隔离的classloader"><a href="#5-7-隔离的classloader" class="headerlink" title="5.7 隔离的classloader"></a>5.7 隔离的classloader</h2><p>对于不同版本的keyclass, valueclass以及comparatorclass, 使用隔离的classloader加载。</p>
<h1 id="5-特别注意"><a href="#5-特别注意" class="headerlink" title="5 特别注意"></a>5 特别注意</h1><ul>
<li>不支持spark的javardd，因为spark javardd的类型会被擦除。</li>
<li>适当提高服务器的max open file的配置。因为合并的时候可能会长时间持有文件。</li>
<li>适当降低rss.server.buffer.capacity, 因为remote merge的过程需要更多的额外内存。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/12/25/RSS-%E8%BF%9C%E7%A8%8BMerge%E7%9A%84%E8%AE%BE%E8%AE%A1/" data-id="cuidU7nfWrp8exMGoEm91MHmz" data-title="RSS-远程Merge的设计" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-RSS-远程Merge的设计EN" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/12/25/RSS-%E8%BF%9C%E7%A8%8BMerge%E7%9A%84%E8%AE%BE%E8%AE%A1EN/" class="article-date">
  <time class="dt-published" datetime="2023-12-25T10:58:15.000Z" itemprop="datePublished">2023-12-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/12/25/RSS-%E8%BF%9C%E7%A8%8BMerge%E7%9A%84%E8%AE%BE%E8%AE%A1EN/">RSS-Remote Merge Design</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="1-Default-shuffle"><a href="#1-Default-shuffle" class="headerlink" title="1 Default shuffle"></a>1 Default shuffle</h1><blockquote>
<p>Note: The first chapter briefly introduces the principle of default shuffle, with the purpose of find where local disks are used, then design remote merge. If you know enough, you can ignore this part.</p>
</blockquote>
<p>We will analyze the shuffle of MapReduce, Tez, and Spark in turn.</p>
<h2 id="1-1-MapReduce"><a href="#1-1-MapReduce" class="headerlink" title="1.1 MapReduce"></a>1.1 MapReduce</h2><p>Map writes the record to the memory. When the memory exceeds the threshold, the memory data is spilled to the disk file, and the Record is written to the disk file in order of partitionid+key. After Map has processed all records, it will spill the data currently in memory to a disk file. Then read all the files spilled to the disk and merge them in the order of partitionid+key to get the sorted Records.</p>
<blockquote>
<p>Note: The purpose of sorting according to partitionid is that when the Reduce side obtains the data from the Map side, it should be read as sequentially as possible. For MR, Tez, and Spark, regardless of whether they are sorted or not, as long as there are partitioned, they need to be sorted according to partitionid.</p>
</blockquote>
<p>The reduce will pull the records of the corresponding partition remotely or locally from the Map, which is called MapOutput. Under normal circumstances, the memory will be used directly. If the memory exceeds the threshold, these records will be written to the disk. Then the reduce will perform merge operations on MapOutputs using minimum heap K-way merge sorting to obtain globally sorted records. During the Merge process, temporary results may be spilled to disk because the memory exceeds the threshold. In addition, if there are too many files spilled to disk, additional merges will be triggered.</p>
<h2 id="1-2-Tez"><a href="#1-2-Tez" class="headerlink" title="1.2 Tez"></a>1.2 Tez</h2><p>There are two cases of tez: (1) ordered io  (2) unordered io.</p>
<p>Ordered io is the same as MapReduce and so ignore it here.</p>
<p>Unordered io is generally used in hashjoin and other situations where keys are not required for sorting. Non-sorted io adopts a ready-to-use solution. Map writes the Record directly to the file or writes it to the file through cache. The Reduce side can also read and use it when reading data.</p>
<h2 id="1-3-Spark"><a href="#1-3-Spark" class="headerlink" title="1.3 Spark"></a>1.3 Spark</h2><p>Spark’s shuffle is more complex and more reasonable. Some tasks do not require sort and combine, so spark users can determine the shuffle logic according to their needs.</p>
<h3 id="1-3-1-Shuffle-write-operation"><a href="#1-3-1-Shuffle-write-operation" class="headerlink" title="1.3.1 Shuffle write operation"></a>1.3.1 Shuffle write operation</h3><p>When writing shuffle data, three writers are supported:</p>
<ul>
<li>(1) BypassMergeSortShuffleWriter</li>
</ul>
<p>A temporary file is generated for each partition. When writing record, find the corresponding partition and write it directly to the corresponding temporary file. Then when all data is processed, these temporary files are written to a final file in order of the partitions, and the temporary files are deleted.</p>
<ul>
<li>(2) UnsafeShuffleWriter</li>
</ul>
<p>UnsafeShuffleWriter mainly implements specific logic through ShuffleExternalSorter. When writing a Record, the serialization operation is performed directly and the serialized bytes are copied to the requested memory. At the same time, the address and partition of the record will also be recorded into the memory (inMemSorter).</p>
<p>When the memory reaches the threshold, spill operation will be performed. Based on the information in memory (inMemSorter), we can easily get a Record sorted by partition and write it to a file.</p>
<p>When all Records are processed, we will spill the records currently in memory into the file. Finally, all spilled files are aggregated once. Since the previously spilled files have been sorted according to the partition, we can copy the corresponding copies of all the spilled files to the final file in the order of the partitions. The final file obtained in this way is the partition-sorted file.</p>
<ul>
<li>(3) SortShuffleWriter</li>
</ul>
<p>SortShuffleWriter mainly implements specific logic through ExternalSorter. ExternalSorter decides whether to combine and sort based on the user’s needs.</p>
<p>When writing record, it will be inserted directly into memory. If combine is required, the memory architecture is map, otherwise it is buffer.</p>
<p>If the current evaluation memory is greater than the threshold, the spill operation will be triggered. During the spill operation, the Record will be spilled to the disk. This process requires sorting. The specific comparator will use different values according to different user needs. If keyordering is set, it will be sorted by key. If keyordering is not set, but aggregator (i.e. combine) is set, the keys are sorted according to  the hashcode of key, thus ensuring that the same keys are organized together to facilitate combine operations. If neither keyordering nor aggregator is set, it will be sorted according to partition.</p>
<p>When all Records are written, the spill files need to be read and merged into a globally ordered file.<br>​<br>Comparison of three writers</p>
<table>
<thead>
<tr>
<th>writer</th>
<th>advantages</th>
<th>disadvantages</th>
<th>scene</th>
</tr>
</thead>
<tbody><tr>
<td>BypassMergeSortShuffleWriter</td>
<td>(1) Only serialized once. <br>(2) Using hashmap-like data structure, inserting data is fast.</td>
<td>(1) Combine and sort are not supported <br>(2) Each partition must generate a temporary file, which will generate too many temporary files.</td>
<td>Suitable for situations where the number of partitions is small (default is less than or equal to 200) and there is no combine.</td>
</tr>
<tr>
<td>UnsafeShuffleWriter</td>
<td>(1) Only serialized once. <br>(2) The number of files spilled to disk is limited and is no longer based on the number of partitions, and can support larger partitions.</td>
<td>(1) Combine, sort is not supported <br>(2) The writing order Record order will be disrupted, and supportsRelocationOfSerializedObjects is required.</td>
<td>Applicable to situations where combine does not exist, and supportsRelocationOfSerializedObjects is true, and the maximum number of supported partitions is 16777216.</td>
</tr>
<tr>
<td>SortShuffleWriter</td>
<td>(1) Supports combine, sort <br> (2) Suitable for all scenarios <br> (3) The number of files spilled to disk is limited</td>
<td>(1) Multiple serializations are required</td>
<td>Suitable for all scenarios.</td>
</tr>
</tbody></table>
<h3 id="1-3-2-shuffle-read"><a href="#1-3-2-shuffle-read" class="headerlink" title="1.3.2 shuffle read"></a>1.3.2 shuffle read</h3><p>Currently there is only one implementation of BlockStoreShuffleReader. The implementation is similar to MapReduce.<br>The reduce will pull the records of the corresponding partition remotely or locally from the map. Under normal circumstances, it will be written directly to the memory, but if the block size to be obtained exceeds the threshold, will use disk.<br>Then it will be decided according to the user’s needs whether to combine or sort, and finally form a record iterator required by the user.<br>Combine and sort use ExternalAppendOnlyMap and ExternalSorter respectively. When the memory exceeds the threshold, the data will be spilled to the local disk.</p>
<h2 id="1-4-Summary"><a href="#1-4-Summary" class="headerlink" title="1.4 Summary"></a>1.4 Summary</h2><p>(1) About the semantics of each framework</p>
<p>For MapReduce and the ordered io of Tez, it is a special case of spark sorting. For Tez’s unordered io, it is essentially a special case of spark’s non-sorting. In essence, the semantics of each framework are the same, and spark is more general.</p>
<p>(2) Where will generate local disk files?</p>
<p>After analyzing the three computing frameworks, we learned that the following processes will use disks:</p>
<ul>
<li>(1) Map may generate intermediate temporary files because the memory exceeds the threshold.</li>
<li>(2) The map will eventually generate disk files to provide shuffle services.</li>
<li>(3) When reduce pulls records, disk files may be generated because the threshold is exceeded.</li>
<li>(4) When merging on the reduce side, temporary disk files may be generated for global sorting.</li>
</ul>
<p>In fact, uniffle has solved (1), (2). For (3), if the parameters are adjusted effectively, it is difficult to generate disk files. In fact, only (4) needs to be discussed in this article.</p>
<h1 id="2-Plans"><a href="#2-Plans" class="headerlink" title="2 Plans"></a>2 Plans</h1><p>In order to solve the problem that Merge on the Reduce side may spill to disk, there are two main solutions:</p>
<ul>
<li>(1) Merge on Shuffle Server</li>
<li>(2) Reduce side Merge on demand</li>
</ul>
<h2 id="2-1-Option-1-Merge-on-ShuffleServer"><a href="#2-1-Option-1-Merge-on-ShuffleServer" class="headerlink" title="2.1 Option 1: Merge on ShuffleServer"></a>2.1 Option 1: Merge on ShuffleServer</h2><p>Move the merge process of reduce to the ShuffleServer side. ShuffleServer will merge the locally sorted Records sent from the map side into a globally sorted records sequence. The reduce side reads directly in the order of the records sequence.</p>
<ul>
<li>Advantages: Does not require too much memory and network RPC.</li>
<li>Disadvantages: Shuffle Server needs to parse Key, Value and Comparator. The Shuffle side cannot combine.</li>
</ul>
<h2 id="2-2-Option-2-On-demand-Merge-on-the-Reduce-side"><a href="#2-2-Option-2-On-demand-Merge-on-the-Reduce-side" class="headerlink" title="2.2 Option 2: On-demand Merge on the Reduce side"></a>2.2 Option 2: On-demand Merge on the Reduce side</h2><img src="/images/rss/on_demand_merge.png" width=50% height=50% text-align=center/>
Since the memory on the reduce side is limited, in order to avoid spilling data to disk when merging on the reduce side. When reduce obtains segment, it can only read part of the buffer of each segment, and then merge all the buffers. Then when the partial buffer reading of a certain segment is completed, the next buffer of this segment will continue to be read, and this buffer will continue to be added to the merge process.
There is a problem with this. The number of times the Reduce side reads data from ShuffleServer is approximately segments_num * (segment_size / buffer_size), which is a large value for large tasks. Too many RPCs means decreased performance.

<blockquote>
<p>The segment here refers to the sorted record collection, which can be understood as the block in which the records have been sorted according to key.</p>
</blockquote>
<ul>
<li>Advantages: Shuffle Server does not need to do anything extra.</li>
<li>Disadvantages: Too many RPCs.</li>
</ul>
<p>**This article chooses option 1, and the following content mainly discusses option 1. **</p>
<h1 id="3-Demand-analysis"><a href="#3-Demand-analysis" class="headerlink" title="3 Demand analysis"></a>3 Demand analysis</h1><h2 id="3-1-What-types-of-tasks-require-remote-merge"><a href="#3-1-What-types-of-tasks-require-remote-merge" class="headerlink" title="3.1 What types of tasks require remote merge?"></a>3.1 What types of tasks require remote merge?</h2><p>Currently, uniffle’s map-side no longer spill disk. This article mainly considers the situation on the reduce. Mainly divided into the following situations:</p>
<ul>
<li>(1) For spark’s non-sorted, non-aggregated, tez unordered io. It does not require any global aggregation and sorting operations, and only requires very little memory. The current version of uniffle will not use disk if related settings are reasonable. Just use the current uniffle solution. This article will not discuss this aspect.</li>
<li>(2) For spark sorting or aggregation tasks, tez ordered io, mapreduce, due to the need for global sorting or aggregation, the memory may not be enough, and the record may be spilled to the disk. This article mainly discusses this situation.<br>**In summary, it can be seen that remote merge is only used for shuffles that require sorting or aggregation. **</li>
</ul>
<h2 id="3-2-How-does-ShuffleServer-sort"><a href="#3-2-How-does-ShuffleServer-sort" class="headerlink" title="3.2 How does ShuffleServer sort?"></a>3.2 How does ShuffleServer sort?</h2><p>For sorting, map is generally sorted to obtain a set of partially sorted records, which is called segment here. Then reduce will obtain all segments and merge them. Spark, MR, and Tez all use minimum heap K-way merge sorting. This method can still be used for remote sorting.</p>
<p>BufferManager and FlushManager maintain block information in memory and disk. We only need to add MergeManager to ShuffleServer and merge the blocks under the same Shuffle to obtain globally sorted Records.</p>
<p>Introducing sorting on the ShuffleServer produces a side effect: the Shuffle’s KeyClass, ValueClass and KeyComparator need to be passed to ShuffleServer.</p>
<h2 id="3-3-How-does-ShuffleServer-combine"><a href="#3-3-How-does-ShuffleServer-combine" class="headerlink" title="3.3 How does ShuffleServer combine?"></a>3.3 How does ShuffleServer combine?</h2><p>Combine is generally a user-defined operation, so ShuffleServer is prohibited from performing combine operations. If combine is performed on the Reduce side, wouldn’t it violate our theme of avoiding spill to disk on the task side? In fact we don’t have to use ExternalAppendOnlyMap for combine. If the Records obtained from ShuffleServer are sorted by key, it means that the same keys have been organized together, and only a small amount of memory is needed to combine.</p>
<h2 id="3-4-How-does-Writer-write"><a href="#3-4-How-does-Writer-write" class="headerlink" title="3.4 How does Writer write?"></a>3.4 How does Writer write?</h2><p>Just write it the way we have it.</p>
<h2 id="3-5-How-does-Reader-read"><a href="#3-5-How-does-Reader-read" class="headerlink" title="3.5 How does Reader read?"></a>3.5 How does Reader read?</h2><p>Currently, Uniffle’s shuffle reader uses blockid as the read mark, which makes it easy to verify whether an accurate and complete records are obtained. For remote merge, MergeManager has merged the original Block collection into a new sequence sorted records by key. Therefore, the blockid generated by the map segment cannot be used:<br>We will use a new way to read Records. When MergerManager performs global Merge, an index will be generated. Reader will read according to this index.</p>
<blockquote>
<p>Note: In principle, using key as a read index is more semantic, and the first version of the demo program was also implemented by this way. However, this proposal was not friendly enough to deal with the problem of data skew, so  gave up the plan.</p>
</blockquote>
<h1 id="4-Scheme-Design"><a href="#4-Scheme-Design" class="headerlink" title="4 Scheme Design"></a>4 Scheme Design</h1><h2 id="4-1-Basic-procedures-for-RemoteMerge"><a href="#4-1-Basic-procedures-for-RemoteMerge" class="headerlink" title="4.1 Basic procedures for RemoteMerge"></a>4.1 Basic procedures for RemoteMerge</h2><img src="/images/rss/remote_merge_structure.png" width=50% height=50% text-align=center/>

<p>The following introduces the process of Remote Merge:</p>
<ul>
<li>(1) Register<br>When AM&#x2F;Driver calls the registerShuffle method, it will additionally register keyClass, valueClass and keyComparator. This information is mainly used by ShuffleServer to parse and sort the Record during merging.</li>
<li>(2) sendShuffleData<br>The sendShuffleData logic is basically consistent with existing RSS tasks. The only difference is the use of unified serializers and deserializers, which ensures that ShuffleServer can parse the Record normally no matter which framework it is.</li>
<li>(3) buffer and flush<br>The shuffle server will store the data in the cache, or cache it to the local file system or remote file system through the flush manager. The logic of the original ShuffleServer is still reused here.</li>
<li>(4) reportUniqueBlocks<br>A new API is provided, reportUniqueBlocks. The Reduce end will deduplicate the blocks generated by the map, and then send the valid block set to ShuffleServer through reportUniqueBlocks. After ShuffleServer receives a valid blocks collection, it will trigger Remote Merge. The results of Remote Merge will be written to the bufferPool like a normal block, and may be flushed to disk when necessary. The result generated by RemoteMerge is an ordinary block, but for convenience of explanation, it is called merged block here. The merged block records the results sorted by key, so when reading the merged block, you need to read it in ascending order in the order of blockid.</li>
<li>(5) getSortedShuffleData<br>Reduce will read merged blocks in the order of block serial numbers, and then choose when to use them for reduce calculations based on certain conditions.</li>
</ul>
<h2 id="4-2-Analyze-the-process-from-the-perspective-of-Record"><a href="#4-2-Analyze-the-process-from-the-perspective-of-Record" class="headerlink" title="4.2 Analyze the process from the perspective of Record"></a>4.2 Analyze the process from the perspective of Record</h2><p>We can use WordCount as an example to explain the flow of record in the entire process. In this example, there are two partitions and one reduce, that is, one reduce processes the data of two partitions.</p>
<img src="/images/rss/remote_merge_from_record_perspective.jpg" width=50% height=50% text-align=center/>

<ul>
<li>(1) MAP side<br>After the document data is processed on the map side, it will be sorted. For Map1, since there are two partitions, records with odd numbers as keys will be written to block101, and records with even numbers as keys will be written to block102. The same goes for Map2. Note that the Records in the block here are all sorted.</li>
<li>(2) Map side sends data<br>The map side sends the block to ShuffleServer through sendShuffleData, and ShuffleServer stores it in the bufferPool.<br>What I mean here is that when registering, the app named APP1 will also be registered, and the app named APP1@RemoteMerge will also be registered, which will be introduced later.</li>
<li>(3) ShuffleServer side Merge<br>After reduce is started, reportUniqueBlocks will be called to report the available block set, and the corresponding partition in ShuffleServer will be triggered to merge. The result of Merge is a globally sorted record collection under this partition.<br>Then the question is where are the results of merge stored? The merge process occurs in memory. Whenever a certain number of records are merged, the results are written to a new block. In order to distinguish it from the original appid, this group of blocks will be managed in an appid ending with “@RemoteMerge”. The blockid of this new group of blocks increases from 1 and is sorted globally. That is, the records inside each block are sorted, and the records with blockid&#x3D;1 must be less than or equal to the records with blockid&#x3D;2.</li>
<li>(4) Reduce end reading<br>According to the previous analysis, the reduce side only needs to read the block managed by the appid ending with “@RemoteMerge”. When reduce reads a block, it starts from the block with blockid&#x3D;1 and reads in the order of blockid. We know that when reduce performs calculations, it is calculated in order. Since the data we obtain on the ShuffleServer side is already sorted, we only need to obtain a small amount of data from the ShuffleServer side each time. This enables on-demand reading from the ShuffleServer side, which greatly reduces memory usage.<br>There are two special situations here, detailed in 5.5.​</li>
</ul>
<h1 id="5-Plan"><a href="#5-Plan" class="headerlink" title="5 Plan"></a>5 Plan</h1><h2 id="5-1-Unified-serializer"><a href="#5-1-Unified-serializer" class="headerlink" title="5.1 Unified serializer"></a>5.1 Unified serializer</h2><p>Since Merge needs to be performed on the ShuffleServer side, a unified serializer that is independent of the computing framework needs to be extracted. Two types of serializers are extracted here: (1) Writable (2) Kryo. Writable serialization is used for classes that handle the org.apache.hadoop.io.Writable interface, used in the MR and TEZ frameworks. Kryo can serialize most classes and is generally used in the Spark framework.</p>
<h2 id="5-2-RecordsFileWriter-RecordFileReader"><a href="#5-2-RecordsFileWriter-RecordFileReader" class="headerlink" title="5.2 RecordsFileWriter&#x2F;RecordFileReader"></a>5.2 RecordsFileWriter&#x2F;RecordFileReader</h2><p>Provides abstract methods for processing Records</p>
<h2 id="5-3-Merger"><a href="#5-3-Merger" class="headerlink" title="5.3 Merger"></a>5.3 Merger</h2><p>Provides basic Merger service to merge multiple data streams according to key. Minimum heap K-way merge sorting is used to merge and sort the data streams that have been partially sorted.</p>
<h2 id="5-4-MergeManager"><a href="#5-4-MergeManager" class="headerlink" title="5.4 MergeManager"></a>5.4 MergeManager</h2><p>Used to merge Records on the server side.</p>
<h2 id="5-5-Tools-for-reading-sorted-data"><a href="#5-5-Tools-for-reading-sorted-data" class="headerlink" title="5.5 Tools for reading sorted data"></a>5.5 Tools for reading sorted data</h2><p>Generally speaking, when the Reduce side reads data, it can be sent directly to downstream calculations. But there are two special situations:<br>(1) For situations where it is necessary to combine in Merge, we need to wait for all the same keys to arrive before combining, and then send them to the downstream.<br>(2) For spark and tez, the reduce end can read data from multiple partitions. Therefore, we need to merge the data of multiple partitions again on the reduce side, and then send it to the downstream.<br>RMRecordsReader is a tool for reading sorted data. The general structure is as follows:<br><img src="/images/rss/rm_records_reader.png" width=50% height=50% text-align=center/><br>The figure depicts a situation where a single Reduce processes two partitions. The RecordsFetcher thread will read the block of the corresponding partition and then parse it into Records. Then send it to combineBuffers. RecordCombiner reads Records from combineBuffer. When all records of a certain key are collected, a combine operation is performed. The result will be sent to mergedBuffer. RecordMerge will obtain all mergedBuffers and then merge and sort them again in memory. Finally, the global sorting results are obtained for downstream.</p>
<h2 id="5-6-Framework-adaptation"><a href="#5-6-Framework-adaptation" class="headerlink" title="5.6 Framework adaptation"></a>5.6 Framework adaptation</h2><p>Compatible with MR, Tez, and Spark architectures.</p>
<blockquote>
<p>I has used online application to conduct large-scale stress testing on MR and Tez. Spark has currently only tested some basic examples and still needs a lot of testing.</p>
</blockquote>
<h2 id="5-7-Isolated-classloader"><a href="#5-7-Isolated-classloader" class="headerlink" title="5.7 Isolated classloader"></a>5.7 Isolated classloader</h2><p>For different versions of keyclass, valueclass and comparatorclass, use isolated classloaders to load them.</p>
<blockquote>
<p>Chinese version document: <a target="_blank" rel="noopener" href="https://zhengchenyu.github.io/2023/12/25/RSS-%E8%BF%9C%E7%A8%8BMerge%E7%9A%84%E8%AE%BE%E8%AE%A1/">https://zhengchenyu.github.io/2023/12/25/RSS-%E8%BF%9C%E7%A8%8BMerge%E7%9A%84%E8%AE%BE%E8%AE%A1/</a></p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/12/25/RSS-%E8%BF%9C%E7%A8%8BMerge%E7%9A%84%E8%AE%BE%E8%AE%A1EN/" data-id="cuidt1MNk5aAyz9hR9aQWn_Pq" data-title="RSS-Remote Merge Design" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-机器学习-3-2-非监督学习之主成分分析" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2017/10/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-3-2-%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/" class="article-date">
  <time class="dt-published" datetime="2017-10-22T07:56:23.000Z" itemprop="datePublished">2017-10-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2017/10/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-3-2-%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/">机器学习-3.2-非监督学习之主成分分析.md</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="主成分分析-PCA"><a href="#主成分分析-PCA" class="headerlink" title="主成分分析(PCA)"></a>主成分分析(PCA)</h1><blockquote>
<p>严格上说PCA应该算是一种降低维度的方法，这里暂时归类为非监督学习中。</p>
</blockquote>
<h2 id="1-PCA理论简述"><a href="#1-PCA理论简述" class="headerlink" title="1 PCA理论简述"></a>1 PCA理论简述</h2><p>PCA的主要思路是将\(n\)维数据降维到\(k\)维子空间中，以滤除不需要的噪声或没有意义的特征信息。譬如我们有汽车的一组运行数据，包括专项半径、速度等。其中就一个用英里&#x2F;每小时和千米&#x2F;每小时描述的速度，很明显两者是呈线性关系的，只是因为舍入带来一些误差，对于这样的问题我们完全可以将这个\(n\)维数据移到\(n-1\)维子空间中。</p>
<p>再举一个例子,对于RC直升机驾驶员的调查，\(x _1\)表示飞行员驾驶技能，\(x _2\)表示他对驾驶直升飞机的爱好程度。因为RC直升机很难驾驶，因此我们可以认为只有任务对驾驶RC直升机感兴趣的驾驶员才能学好它。因此,\(x _1\)和\(x _2\)有强相关性。如下图所示，我们可以假定一个方向\(u _1\)，用来表示\(x _1\)和\(x _2\)两个属性，仅仅在其垂直轴\(u _2\)上有少量噪声。</p>
<img src="/images/机器学习/PCA驾驶员技能兴趣关系曲线.png" width=50% height=50% text-align=center/>

<p>对数据降维之前，我们需要对数据进行初始化，主要是一个归整的过程，将均值归整为0，将方差归整为1。依次按照如下公式进行归整。</p>
<ul>
<li>\(\mu  &#x3D; \frac{1}{m}\sum\limits _{i &#x3D; 1}^m {x^{(i)}} \)</li>
<li>\({x^{(i)}}: &#x3D; {x^{(i)}} - \mu \)</li>
<li>\({\sigma ^2} &#x3D; \frac{1}{m}\sum\limits _{i &#x3D; 1}^m {(x^{(i)})^2} \)</li>
<li>\({x^{(i)}}: &#x3D; {x^{(i)}}&#x2F;\sigma \)</li>
</ul>
<p>归整后的数据如下，可以知道如果\(x\)在向量\(u\)的投影最大。说明\(u\)在对\(x _1\)和\(x _2\)的降维过程中带来的损失越小。</p>
<img src="/images/机器学习/PCA降维示例.png" width=50% height=50% text-align=center/>

<p>因此，可以需要保证下式最大：</p>

$$\frac{1}{m}\sum\limits _{i = 1}^m {{{({x^{{{(i)}^T}}}u)}^2}}  = \frac{1}{m}\sum\limits _{i = 1}^m {{u^T}{x^{(i)}}{x^{{{(i)}^T}}}u}  = {u^T}(\frac{1}{m}\sum\limits _{i = 1}^m {{x^{(i)}}{x^{{{(i)}^T}}}} )u$$


<p>问题也就转变为找到一组\(u\)使得，上式最大的问题。然后我们假定\(u\)为一组标准正交基，因此\(||u||&#x3D;1\)。因此问题可写为如下形式：</p>

$$\begin{gathered}
   & \max {u^T}(\frac{1}{m}\sum\limits _{i = 1}^m {{x^{(i)}}{x^{{{(i)}^T}}}} )u \hfill \\\
  s.t.: & ||u|| = 1 \hfill \\\ 
\end{gathered} $$


<p>组成其拉格朗日函数如下：</p>

$$L(u,\lambda ) = {u^T}(\frac{1}{m}\sum\limits _{i = 1}^m {x^{(i)}x^{(i)^T}} )u - \lambda (||u|| - 1)$$


<p>这里我们设\(\Sigma&#x3D;\frac{1}{m}\sum\limits _{i &#x3D; 1}^m {x^{(i)}{x^{(i)^T}}}\)，然后对\(u\)求导数，易得:</p>

$${\nabla _u}L(u,\lambda ) = \sum u - \lambda u = 0$$


<p>因此，我们知道\(\lambda\)为\(\Sigma\)的特征值，\(u\)为对应的特征向量。恰好为我们要求的向量\(u\)。得到一组新的正交基后，我们就可以映射到新的空间中了。具体如下:</p>

$${y^{(i)}} = {(u _1^T{x^{(i)}},u _2^T{x^{(i)}},...,u _k^T{x^{(i)}})^T}$$


<h2 id="2-PCA的奇异值分解"><a href="#2-PCA的奇异值分解" class="headerlink" title="2. PCA的奇异值分解"></a>2. PCA的奇异值分解</h2><p>由于一般为\(n*n\)的维度，因此往往是一个很大的矩阵。对于维度很大，样本数目要少于维度很多的数据，这样计算往往有不够划算。因此我们可以通过求解\(x\)的特征值的方法来求节\(\Sigma\)的特征向量。</p>
<blockquote>
<p>由于求矩阵的单位特征向量，可以不考虑1&#x2F;m这个系数。</p>
</blockquote>
<p>假如对\(x\)进行奇异值分解，其中\(U\)和\(V\)均为酉阵，\(\Lambda\)为对角阵。另外值得注意的是酉阵的可逆矩阵与转置矩阵相同。具体分解形式如下：</p>

$$x = U\Lambda {V^T}$$


<p>因此可以得到：</p>

$$\Sigma  = x{x^T} = U\Lambda {V^T}V\Lambda {U^T} = U{\Lambda ^2}{U^T}$$
$$\Sigma U = {\Lambda ^2}{U^T}$$


<p>因此可以知道\(\Sigma\)的特征向量对应着\(x\)的奇异值分解中的\(U\)。同时\(\Sigma\)的特征值为\(x\)的奇异值的平方。</p>
<h2 id="3-源码实现"><a href="#3-源码实现" class="headerlink" title="3 源码实现"></a>3 源码实现</h2><p>我们对k临近算法中的手写数字识别做分析。我们首先对数字映射到三维空间中，然后进行展示。然后在映射到样本数量维度的空间，实现对数字的识别。值得一提是，PCA可以将多种无法直接展示的多维数组转化为三维数据，然后更直观地进行分析。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> linalg</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> axes3d, Axes3D</span><br><span class="line"></span><br><span class="line"><span class="keyword">global</span> g_label      <span class="comment"># 训练集的label</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">makeTrain</span>(<span class="params"><span class="built_in">dir</span></span>):</span><br><span class="line">  dirs=os.listdir(<span class="built_in">dir</span>)</span><br><span class="line">  files = <span class="built_in">filter</span>(<span class="keyword">lambda</span> item:<span class="keyword">not</span> os.path.isdir(item), dirs)</span><br><span class="line">  mat = []</span><br><span class="line">  label = []</span><br><span class="line">  <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">    arr = []</span><br><span class="line">    f = <span class="built_in">open</span>(<span class="built_in">dir</span>+<span class="string">&quot;/&quot;</span>+file)</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">      line = f.readline()</span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(line)-<span class="number">1</span>):        <span class="comment"># line[len(line)]=&#x27;\n&#x27;</span></span><br><span class="line">        arr.append(<span class="built_in">float</span>(line[i]))</span><br><span class="line">    mat.append(arr)</span><br><span class="line">    label.append(<span class="built_in">int</span>(file.split(<span class="string">&quot;_&quot;</span>)[<span class="number">0</span>]))</span><br><span class="line">  <span class="keyword">return</span> (mat,label)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocessing</span>(<span class="params">trainList</span>):</span><br><span class="line">  train = np.array(trainList)</span><br><span class="line">  rows = <span class="built_in">len</span>(train)</span><br><span class="line">  cols = <span class="built_in">len</span>(train[<span class="number">0</span>])</span><br><span class="line">  <span class="comment"># 使数学期望为0</span></span><br><span class="line">  <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(cols):</span><br><span class="line">    mean = np.mean(train[:,col])</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(rows):</span><br><span class="line">      train[row][col] = train[row][col]-mean</span><br><span class="line">    <span class="comment"># 使方差为1。 如果方差小于1,任务反差为0，则不更新该行。</span></span><br><span class="line">    <span class="comment"># 实际上，由于手写数字已经被归整为0,1这样的二值化图像，因此这里没有修改波定性。</span></span><br><span class="line">    var = np.var(train[:,col])</span><br><span class="line">    <span class="keyword">if</span> var &gt; <span class="number">1</span>:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&quot;hello&quot;</span>)</span><br><span class="line">      standard = np.sqrt(var)</span><br><span class="line">      <span class="keyword">for</span> row <span class="keyword">in</span> <span class="built_in">range</span>(rows):</span><br><span class="line">        train[row][col] = train[row][col]/standard</span><br><span class="line">  <span class="keyword">return</span> train.tolist()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lowerDimension</span>(<span class="params">u3t,<span class="built_in">dir</span></span>):</span><br><span class="line">  <span class="comment"># 该函数将1024维度的数据转化为3维</span></span><br><span class="line">  dirs=os.listdir(<span class="built_in">dir</span>)</span><br><span class="line">  files = <span class="built_in">filter</span>(<span class="keyword">lambda</span> item:<span class="keyword">not</span> os.path.isdir(item), dirs)</span><br><span class="line">  mat = []</span><br><span class="line">  label = []</span><br><span class="line">  <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">    arr = []</span><br><span class="line">    f = <span class="built_in">open</span>(<span class="built_in">dir</span>+<span class="string">&quot;/&quot;</span>+file)</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">      line = f.readline()</span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(line)-<span class="number">1</span>):        <span class="comment"># line[len(line)]=&#x27;\n&#x27;</span></span><br><span class="line">        arr.append(<span class="built_in">float</span>(line[i]))</span><br><span class="line">    new_mat = u3t*np.mat(arr).T</span><br><span class="line">    label.append(<span class="built_in">int</span>(file.split(<span class="string">&quot;_&quot;</span>)[<span class="number">0</span>]))</span><br><span class="line">    mat.append((np.array(new_mat.T)[<span class="number">0</span>]).tolist())</span><br><span class="line">  <span class="keyword">return</span> (mat,label)         <span class="comment"># mat为num*3的list</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show3D</span>(<span class="params">g_train_3d</span>):</span><br><span class="line">  fig = plt.figure()</span><br><span class="line">  ax = Axes3D(fig)</span><br><span class="line">  <span class="comment">#将数据点分成三部分画，在颜色上有区分度</span></span><br><span class="line">  m = <span class="built_in">len</span>(g_train_3d)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">    <span class="keyword">if</span> g_label[i] == <span class="number">0</span>:</span><br><span class="line">      ax.scatter(g_train_3d[i][<span class="number">0</span>], g_train_3d[i][<span class="number">1</span>], g_train_3d[i][<span class="number">2</span>], c=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> g_label[i] == <span class="number">1</span>:</span><br><span class="line">      ax.scatter(g_train_3d[i][<span class="number">0</span>], g_train_3d[i][<span class="number">1</span>], g_train_3d[i][<span class="number">2</span>], c=<span class="string">&#x27;c&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> g_label[i] == <span class="number">2</span>:</span><br><span class="line">      ax.scatter(g_train_3d[i][<span class="number">0</span>], g_train_3d[i][<span class="number">1</span>], g_train_3d[i][<span class="number">2</span>], c=<span class="string">&#x27;g&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> g_label[i] == <span class="number">3</span>:</span><br><span class="line">      ax.scatter(g_train_3d[i][<span class="number">0</span>], g_train_3d[i][<span class="number">1</span>], g_train_3d[i][<span class="number">2</span>], c=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> g_label[i] == <span class="number">4</span>:</span><br><span class="line">      ax.scatter(g_train_3d[i][<span class="number">0</span>], g_train_3d[i][<span class="number">1</span>], g_train_3d[i][<span class="number">2</span>], c=<span class="string">&#x27;m&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> g_label[i] == <span class="number">5</span>:</span><br><span class="line">      ax.scatter(g_train_3d[i][<span class="number">0</span>], g_train_3d[i][<span class="number">1</span>], g_train_3d[i][<span class="number">2</span>], c=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> g_label[i] == <span class="number">6</span>:</span><br><span class="line">      ax.scatter(g_train_3d[i][<span class="number">0</span>], g_train_3d[i][<span class="number">1</span>], g_train_3d[i][<span class="number">2</span>], c=<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> g_label[i] == <span class="number">7</span>:</span><br><span class="line">      ax.scatter(g_train_3d[i][<span class="number">0</span>], g_train_3d[i][<span class="number">1</span>], g_train_3d[i][<span class="number">2</span>], c=<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line"><span class="comment">#    elif g_label[i] == 8:</span></span><br><span class="line"><span class="comment">#      ax.scatter(g_train_3d[i][0], g_train_3d[i][1], g_train_3d[i][2], c=&#x27;b&#x27;, depthshade=False)</span></span><br><span class="line"><span class="comment">#    elif g_label[i] == 9:</span></span><br><span class="line"><span class="comment">#      ax.scatter(g_train_3d[i][0], g_train_3d[i][1], g_train_3d[i][2], c=&#x27;c&#x27;, depthshade=False)</span></span><br><span class="line">  ax.set_zlabel(<span class="string">&#x27;Z&#x27;</span>) <span class="comment">#坐标轴</span></span><br><span class="line">  ax.set_ylabel(<span class="string">&#x27;Y&#x27;</span>)</span><br><span class="line">  ax.set_xlabel(<span class="string">&#x27;X&#x27;</span>)</span><br><span class="line">  plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">ukt, train_3d, <span class="built_in">dir</span></span>):</span><br><span class="line">  test_kd,test_label = lowerDimension(ukt, <span class="built_in">dir</span>)</span><br><span class="line">  testN = <span class="built_in">len</span>(test_kd)</span><br><span class="line">  right=<span class="number">0</span></span><br><span class="line">  wrong=<span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(testN):</span><br><span class="line">    label = classify(np.array(test_kd[i]),np.array(train_3d),<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">str</span>(test_label[i]) == label:</span><br><span class="line">      right = right + <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      wrong = wrong + <span class="number">1</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;right=&quot;</span>, right, <span class="string">&quot;, wrong=&quot;</span>, wrong)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">classify</span>(<span class="params">vec,train_kd,k</span>):</span><br><span class="line">  <span class="comment"># 计算各个训练数据与测试数据的距离</span></span><br><span class="line">  m = <span class="built_in">len</span>(g_label)</span><br><span class="line">  dis = []</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">    dis.append([linalg.norm(vec-train_kd[i]),g_label[i]])</span><br><span class="line">  dis = <span class="built_in">sorted</span>(dis, key=<span class="keyword">lambda</span> v:v[<span class="number">0</span>])</span><br><span class="line">  <span class="comment"># 计算相似度最高的k个值，这里写入map做累积</span></span><br><span class="line">  dic = &#123;&#125;</span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">str</span>(dis[j][<span class="number">1</span>]) <span class="keyword">in</span> dic:</span><br><span class="line">      dic[<span class="built_in">str</span>(dis[j][<span class="number">1</span>])]=<span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      dic[<span class="built_in">str</span>(dis[j][<span class="number">1</span>])]=dic[<span class="built_in">str</span>(dis[j][<span class="number">1</span>])]+<span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">max</span>(dic.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">  <span class="comment"># 1 预处理</span></span><br><span class="line">  <span class="comment"># 这里为了显示降低维度在训练样本中的作用，仅仅是用了300个样本</span></span><br><span class="line">  (train,g_label) = makeTrain(<span class="string">&quot;/Users/zcy/Desktop/study/git/mlearning/res/trainingDigits1&quot;</span>)</span><br><span class="line">  <span class="comment"># 进行预处理操作，将均值设置为0，将方差归整为1</span></span><br><span class="line">  train_processed = preprocessing(train)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 2 降低维度</span></span><br><span class="line">  <span class="comment"># 首先计算x的奇异值</span></span><br><span class="line">  train_mat = np.mat(train_processed)</span><br><span class="line">  train_mat = train_mat.T               <span class="comment"># 转化为n*m 1024*200</span></span><br><span class="line">  U,sigma,VT = linalg.svd(train_mat)      <span class="comment"># U的维度为n*n 即1024*1024. sigma为m*1. vt为300*300</span></span><br><span class="line">  u3=U[np.ix_(np.arange(<span class="number">1024</span>), np.arange(<span class="number">3</span>))]      <span class="comment"># 提取对应最高特征值最高的三个方向,u3的维度为1024*3</span></span><br><span class="line">  <span class="comment"># 将1024维的数字图像降低维度到三维向量</span></span><br><span class="line">  train_3d,_ = lowerDimension(u3.T,<span class="string">&quot;/Users/zcy/Desktop/study/git/mlearning/res/trainingDigits1&quot;</span>)</span><br><span class="line">  train_kd,_ = lowerDimension(U.T,<span class="string">&quot;/Users/zcy/Desktop/study/git/mlearning/res/trainingDigits1&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 3 展示3维下的模型信息</span></span><br><span class="line">  <span class="comment">#show3D(train_3d)</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 4 使用k邻域验证测试样本</span></span><br><span class="line">  <span class="comment">#test(u3.T, train_3d, &quot;/Users/zcy/Desktop/study/git/mlearning/res/testDigits1&quot;)</span></span><br><span class="line">  test(U.T,train_kd,<span class="string">&quot;/Users/zcy/Desktop/study/git/mlearning/res/testDigits1&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>500个测试样本中，有27个识别错误，具体识别率为94.6%。这与未使用PCA降维的完全一致。该例子似乎尚未体现到PCA有什么优势，以后有机会在分析。当然如果映射到三维空间，识别率仅仅为75.2%，因此不要过度降维。下面是一个展示到部分数据的三维图。</p>
<blockquote>
<p>考虑到颜色，图片只显示部分类别数据。</p>
</blockquote>
<img src="/images/机器学习/PCA手写数字三维展示图.png" width=50% height=50% text-align=center/>

<h2 id="4-参考文献"><a href="#4-参考文献" class="headerlink" title="4 参考文献"></a>4 参考文献</h2><ul>
<li>cs229-note10</li>
<li>机器学习实战</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2017/10/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-3-2-%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/" data-id="cuidl7iBly4fuATgxrwlm8YgF" data-title="机器学习-3.2-非监督学习之主成分分析.md" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-机器学习-2-8-监督学习之k近邻算法" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2017/10/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-8-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8Bk%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/" class="article-date">
  <time class="dt-published" datetime="2017-10-15T12:29:06.000Z" itemprop="datePublished">2017-10-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2017/10/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-8-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8Bk%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/">机器学习-2.8-监督学习之k近邻算法</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<h1 id="K近邻算法"><a href="#K近邻算法" class="headerlink" title="K近邻算法"></a>K近邻算法</h1><p>K近邻算法是一种常用的监督学习算法。其工作机制非常简单: 给定测试样本，基于某种距离度量找到与测试样本最近的k个训练样本，然后可以根据k个样本决定分类。譬如可以选择k个样本中最多的测试样本进行分类。</p>
<p>下面我们使用k近邻算法识别手写数字。手写数字为一个32*32维的向量，我们可以把它看成一个1024维的向量。然后任意两个样本的距离完全可以通过计算1024维上的两个点之间的距离获得，然后我们可以找到与测试样本距离最近的的10个样本，其中最多的分类我们认为他就是测试样本的分类。具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np<span class="keyword">import</span> os<span class="keyword">from</span> numpy <span class="keyword">import</span> linalg<span class="keyword">global</span> g_train<span class="keyword">global</span> g_label<span class="keyword">def</span> <span class="title function_">makeTrain</span>(<span class="params"><span class="built_in">dir</span></span>):  dirs=os.listdir(<span class="built_in">dir</span>)  files = <span class="built_in">filter</span>(<span class="keyword">lambda</span> item:<span class="keyword">not</span> os.path.isdir(item), dirs)  mat = []  label = []  <span class="keyword">for</span> file <span class="keyword">in</span> files:    arr = []    f = <span class="built_in">open</span>(<span class="built_in">dir</span>+<span class="string">&quot;/&quot;</span>+file)    <span class="keyword">while</span> <span class="literal">True</span>:      line = f.readline()      <span class="keyword">if</span> <span class="keyword">not</span> line:        <span class="keyword">break</span>      <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(line)-<span class="number">1</span>):        <span class="comment"># line[len(line)]=&#x27;\n&#x27;</span>        arr.append(<span class="built_in">int</span>(line[i]))    mat.append(arr)    label.append(<span class="built_in">int</span>(file.split(<span class="string">&quot;_&quot;</span>)[<span class="number">0</span>]))  <span class="keyword">return</span> (np.array(mat),np.array(label))<span class="keyword">def</span> <span class="title function_">testClassify</span>(<span class="params"><span class="built_in">dir</span></span>):  dirs=os.listdir(<span class="built_in">dir</span>)  files = <span class="built_in">filter</span>(<span class="keyword">lambda</span> item:<span class="keyword">not</span> os.path.isdir(item), dirs)  mat = []  right=<span class="number">0</span>  wrong=<span class="number">0</span>  <span class="keyword">for</span> file <span class="keyword">in</span> files:    arr = []    f = <span class="built_in">open</span>(<span class="built_in">dir</span>+<span class="string">&quot;/&quot;</span>+file)    <span class="keyword">while</span> <span class="literal">True</span>:      line = f.readline()      <span class="keyword">if</span> <span class="keyword">not</span> line:        <span class="keyword">break</span>      <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(line)-<span class="number">1</span>):        <span class="comment"># line[len(line)]=&#x27;\n&#x27;</span>        arr.append(<span class="built_in">int</span>(line[i]))    mat.append(arr)    testLabel = file.split(<span class="string">&quot;_&quot;</span>)[<span class="number">0</span>]    label=classify(np.array(arr),<span class="number">10</span>)    <span class="keyword">if</span> testLabel == label:      right=right+<span class="number">1</span>    <span class="keyword">else</span>:      wrong=wrong+<span class="number">1</span>  <span class="built_in">print</span>(<span class="string">&quot;right=&quot;</span>,right,<span class="string">&quot;, wrong=&quot;</span>,wrong)<span class="keyword">def</span> <span class="title function_">classify</span>(<span class="params">vec,k</span>):  <span class="comment"># 计算各个训练数据与测试数据的距离</span>  m = <span class="built_in">len</span>(g_label)  dis = []  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):    dis.append([linalg.norm(vec-g_train[i]),g_label[i]])  dis = <span class="built_in">sorted</span>(dis, key=<span class="keyword">lambda</span> v:v[<span class="number">0</span>])  <span class="comment"># 计算相似度最高的k个值，这里写入map做累积</span>  dic = &#123;&#125;  <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(k):    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">str</span>(dis[j][<span class="number">1</span>]) <span class="keyword">in</span> dic:      dic[<span class="built_in">str</span>(dis[j][<span class="number">1</span>])]=<span class="number">1</span>    <span class="keyword">else</span>:      dic[<span class="built_in">str</span>(dis[j][<span class="number">1</span>])]=dic[<span class="built_in">str</span>(dis[j][<span class="number">1</span>])]+<span class="number">1</span>  <span class="keyword">return</span> <span class="built_in">max</span>(dic.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])[<span class="number">0</span>]<span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:  <span class="comment"># 1 formate trainning date</span>  (g_train,g_label) =makeTrain(<span class="string">&quot;/Users/zcy/Desktop/study/git/mlearning/res/trainingDigits1&quot;</span>)  <span class="comment"># 2 test</span>  testClassify(<span class="string">&quot;/Users/zcy/Desktop/study/git/mlearning/res/testDigits1&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>计算946个测试样本集，有926个为正确计算，20个为错误计算，识别为97.89%</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2017/10/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-8-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8Bk%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/" data-id="cuidZ3bkSbAoUh5wTvcVk3CWM" data-title="机器学习-2.8-监督学习之k近邻算法" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-机器学习-3-1-非监督学习之聚类" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2017/09/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-3-1-%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%81%9A%E7%B1%BB/" class="article-date">
  <time class="dt-published" datetime="2017-09-24T13:36:20.000Z" itemprop="datePublished">2017-09-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2017/09/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-3-1-%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%81%9A%E7%B1%BB/">机器学习-3.1-非监督学习之聚类.md</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="聚类问题"><a href="#聚类问题" class="headerlink" title="聚类问题"></a>聚类问题</h1><p>本节主要介绍非监督学习的聚类算法。</p>
<h2 id="1-常见的聚类算法"><a href="#1-常见的聚类算法" class="headerlink" title="1 常见的聚类算法"></a>1 常见的聚类算法</h2><p>聚类问题中，我们给定训练集合\(\{ {x^{(1)}},{x^{(2)}},…,{x^{(m)}}\}\)，目的是将训练样本聚合几个类中。由于问题过程中，\(y\)并没有指定，所以这是一个非监督问题。</p>
<h3 id="1-1-k-means"><a href="#1-1-k-means" class="headerlink" title="1.1 k-means"></a>1.1 k-means</h3><p>下面介绍k-means算法。算法的主要流程如下：</p>
<ul>
<li>(1) 随机初始化重心\(\{ {x^{(1)}},{x^{(2)}},…,{x^{(m)}}\}\)(假设有k个分类)</li>
<li>(2) 根据当前的\(\{ {x^{(1)}},{x^{(2)}},…,{x^{(m)}}\}\)，计算距离每个样本距离最近的中心，即为所属类。</li>
<li>(3) 然后根据2中得到新的所属类关系，更新一组新的重心值。</li>
<li>(4) 重复2,3直到某个截止条件。</li>
</ul>
<p>下面我们随机制造以三个点为高斯分布的一组数据，试图从该组数据完成聚类操作，具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">makeData</span>():</span><br><span class="line">  <span class="comment"># 0 make data</span></span><br><span class="line">  mean_1 = [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">  mean_2 = [<span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">  mean_3 = [<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">  cov = [[<span class="number">0.05</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0.05</span>]]</span><br><span class="line">  arr1 = np.random.multivariate_normal(mean_1, cov, <span class="number">100</span>)</span><br><span class="line">  arr2 = np.random.multivariate_normal(mean_2, cov, <span class="number">50</span>)</span><br><span class="line">  arr3 = np.random.multivariate_normal(mean_3, cov, <span class="number">50</span>)</span><br><span class="line">  figure, ax = plt.subplots()</span><br><span class="line">  ax.set_xlim(left=-<span class="number">1</span>, right=<span class="number">4</span>)</span><br><span class="line">  ax.set_ylim(bottom=-<span class="number">1</span>, top=<span class="number">4</span>)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(arr1)):</span><br><span class="line">    plt.plot(arr1[i][<span class="number">0</span>], arr1[i][<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;+&#x27;</span>, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(arr2)):</span><br><span class="line">    plt.plot(arr2[i][<span class="number">0</span>], arr2[i][<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, color=<span class="string">&#x27;g&#x27;</span>)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(arr3)):</span><br><span class="line">    plt.plot(arr3[i][<span class="number">0</span>], arr3[i][<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;*&#x27;</span>, color=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">  plt.xlabel(<span class="string">&quot;x1&quot;</span>)</span><br><span class="line">  plt.ylabel(<span class="string">&quot;x2&quot;</span>)</span><br><span class="line">  <span class="keyword">return</span> np.vstack((arr1,arr2,arr3))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updateSingleLabel</span>(<span class="params">x,center</span>):</span><br><span class="line">  n = <span class="built_in">len</span>(center)</span><br><span class="line">  <span class="built_in">min</span> = sys.maxsize</span><br><span class="line">  minIndex = -<span class="number">1</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    tmp = np.linalg.norm(x - center[i])</span><br><span class="line">    <span class="keyword">if</span> tmp &lt; <span class="built_in">min</span>:</span><br><span class="line">      minIndex=i</span><br><span class="line">      <span class="built_in">min</span> = tmp</span><br><span class="line">  <span class="keyword">return</span> minIndex</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updateLable</span>(<span class="params">data,label,center</span>):</span><br><span class="line">  numChanged = <span class="number">0</span>;</span><br><span class="line">  n=<span class="built_in">len</span>(data)</span><br><span class="line">  label_new = np.zeros(n);</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    label_new[i] = updateSingleLabel(data[i], center)</span><br><span class="line">    <span class="keyword">if</span> label_new[i] != label[i]:</span><br><span class="line">      numChanged = numChanged + <span class="number">1</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    label[i] = label_new[i]</span><br><span class="line">  <span class="keyword">return</span> numChanged;</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updateCenter</span>(<span class="params">data,label,center</span>):</span><br><span class="line">  newCenter=np.array([[<span class="number">0.0</span>,<span class="number">0.0</span>],[<span class="number">0.0</span>,<span class="number">0.0</span>],[<span class="number">0.0</span>,<span class="number">0.0</span>]])</span><br><span class="line">  newCenterSum=np.array([<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)):</span><br><span class="line">    <span class="keyword">if</span> label[i]==<span class="number">0</span>:</span><br><span class="line">      newCenter[<span class="number">0</span>] = newCenter[<span class="number">0</span>] + data[i]</span><br><span class="line">      newCenterSum[<span class="number">0</span>] = newCenterSum[<span class="number">0</span>] + <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> label[i] == <span class="number">1</span>:</span><br><span class="line">      newCenter[<span class="number">1</span>] = newCenter[<span class="number">1</span>] + data[i]</span><br><span class="line">      newCenterSum[<span class="number">1</span>] = newCenterSum[<span class="number">1</span>] + <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> label[i] == <span class="number">2</span>:</span><br><span class="line">      newCenter[<span class="number">2</span>] = newCenter[<span class="number">2</span>] + data[i]</span><br><span class="line">      newCenterSum[<span class="number">2</span>] = newCenterSum[<span class="number">2</span>] + <span class="number">1</span></span><br><span class="line">  <span class="keyword">if</span> newCenterSum[<span class="number">0</span>] &gt; <span class="number">0</span> :</span><br><span class="line">    center[<span class="number">0</span>] = newCenter[<span class="number">0</span>]/newCenterSum[<span class="number">0</span>]</span><br><span class="line">  <span class="keyword">if</span> newCenterSum[<span class="number">1</span>] &gt; <span class="number">0</span> :</span><br><span class="line">    center[<span class="number">1</span>] = newCenter[<span class="number">1</span>]/newCenterSum[<span class="number">1</span>]</span><br><span class="line">  <span class="keyword">if</span> newCenterSum[<span class="number">2</span>] &gt; <span class="number">0</span> :</span><br><span class="line">    center[<span class="number">2</span>] = newCenter[<span class="number">2</span>]/newCenterSum[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">showPic</span>(<span class="params">x,label,label1</span>):</span><br><span class="line">  figure, ax = plt.subplots()</span><br><span class="line">  ax.set_xlim(left=-<span class="number">1</span>, right=<span class="number">4</span>)</span><br><span class="line">  ax.set_ylim(bottom=-<span class="number">1</span>, top=<span class="number">4</span>)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)):</span><br><span class="line">    <span class="keyword">if</span> label[i] == <span class="number">0</span>:</span><br><span class="line">      plt.plot(x[i][<span class="number">0</span>], x[i][<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;+&#x27;</span>, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> label[i] == <span class="number">1</span>:</span><br><span class="line">      plt.plot(x[i][<span class="number">0</span>], x[i][<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, color=<span class="string">&#x27;g&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> label[i] == <span class="number">2</span>:</span><br><span class="line">      plt.plot(x[i][<span class="number">0</span>], x[i][<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;*&#x27;</span>, color=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">  figure, ax = plt.subplots()</span><br><span class="line">  ax.set_xlim(left=-<span class="number">1</span>, right=<span class="number">4</span>)</span><br><span class="line">  ax.set_ylim(bottom=-<span class="number">1</span>, top=<span class="number">4</span>)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)):</span><br><span class="line">    <span class="keyword">if</span> label1[i] == <span class="number">0</span>:</span><br><span class="line">      plt.plot(x[i][<span class="number">0</span>], x[i][<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;+&#x27;</span>, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> label1[i] == <span class="number">1</span>:</span><br><span class="line">      plt.plot(x[i][<span class="number">0</span>], x[i][<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, color=<span class="string">&#x27;g&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> label1[i] == <span class="number">2</span>:</span><br><span class="line">      plt.plot(x[i][<span class="number">0</span>], x[i][<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;*&#x27;</span>, color=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">  plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 0 make data</span></span><br><span class="line">  data=makeData()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 1 initial</span></span><br><span class="line">  n = <span class="built_in">len</span>(data)</span><br><span class="line">  label = np.zeros(n)       <span class="comment"># 0,1,2 represent center[0],center[1],center[2]</span></span><br><span class="line">  center = np.array([[<span class="number">0.0</span>,<span class="number">3.0</span>],[<span class="number">3.0</span>,<span class="number">3.0</span>],[<span class="number">0.0</span>,<span class="number">0.0</span>]])</span><br><span class="line">  label1 = np.zeros(n)</span><br><span class="line">  center1 = np.array([[<span class="number">0.0</span>,<span class="number">10.0</span>],[<span class="number">2.0</span>,<span class="number">3.0</span>],[<span class="number">0.5</span>,<span class="number">5.0</span>]])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 2. trainning</span></span><br><span class="line">  <span class="comment"># 2.1 trainning for good initial</span></span><br><span class="line">  <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="comment"># update label</span></span><br><span class="line">    numChanged = updateLable(data,label,center)</span><br><span class="line">    <span class="comment"># update center</span></span><br><span class="line">    updateCenter(data,label,center)</span><br><span class="line">    <span class="keyword">if</span> numChanged==<span class="number">0</span>:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 2.1 trainning for bad initial</span></span><br><span class="line">  <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="comment"># update label</span></span><br><span class="line">    numChanged = updateLable(data,label1,center1)</span><br><span class="line">    <span class="comment"># update center</span></span><br><span class="line">    updateCenter(data,label1,center1)</span><br><span class="line">    <span class="keyword">if</span> numChanged==<span class="number">0</span>:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 3. showData</span></span><br><span class="line">  showPic(data,label,label1)</span><br></pre></td></tr></table></figure>

<p>下面是随机产生的基于(1，1),(2，2),(1,2)的高斯分布。</p>
<img src="/images/机器学习/非监督学习-kmeans实验原始数据.png" width=50% height=50% text-align=center/>

<p>然后我们从(0.0,3.0),(3.0,3.0),(0.0,0.0)开始迭代得到如下效果，可以看出结果还是非常理想的。</p>
<img src="/images/机器学习/非监督学习-kmeans实验结果1.png" width=50% height=50% text-align=center/>

<p>然后我们试图从(0.0,10.0),(2.0,3.0),(0.5,5.0)开始迭代,则会得到这样的结果。</p>
<img src="/images/机器学习/非监督学习-kmeans实验结果2.png" width=50% height=50% text-align=center/>

<p>可以从上文中看出，k-means对初始值的敏感度很高。对于现实问题，也许我们并不知道训练样本中本身存在多个分类，我们可以设置多个分类，如果某一个分类里面的样本过少，就删除分类，这样就不再依赖于事先知道分类的数量了。</p>
<h3 id="1-2-高斯混合聚类"><a href="#1-2-高斯混合聚类" class="headerlink" title="1.2 高斯混合聚类"></a>1.2 高斯混合聚类</h3><p>假设我们的分类都服从于各自的高斯分布，我们试图从样本中对其分类。该问题与之前的高斯判别分析类似，区别仅仅在于该问题没有样本标签。<br>我们设置z表示样本的距离分类，可以知道他服从一个多项式分布, \({z^{(i)}} \sim Multinomial(\phi )\)，其中。然后已经\(z\)之后，\(x\)服从的是一个高斯分布，\({x^{(i)}}|{z^{(i)}} &#x3D; j \sim N({\mu _j},{\Sigma _j})\)。</p>
<p>下面直接写出算法过程(具体的算法推导见EM算法小节)：</p>
<ul>
<li>(1)	随机初始化\(\phi\),\(\mu\),\(\Sigma\)。</li>
<li>(2)	遍历样本，计算得\([w _j^{(i)}\),如下：</li>
</ul>

$$w _j^{(i)} = p({z^{(i)}} = j|{x^{(i)}},\phi ,\mu ,\Sigma )$$$$w _j^{(i)} = p({z^{(i)}} = j|{x^{(i)}},\phi ,\mu ,\Sigma ) = \frac{{p({z^{(i)}} = j,{x^{(i)}})}}{{p({x^{(i)}})}} = \frac{{p({z^{(i)}} = j,{x^{(i)}})}}{{\sum\limits _{j = 1}^k {p({x^{(i)}},{z^{(i)}} = j)} }}$$

<ul>
<li>(3)	根据计算得到的\(w\)重新更新各个分类的分布，如下：</li>
</ul>

$${\phi _j} = \frac{1}{m}\sum\limits _{i = 1}^m {1\{ {z^{(i)}} = j\} } $$
$${\mu _j} = \frac{{\sum\limits _{i = 1}^m {1\{ {z^{(i)}} = j\} } {x^{(i)}}}}{{\sum\limits _{i = 1}^m {1\{ {z^{(i)}} = j\} } }}$$

$${\Sigma _j} = \frac{{\sum\limits _{i = 1}^m {1\{ {z^{(i)}} = j\} } ({x^{(i)}} - {\mu _j}){{({x^{(i)}} - {\mu _j})}^T}}}{{\sum\limits _{i = 1}^m {1\{ {z^{(i)}} = j\} } }}$$


<ul>
<li>(4)	重复2,3知道达到截止条件。</li>
</ul>
<p>下面我们制作一组由三个高斯分布组成的样本数据，对其进行聚类。代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> multivariate_normal</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_data</span>():</span><br><span class="line">  mean_1 = [<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">  mean_2 = [<span class="number">4</span>,<span class="number">4</span>]</span><br><span class="line">  mean_3 = [<span class="number">1</span>,<span class="number">4</span>]</span><br><span class="line">  cov1 = [[<span class="number">0.1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.1</span>]]</span><br><span class="line">  cov2 = [[<span class="number">0.2</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.2</span>]]</span><br><span class="line">  cov3 = [[<span class="number">0.6</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.6</span>]]</span><br><span class="line">  arr1 = np.random.multivariate_normal(mean_1, cov1, <span class="number">100</span>)</span><br><span class="line">  arr2 = np.random.multivariate_normal(mean_2, cov2, <span class="number">50</span>)</span><br><span class="line">  arr3 = np.random.multivariate_normal(mean_3, cov3, <span class="number">50</span>)</span><br><span class="line">  figure, ax = plt.subplots()</span><br><span class="line">  ax.set_xlim(left=-<span class="number">4</span>, right=<span class="number">8</span>)</span><br><span class="line">  ax.set_ylim(bottom=-<span class="number">4</span>, top=<span class="number">8</span>)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(arr1)):</span><br><span class="line">    plt.plot(arr1[i][<span class="number">0</span>], arr1[i][<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;+&#x27;</span>, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(arr2)):</span><br><span class="line">    plt.plot(arr2[i][<span class="number">0</span>], arr2[i][<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, color=<span class="string">&#x27;g&#x27;</span>)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(arr3)):</span><br><span class="line">    plt.plot(arr3[i][<span class="number">0</span>], arr2[i][<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;*&#x27;</span>, color=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">  plt.xlabel(<span class="string">&quot;x1&quot;</span>)</span><br><span class="line">  plt.ylabel(<span class="string">&quot;x2&quot;</span>)</span><br><span class="line">  plt.plot()</span><br><span class="line">  plt.title(<span class="string">&quot;sample&quot;</span>)</span><br><span class="line">  <span class="comment">#plt.show()</span></span><br><span class="line">  <span class="keyword">return</span> np.vstack((arr1, arr2, arr3))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updatePhi</span>(<span class="params">w</span>):</span><br><span class="line">  n1 = <span class="built_in">len</span>(w)</span><br><span class="line">  phi=np.zeros(n1)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n1):</span><br><span class="line">    <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">    n2 = <span class="built_in">len</span>(w[i])</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n2):</span><br><span class="line">      <span class="built_in">sum</span> = <span class="built_in">sum</span> + w[i][j]</span><br><span class="line">    phi[i] = <span class="built_in">sum</span>/n2</span><br><span class="line">  <span class="keyword">return</span> phi</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updateW</span>(<span class="params">data,w,phi,mu,sigma</span>):</span><br><span class="line">  n1=<span class="built_in">len</span>(w)           <span class="comment"># 3</span></span><br><span class="line">  n2 = <span class="built_in">len</span>(w[<span class="number">0</span>])      <span class="comment"># 200</span></span><br><span class="line">  var = []</span><br><span class="line">  <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(n1):</span><br><span class="line">    var.append(multivariate_normal(mean=mu[k].tolist(), cov=sigma[k].tolist()))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n2):</span><br><span class="line">    <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n1):</span><br><span class="line">      <span class="built_in">sum</span> = <span class="built_in">sum</span> + var[i].pdf(data[j])*phi[i]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n1):</span><br><span class="line">      w[i][j] = var[i].pdf(data[j])*phi[i]/<span class="built_in">sum</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updateMu</span>(<span class="params">data,w,mu</span>):</span><br><span class="line">  changed=<span class="literal">False</span></span><br><span class="line">  n1 = <span class="built_in">len</span>(w)  <span class="comment"># 3</span></span><br><span class="line">  n2 = <span class="built_in">len</span>(w[<span class="number">0</span>])  <span class="comment"># 200</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n1):</span><br><span class="line">    sumW = <span class="number">0.0</span></span><br><span class="line">    sumX = np.array([<span class="number">0.0</span>,<span class="number">0.0</span>])</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n2):</span><br><span class="line">      sumW = sumW + w[i][j]</span><br><span class="line">      sumX = sumX + w[i][j]*data[j]</span><br><span class="line">    mu_new = sumX / sumW</span><br><span class="line">    <span class="keyword">if</span> np.dot(mu_new-mu[i],mu_new-mu[i]) &gt; <span class="number">0.001</span>:</span><br><span class="line">      changed = <span class="literal">True</span></span><br><span class="line">    mu[i] = mu_new</span><br><span class="line">  <span class="keyword">return</span> changed</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updateSigma</span>(<span class="params">data,w,mu,sigma</span>):</span><br><span class="line">  n1 = <span class="built_in">len</span>(w)  <span class="comment"># 3</span></span><br><span class="line">  n2 = <span class="built_in">len</span>(w[<span class="number">0</span>])  <span class="comment"># 200</span></span><br><span class="line">  <span class="built_in">sum</span>=np.array([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>]])</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n1):</span><br><span class="line">    sumW = <span class="number">0.0</span></span><br><span class="line">    sumX = np.array([<span class="number">0.0</span>,<span class="number">0.0</span>])</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n2):</span><br><span class="line">      sumW = sumW + w[i][j]</span><br><span class="line">      z0 = np.array([data[j] - mu[i]])</span><br><span class="line">      z0T = np.array([data[j] - mu[i]]).transpose()</span><br><span class="line">      sumX = sumX + w[i][j]*np.dot(z0T, z0)</span><br><span class="line">    sigma[i] = sumX/sumW</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">classify</span>(<span class="params">data,mu,sigma</span>):</span><br><span class="line">  n1=<span class="built_in">len</span>(w)           <span class="comment"># 3</span></span><br><span class="line">  n2 = <span class="built_in">len</span>(w[<span class="number">0</span>])      <span class="comment"># 200</span></span><br><span class="line">  var=[]</span><br><span class="line">  <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(n1):</span><br><span class="line">    var.append(multivariate_normal(mean=mu[k].tolist(), cov=sigma[k].tolist()))</span><br><span class="line"></span><br><span class="line">  figure, ax = plt.subplots()</span><br><span class="line">  ax.set_xlim(left=-<span class="number">4</span>, right=<span class="number">8</span>)</span><br><span class="line">  ax.set_ylim(bottom=-<span class="number">4</span>, top=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n2):</span><br><span class="line">    tmp_arr=np.array([])</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n1):</span><br><span class="line">      tmp_arr=np.append(tmp_arr,var[j].pdf(data[i]))</span><br><span class="line">    index = tmp_arr.argmax()</span><br><span class="line">    <span class="keyword">if</span> index == <span class="number">0</span>:</span><br><span class="line">      plt.plot(data[i][<span class="number">0</span>], data[i][<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;+&#x27;</span>, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> index == <span class="number">1</span>:</span><br><span class="line">      plt.plot(data[i][<span class="number">0</span>], data[i][<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, color=<span class="string">&#x27;g&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> index == <span class="number">2</span>:</span><br><span class="line">      plt.plot(data[i][<span class="number">0</span>], data[i][<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, color=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">  plt.xlabel(<span class="string">&quot;x1&quot;</span>)</span><br><span class="line">  plt.ylabel(<span class="string">&quot;x2&quot;</span>)</span><br><span class="line">  plt.title(<span class="string">&quot;result&quot;</span>)</span><br><span class="line">  plt.plot()</span><br><span class="line">  plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">  <span class="comment"># 1 make data</span></span><br><span class="line">  classN=<span class="number">3</span></span><br><span class="line">  data=make_data()</span><br><span class="line">  n=<span class="built_in">len</span>(data)</span><br><span class="line">  <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    w = np.array([np.zeros(n), np.zeros(n), np.zeros(n)])</span><br><span class="line">  <span class="comment"># w = array[classN][n] =array[3][200]</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(classN):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">      w[i][j]= <span class="number">1.0</span>/classN</span><br><span class="line"></span><br><span class="line">  phi=updatePhi(w)</span><br><span class="line">  mu = np.array([[<span class="number">6.0</span>, <span class="number">6.0</span>], [<span class="number">4.0</span>, -<span class="number">1.0</span>], [-<span class="number">2.0</span>, <span class="number">2.0</span>]])</span><br><span class="line">  sigma=np.array([[[<span class="number">0.1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.1</span>]],[[<span class="number">0.1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.1</span>]],[[<span class="number">0.1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.1</span>]]])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 2 training</span></span><br><span class="line">  <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    updateW(data,w,phi,mu,sigma)</span><br><span class="line">    updatePhi(w)</span><br><span class="line">    updateSigma(data,w,mu,sigma)</span><br><span class="line">    changed = updateMu(data,w,mu)</span><br><span class="line">    <span class="keyword">if</span> changed == <span class="literal">False</span>:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&quot;迭代完成&quot;</span>)</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 3 show</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;mu=&quot;</span>,mu,<span class="string">&quot;, sigma=&quot;</span>,sigma)</span><br><span class="line">  classify(data,mu,sigma)</span><br></pre></td></tr></table></figure>

<p>我们分别以[6.0, 6.0], [4.0, -1.0], [-2.0, 2.0]为均值，以[[0.1,0],[0,0.1]],[[0.1,0],[0,0.1]],[[0.1,0],[0,0.1]]为标准差生成一组高斯分布如下：</p>
<img src="/images/机器学习/非监督学习-高斯混合聚类原始数据.png" width=50% height=50% text-align=center/>

<p>然后通过训练的到的训练均值分别为[4.00729834, 3.99848889], [1.01089497,  1.05225006], [0.92949217, 4.18380895]]。标准差为[[0.23196114,-0.01896477],[-0.01896477, 0.17165715]], [[0.10205901, 0.00157192], [0.00157192,  0.11477843]], [[0.7010517, 0.04783335], [0.04783335, 0.51147277]]。具体结果如下:</p>
<img src="/images/机器学习/非监督学习-高斯混合聚类实验结果.png" width=50% height=50% text-align=center/>
### 2.1 Jensen不等式
对于一个严格凸函数,即\\(f''(x) \geqslant 0\\)，我们容易得到下式:

$$E[f(x)] \geqslant f(E[x])$$


<p>当且仅当x为常数的时候，上式等号成立。对于严格凹函数，则正好相反。</p>
<h3 id="2-2-EM算法模型建立"><a href="#2-2-EM算法模型建立" class="headerlink" title="2.2 EM算法模型建立"></a>2.2 EM算法模型建立</h3><p>假定我们的数据有\(k\)个分类。我们聚类的目标是，样本在自己分类中出现的概率最大。或者换句话说，让其在所属分类的分布(可以用高斯分类假想该问题)中出现的概率最大。可是对于非监督学习问题，我们不知道具体的分类。因此，我们可以将模型假定为找到给定参数\(\theta\)对应分布，是的\(x\)在分布中出现的概率足够大，这说明(\theta\)对应的分布能够充分的表示某一组分类。因此可以构造如下的最大释然函数：</p>

$$\ell (\theta ) = \sum\limits _{i = 1}^m {\ln p({x^{(i)}};\theta )} $$

<p>进入推导最大释然函数：</p>
$$\ell (\theta ) = \sum\limits _{i = 1}^m {\ln \sum\limits _{j = 1}^k {p({x^{(i)}},{z^{(j)}};\theta )} }  = \sum\limits _{i = 1}^m {\ln \sum\limits _{j = 1}^k {{Q _i}({z^{(i)}} = j)\frac{{p({x^{(i)}},{z^{(i)}} = j;\theta )}}{{{Q _i}({z^{(i)}} = j)}}} } $$


<p>上面\(k\)为分类的个数。对上面的\({Q _i}\)为一个概率分布，有:</p>

$$\sum\limits _{j=1} ^k {{Q _i}({z^{(i)}}=j)}=1$$


<p>对于\(f(x) &#x3D; \ln x\),我们知道\(f’’(x) &#x3D;  - \frac{1}{x^2}\),可知其为一个严格凹函数。上式可以写成：</p>

$$\ell (\theta ) = \sum\limits _{i = 1}^m {f(E[\frac{{p({x^{(i)}},{z^{(i)}} = j;\theta )}}{{{Q _i}({z^{(i)}} = j)}}])}$$


<p>根据Jensen不等式，我们有:</p>
$$\ell (\theta ) = \sum\limits _{i = 1}^m {f(E[\frac{p({x^{(i)},{z^{(i)}};\theta )}}{{Q _i}({z^{(i)}} = j)}])} \geqslant \sum\limits _{i = 1}^m {E[f(\frac{p({x^{(i)}},{z^{(i)}} = j;\theta )}{{Q _i}({z^{(i)}} = j)})]} = \sum\limits _{i = 1}^m {\sum\limits _{j = 1}^k {{Q _i}({z^{(i)}} = j)\ln (\frac{p({x^{(i)}},{z^{(i)}} = j;\theta )}{{Q _i}({z^{(i)}} = j)})} }$$


<p>因此我们得到最大释然函数的下确定，即为上式子中后面的部分。我们只要保证下确定随着迭代的方向单调递增即可，具体的要保证\(\ell ({\theta ^{(t + 1)}}) \geqslant \ell ({\theta ^{(t)}})\)。这里我们设\(low(\theta )\)是已知\({Q _i}\)的情况下，最大释然函数的下确定关于\(\theta\)的函数。我们看如下公式:</p>

$$\ell ({\theta ^{(t + 1)}}) \geqslant low({\theta ^{(t + 1)}}) \geqslant low({\theta ^{(t)}}) = \ell ({\theta ^{(t)}})$$

<p>事实上，我们只要保证上面的式子我们就可以保证迭代方向是正确的。其中，第一个不等号是必然成立的。那我们分头来构造条件是后面的式子成立。<br>首先如果保证最后一个等号的成立，根据Jensen不等式，只有自变量为常数才能事等号成立，这样我们设置如下式子(其中\(c\)为常数):</p>

$$\frac{{p({x^{(i)}},{z^{(i)}} = j;\theta )}}{{{Q _i}({z^{(i)}} = j)}} = c$$


<p>然后对上面的式子按照分类累积求和得:</p>
$$\sum\limits _{j = 1}^k {p({x^{(i)}},{z^{(i)}} = j;\theta )}  = c\sum\limits _{j = 1}^k {{Q _i}({z^{(i)}} = j)}  = c$$

<p>因此得:</p>

$${Q _i}({z^{(i)}} = j) = \frac{{p({x^{(i)}},{z^{(i)}} = j;\theta )}}{{\sum\limits _{j = 1}^k {p({x^{(i)}},{z^{(i)}} = j;\theta )} }} = p({z^{(i)}}|{x^{(i)}};\theta)$$


<p>这样我们完成EM算法的E步骤。然后解决中间大于等于号的问题。这个就比较好解决了，我们只需要计算下确定函数关于\(\theta\)求最大值，最大值对应的参数，可保证不等号的成立，即为迭代的正确方向。具体公式如下：</p>

$$\theta  = \arg {\max _\theta }\sum\limits _{i = 1}^m {\sum\limits _{j = 1}^k {{Q _i}({z^{(i)}} = j)\ln \frac{{p({x^{(i)}},{z^{(i)}} = j;\theta )}}{{{Q _i}({z^{(i)}} = j)}}} }$$

<p>综上，我们来重新整理一下EM算法，具体如下：</p>
<ul>
<li>(1) 初始化相关参数</li>
<li>(2) E步骤:计算\({Q _i}\)，如下：</li>
</ul>

$${Q _i}({z^{(i)}} = j) = p({z^{(i)}}|{x^{(i)}};\theta)$$

<ul>
<li>(3) M步骤：更新参数\(\theta\)，如下：</li>
</ul>

$$\theta  = \arg {\max _\theta }\sum\limits _{i = 1}^m {\sum\limits _{j = 1}^k {{Q _i}({z^{(i)}} = j)\ln \frac{{p({x^{(i)}},{z^{(i)}} = j;\theta )}}{{{Q _i}({z^{(i)}} = j)}}} } $$

<p>(4)	重复2,3直到截止条件</p>
<p>注: 上面的例子是不断的更新迭代\({Q _i}\)和\(\theta\)。我们完全可以使用梯度上升发不断从各个方向更新\({Q _i}\)和\(\theta\)，来完成最大值的逼近。</p>
<h3 id="2-3-混合高斯分布的公式推导"><a href="#2-3-混合高斯分布的公式推导" class="headerlink" title="2.3 混合高斯分布的公式推导"></a>2.3 混合高斯分布的公式推导</h3><p>对比之前的内容，我们可以发现混合高斯分布的聚类问题为EM算法的一个特例。可以通过通用的EM算法来证明，下面我们来证明这一过程。</p>
<p>注： 这里只简单地提示计算，不展开了，因为与之前的高斯判别分析类似。<br>根据上一节，我们已得到E步骤的公式：</p>

$$w _j^{(i)}={Q _i}({z^{(i)}}=j) = p({z^{(i)}}|{x^{(i)}};\theta )$$


<p>然后对于M步骤，我们将\({Q _i}\)为一个已知值的方式对\(\theta\)进行求导，从而求得下确定的最大值，记为下一个迭代值。将最大释然函数展开记为如下函数：</p>
$$\ell (\theta ) = \sum\limits _{i = 1}^m {\sum\limits _{j = 1}^k {{Q _i}({z^{(i)}} = j)\ln (\frac{{p({x^{(i)}},{z^{(i)}} = j;\theta )}}{{{Q _i}({z^{(i)}} = j)}})} }  = \sum\limits _{i = 1}^m {\sum\limits _{j = 1}^k {w _j^{(i)}\ln (\frac{{\frac{1}{{{{(2\pi )}^{\frac{n}{2}}}|\Sigma {|^{\frac{1}{2}}}}}\exp ( - \frac{1}{2}{{({x^{(i)}} - {\mu _j})}^T}{\Sigma ^{ - 1}}({x^{(i)}} - {\mu _j})){\phi _j}}}{{w _j^{(i)}}})} } $$

<p>然后对\(\phi\),\(\mu\),\(\Sigma\), 即可得到M步骤的更新公式。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2017/09/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-3-1-%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%81%9A%E7%B1%BB/" data-id="cuidZmoKIMtKQLIfMS5aJHZtC" data-title="机器学习-3.1-非监督学习之聚类.md" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-机器学习-2-7-监督学习之支持向量机" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2017/09/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-7-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" class="article-date">
  <time class="dt-published" datetime="2017-09-08T08:18:25.000Z" itemprop="datePublished">2017-09-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2017/09/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-7-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/">机器学习-2.7-监督学习之支持向量机</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="支持向量机-SVM"><a href="#支持向量机-SVM" class="headerlink" title="支持向量机(SVM)"></a>支持向量机(SVM)</h1><h2 id="1-Margins-间隔"><a href="#1-Margins-间隔" class="headerlink" title="1 Margins(间隔)"></a>1 Margins(间隔)</h2><p>对于一个逻辑回归问题，\(p(y &#x3D; 1|x, \theta )\)可以通过下面的式子表示：</p>

$${h _ \theta }(x) = g({\theta ^T}x)$$


<p>\(\theta^Tx\)越大， \(g({\theta ^T}x)\)就越大， 我们就有可高的信心认为\(y&#x3D;1\)。 同理假如\({ h _\theta }(x) &lt;&lt; 0\), 我们就有很强的信息认为\(y&#x3D;0\)。 (注: &gt;&gt; 表示远大于，&lt;&lt;表示远小于)</p>
<img src="/images/机器学习/监督学习-支持向量机图1.png" width=50% height=50% text-align=center/>

<p>上面这张图，x为正样本，o为负样本。中间的直线是分割超平面。上面图中，我们可以知道对于样本A，我们有很强的信心认为\(y&#x3D;1\)。然后对于C却接近边界，如果稍微改变分割超平面，就可能导致\(y&#x3D;0\)。因此，越远离分割超平面的样本，我们越有信息获得他的分类。</p>
<h2 id="2-函数化与几何间隔"><a href="#2-函数化与几何间隔" class="headerlink" title="2 函数化与几何间隔"></a>2 函数化与几何间隔</h2><p>区别与之前的逻辑回归。引入支持向量机，我们需要重新定义符号。对于SVM问题, \(y \in \{  - 1,1\} \),定义函数为:</p>
$${h_{w,b}}(x) = g({w^T}x + b)$$

<p>对于\(z &#x3D; {w^T}x + b &gt; 0\), 有\({h _ \theta }(x) &#x3D; 1\)。 否则\({h _ \theta }(x) &#x3D; -1\)。</p>
<p>函数化几何间隔，有如下公式:</p>

$${\hat \gamma ^{(i)}} = {y^{(i)}}({w^T}x + b)$$
	

<p>上式很容易理解，对于正样本\(y&#x3D;1\),对于负样本\(y&#x3D;-1\)。因此可以得到\({\hat \gamma ^{(i)}}\)为一个正值。我们可以就可以定义其为几何间隔，可以理解为距离。根据上一节的分析，一个超平面对样本分割的好坏，取决于距离超平面最近的样本。因为我们定义最小间隔如下：</p>

$$\hat \gamma  = {\min _{i = 1,...,m}}{\hat \gamma ^{(i)}}$$

<img src="/images/机器学习/监督学习-支持向量机图2.png" width=50% height=50% text-align=center/>

<p>根据上图，我们知道向量\(w\)与超平面\({w^T}x + b &#x3D; 0\)垂直。</p>
<blockquote>
<p>在超平面找任意两点做直线，易证其与法向量內积为零。因此可知法向量与平面垂直。</p>
</blockquote>
<p>我们可以得知B点的坐标为\({x^{(i)}} - {r^{(i)}}\frac{w}{||w||}\)，我们将坐标代入超平面方程，有:</p>

$${w^T}({x^{(i)}} - {r^{(i)}}\frac{w}{||w||}) + b = 0$$

<p>由于知道\(||w|| &#x3D; {w^T}w\)，有：</p>

$${r^{(i)}} = {(\frac{w}{||w||})^T}{x^{(i)}} + \frac{b}{||w||}$$

<p>上面公式对应于正样本，对于负样本则需要加一个符号。因此我们可以综合得到如下公式:</p>

$${r^{(i)}} = {y^{(i)}}({(\frac{w}{||w||})^T}{x^{(i)}} + \frac{b}{||w||})$$

<p>对于公式\({\hat \gamma ^{(i)}} &#x3D; {y^{(i)}}({w^T}x + b)\)中，\(||w|| &#x3D; 1\)的时候为几何间隔。上面的式子实际上就是定义了几何间隔。我们通过定义了向量\(w\)的尺度，这样不会因为选择w比例的不同而产生不同间隔的问题。</p>
<h2 id="3-最优间隔分类器"><a href="#3-最优间隔分类器" class="headerlink" title="3 最优间隔分类器"></a>3 最优间隔分类器</h2><blockquote>
<p>以下假定我们样本数据是严格线性可分的，否则通过间隔最大化来计算就毫无意义了。</p>
</blockquote>
<p>根据前面的分析，我们只需要找到超平面，使得间隔\(\hat \gamma \)最大即可。因此我们可以得到如下最优化问题:</p>
$$\begin{gathered}   & {\max _{w,b}}\gamma  \hfill \\\  s.t.: & {y^{(i)}}({w^T}{x^{(i)}} + b) \geqslant \gamma , & i = 1,...,m \hfill \\\   & ||w|| = 1 \hfill \\\ \end{gathered} $$

<p>由于\(||w|| - 1 &#x3D; 0\)不是凸函数，所以需要转化问题。<br>注&gt; 为什么凸函数如此重要？因为对于一个凸优化问题，认为局部极小值点必为全局极小值点。对\(f(\lambda x + (1 - \lambda )y) \leqslant \lambda f(x) + (1 - \lambda )f(y)\),其中\(0 \leqslant \lambda  \leqslant 1\),则\(f(x)\)为凸函数。可以理解为类似于\(y &#x3D; {x^2}\)的图形。然而对于\(||w|| - 1 &#x3D; 0\)，具体为\(\sqrt {w _1^2 + w _2^2 + … + w _n^2}  &#x3D; 1\)，二维的情景可以理解为一个圆，三维的话则为一个球。几何图形中，可以发现对于球或圆的上半部分正好与凸函数相反，因此不是凸函数。可以代入公式证明。<br>进一步转化问题，如下：</p>

$$\begin{gathered}   & {\max _{w,b}}\frac{\gamma }{||w||} \hfill \\\  s.t.: & {y^{(i)}}({w^T}{x^{(i)}} + b) \geqslant \gamma , & i = 1,...,m \hfill \\\\end{gathered} $$

<p>我们知道\(\gamma \)大小，取决于\(w\)和\(b\)的尺度，但是\(w\)和\(b\)的尺度的改变不会影响分配效果。因此我们固定\(\gamma \)为1。将问题转化为:</p>

$$\begin{gathered}   & {\min _{w,b}}\frac{1}{2}||w|{|^2} \hfill \\  s.t.: & {y^{(i)}}({w^T}{x^{(i)}} + b) \geqslant 1, & i = 1,...,m \hfill \\ \end{gathered}$$

<p>事实上对于这个问题，我们可以换一种几何意义的解释。如下图所示，我们需要找到\({w^T}x + b &#x3D; 1\)和\({w^T}x + b &#x3D; -1\)能够有效分割样本，并且保证是两个超平面之间间隔最大，即使\(\frac{2}{||w||}\)最大，也就意味着使\(\frac{1}{2}||w|{|^2}\)最小。同样我们可以得到上面的优化问题。</p>
<img src="/images/机器学习/监督学习-支持向量机图3.png" width=50% height=50% text-align=center/>

<p>另外，我们距离超平面最近的点，即\(\gamma  &#x3D; 1\)的点，我们称之为支持向量。</p>
<h2 id="4-拉格朗日对偶问题"><a href="#4-拉格朗日对偶问题" class="headerlink" title="4 拉格朗日对偶问题"></a>4 拉格朗日对偶问题</h2><p>在对计算上一级得到的优化问题之前，我们介绍一下拉格朗日对偶问题与KKT条件，以便于更容易解决问题。考虑一个通用的优化问题，如下：</p>
$$\begin{gathered}   & {\min _x}f(x) \hfill \\\  s.t.: & {g _i}(x) \leqslant 0, & i = 1,...,k \hfill \\\   & {h _j}(x) = 0, & j = 1,...,l \hfill \\\ \end{gathered} $$


<p>然后得到拉格朗日函数，如下：</p>
$$L(x,\alpha ,\beta ) = f(x) + \sum\limits _{i = 1}^k {{\alpha _i}{g _i}(x)}  + \sum\limits _{j = 1}^l {{\beta _i}{h _i}(x)} $$$${\alpha _i} \geqslant 0$$

<p>对于我们的原始问题如何用拉格朗日函数表达呢？我们知道上面的拉格朗日后面两项的最大值为零。因此我们就可以将原始问题转化为以\({\alpha _i}\)和\({\beta _i}\)为参数情况下，求拉格朗日函数的最大值。具体转化为如下形式:</p>

$${\theta _P}(x) = {\max _{\alpha ,\beta ;{\alpha _i} \geqslant 0}}L(x,\alpha ,\beta )$$

<p>然后我们上面的式子，关于x取极小值，就与目标问题一致了。原始为的最优解最终可以通过如下方式表述:</p>

$${p^*} = {\min _x}{\theta _P}(x) = {\min _x}{\max _{\alpha ,\beta ;{\alpha _i} \geqslant 0}}L(x,\alpha ,\beta )$$

<p>我们这里可以写出其对偶问题：</p>
$${\theta _D}(\alpha ,\beta ) = {\min _x}L(x,\alpha ,\beta )$$


<p>对偶问题的最优解如下：</p>
$${d^*} = {\max _{\alpha ,\beta ;{\alpha _i} \geqslant 0}}{\theta _D}(\alpha ,\beta ) = {\max _{\alpha ,\beta ;{\alpha _i} \geqslant 0}}{\min _x}L(x,\alpha ,\beta )$$

<p>我们知道maxmin&lt;minmax,所以我们得到如下式子:</p>
$${d^*} = {\max _{\alpha ,\beta ;{\alpha _i} \geqslant 0}}{\min _x}L(x,\alpha ,\beta ) \leqslant {\min _x}{\max _{\alpha ,\beta ;{\alpha _i} \geqslant 0}}L(x,\alpha ,\beta ) = {p^\*}$$

<p>对于上面的式子只要\(f _i\)和\(g _i\)为凸函数，\(h _i\)为仿射函数，即可使等式两端相等。</p>
<blockquote>
<p>仿射函数为线性函数加截距。<br>另外对于该问题的最优值必满足KKT条件。另外，上述拉格朗日函数对相应参数需要取得极值。最终得到如下条件:</p>
</blockquote>

$$\frac{\partial }{{\partial {x _i}}}L({x^*},{\alpha^*},{\beta^*}) = 0$$
$$\frac{\partial }{{\partial \beta }}L({x^*},{\alpha ^*},{\beta ^*}) = 0$$$${\alpha ^*}{g _i}(x) = 0$$
$${g _i}(x) \leqslant 0$$$${\alpha ^*} \geqslant 0$$

<blockquote>
<p>KKT条件的原理暂时不深入，目前处于应用阶段，有时间再考虑具体原理。</p>
</blockquote>
<h1 id="5-最优边界分类器"><a href="#5-最优边界分类器" class="headerlink" title="5 最优边界分类器"></a>5 最优边界分类器</h1><p>将我们的优化问题转化为如下标准型：</p>
$$\begin{gathered}   & {\min _{w,b}}\frac{1}{2}||w|{|^2} \hfill \\\  s.t.: &  - {y^{(i)}}({w^T}{x^{(i)}} + b) + 1 \leqslant 0, & i = 1,...,n \hfill \\\\end{gathered} $$

<p>根据前面的说明，我们可以通过解对偶问题最优解来获得该问题的最优解。<br>首先写出拉格朗日对偶函数：</p>
$$L(w,b,\alpha ) = \frac{1}{2}||w|{|^2} - \sum\limits _{i = 1}^m {{\alpha _i}({y^{(i)}}({w^T}{x^{(i)}} + b) - 1)} $$

<p>参数上一节的公式，这里\(w\)和\(b\)对应于\(x\)。我们需要关于\(w\)，\(b\)求极小值，然后求得的极值点代入拉格朗日函数，然后求转化后的拉格朗日函数的极大值即可。</p>
$$\frac{\partial }{{\partial w}}L(w,b,\alpha ) = w - \sum\limits _{i = 1}^m {{\alpha _i}{y^{(i)}}{x^{(i)}}}  = 0$$$$\frac{\partial }{{\partial b}}L(w,b,\alpha ) = \sum\limits _{i = 1}^m {{\alpha _i}{y^{(i)}}}  = 0$$

<p>得到\(w &#x3D; \sum\limits _{i &#x3D; 1}^m {\alpha _i}{y^{(i)}{x^{(i)}}} \)，\(\sum\limits _{i &#x3D; 1}^m {\alpha _i}{y^{(i)}}  &#x3D; 0\)。然后将其代入拉格朗日函数：</p>

$$L(\alpha ) = \frac{1}{2}{w^T}w - \sum\limits _{i = 1}^m {{\alpha _i}({y^{(i)}}({w^T}{x^{(i)}} + b) - 1)} $$
$$L(\alpha ) =  - \frac{1}{2}{(\sum\limits _{i = 1}^m {{\alpha _i}{y^{(i)}}{x^{(i)}}} )^T}(\sum\limits _{i = 1}^m {{\alpha _i}{y^{(i)}}{x^{(i)}}} ) + \sum\limits _{i = 1}^m {{\alpha _i}} $$


<p>由于\({\alpha _i}\)和\({y^{(i)}}\)为变量，实际上面的式子就是一个\((Ax + By + Cz)(Ax + By + Cz)\)的问题。因此可以归纳为如下公式：</p>
$$L(\alpha ) =  - \frac{1}{2}\sum\limits _{i = 1}^m {\sum\limits _{j = 1}^m {{\alpha _i}{\alpha _j}{y^{(i)}}{y^{(j)}} < {x^{(i)}},{x^{(j)}} > } }  + \sum\limits _{i = 1}^m {{\alpha _i}} $$

<p>问题就转化为：</p>

$$\begin{gathered}   & {\max _\alpha }L(\alpha ) =  - \frac{1}{2}\sum\limits _{i = 1}^m {\sum\limits _{j = 1}^m {{\alpha _i}{\alpha _j}{y^{(i)}}{y^{(j)}} < {x^{(i)}},{x^{(j)}} > } }  + \sum\limits _{i = 1}^m {{\alpha _i}}  \hfill \\\  s.t.: & {\alpha _i} \geqslant 0 \hfill \\\   & \sum\limits _{i = 1}^n {{\alpha _i}{y^{(i)}}}  = 0 \hfill \\\\end{gathered} $$

<p>由于我们知道支持向量的间隔必须为1，因此我们可以根据其计算\(b\)。设支持向量的集合为S,对属于结合S的样本有\({y^{(i)}}({w^T}{x^{(i)}} + b) &#x3D; 1\)。由于\(w &#x3D; \sum\limits _{i &#x3D; 1}^m {\alpha _i}{y^{(i)}}{x^{(i)}} \)，又由于对所有的非支持向量，有\({\alpha _i} &#x3D; 0\)。因此我们可以综合均值得到：</p>
$$b = \frac{1}{{|S|}}\sum\limits _{i = 1}^S {({y^{(i)}} - \sum\limits _{j = 1}^S {{\alpha _j}{y^{(j)}} < {x^{(i)}},{x^{(j)}} > } )} $$


<h2 id="6-正则化"><a href="#6-正则化" class="headerlink" title="6 正则化"></a>6 正则化</h2><p>关于之前的问题我们假定样本严格可分。但是实际上需要容忍一些误差。因此我们将公式修正为如下形式(C为常数)：</p>

$$\begin{gathered}   & {\min _{w,b,\xi }}\frac{1}{2}||w|{|^2} + C\sum\limits _{i = 1}^m {{\xi _i}}  \hfill \\\  s.t.: & {y^{(i)}}({w^T}{x^{(i)}} + b) \geqslant 1 - {\xi _i} &  & i = 1,...,m \hfill \\\   & {\xi _i} \geqslant 0 &  &  &  & i = 1,...,m \hfill \\\\end{gathered} $$

<p>我们允许\({y^{(i)}}({w^T}{x^{(i)}} + b)\)小于1，但是不希望小太多。所以，我们需要保证\({\xi _i}\)的总和尽可能小，因此得上面的式子。然后我们可以得到拉格朗日公式如下：</p>
$$L(w,b,\xi ,\alpha ,r) = \frac{1}{2}||w|{|^2} + C\sum\limits _{i = 1}^m {{\xi _i}}  - \sum\limits _{i = 1}^m {{\alpha _i}({y^{(i)}}({w^T}{x^{(i)}} + b) - 1 + {\xi _i})}  - \sum\limits _{i = 1}^m {{r _i}{\xi _i}} $$


<p>对\(w\)，\(b\)，\(\xi \), 求导得:</p>
$$\frac{\partial }{{\partial w}}L(w,b,\xi ,\alpha ,r) = w - \sum\limits _{i = 1}^m {{\alpha _i}{y^{(i)}}{x^{(i)}}}  = 0$$
$$\frac{\partial }{{\partial b}}L(w,b,\xi ,\alpha ,r) =  - \sum\limits _{i = 1}^m {{\alpha _i}{y^{(i)}}}  = 0$$
$$\frac{\partial }{{\partial {\xi _i}}}L(w,b,\xi ,\alpha ,r) = C - {r _i} - {\alpha _i} = 0$$

<p>可以得到\(w &#x3D; \sum\limits _{i &#x3D; 1}^m {\alpha _i}{y^{(i)}}{x^{(i)}}\),\(\sum\limits _{i &#x3D; 1}^m {\alpha _i}{y^{(i)}} &#x3D; 0\),\(C &#x3D; {r _i} + {\alpha _i}\)。其中我们知道\({r _i} \geqslant 0\)，所以也可得到\(0 \leqslant {\alpha _i} \leqslant C\)。带入拉格朗日函数得:</p>
$$L(\alpha ) =  - \frac{1}{2}\sum\limits _{i = 1}^m {\sum\limits _{j = 1}^m {{\alpha _i}{\alpha _j}{y^{(i)}}{y^{(j)}} < {x^{(i)}},{x^{(j)}} > } }  + \sum\limits _{i = 1}^m {({r _i} + {\alpha _i}){\xi _i}}  - \sum\limits _{i = 1}^m {{\alpha _i}({\xi _i}}  - 1) - \sum\limits _{i = 1}^m {{r _i}{\xi _i}} $$

<p>我们对上面的式子乘以-1，转化为求最小值的问题，可以得到最终的优化问题：</p>
$$\begin{gathered}   & \min L(\alpha ) = \min \frac{1}{2}\sum\limits _{i = 1}^m {\sum\limits _{j = 1}^m {{\alpha _i}{\alpha _j}{y^{(i)}}{y^{(j)}} < {x^{(i)}},{x^{(j)}} > } }  - \sum\limits _{i = 1}^m {{\alpha _i}}  \hfill \\\  s.t.: & \sum\limits _{i = 1}^m {{\alpha _i}{y^{(i)}}}  = 0 \hfill \\\   & 0 \leqslant {\alpha _i} \leqslant C \hfill \\\\end{gathered} $$


<p>可以得到如下的KKT条件：</p>

$${\alpha _i},{r _i} \geqslant 0$$$${y^{(i)}}({w^T}{x^{(i)}} + b) - 1 + {\xi _i} \geqslant 0$$$${\alpha _i}({y^{(i)}}({w^T}{x^{(i)}} + b) - 1 + {\xi _i}) = 0$$$${\xi _i} \geqslant 0,{r _i}{\xi _i} = 0$$

<p>对于上述KKT条件我们可以转换为如下形式：</p>
$${y^{(i)}}({w^T}{x^{(i)}} + b) \geqslant 1 , {\alpha _i} = 0$$
$${y^{(i)}}({w^T}{x^{(i)}} + b) \leqslant 1 , {\alpha _i} = C$$
$${y^{(i)}}({w^T}{x^{(i)}} + b) = 1 , 0 \leqslant {\alpha _i} \leqslant C$$

<p>很容易转化上面的式子，三个条件分别表示在比支持向量距离分割超平面远的样本，比支持向量距离分割超平面近的可容忍的误差样本，支持向量对应的样本。</p>
<h2 id="7-SMO算法理论"><a href="#7-SMO算法理论" class="headerlink" title="7 SMO算法理论"></a>7 SMO算法理论</h2><p>这一节使用SMO算法解决上一节归纳出来的优化问题。<br>SMO算法的思想来自于坐标上升算法，坐标上升算法的主要思想是一次遍历一个变量，然后把其他变量当做是常亮，进在一个维度上优化。<br>然后对于我们之前的问题，有\(\sum\limits _{i &#x3D; 1}^m {\alpha _i}{y^{(i)}}  &#x3D; 0\)。设置我们设\(\zeta  &#x3D; {\alpha _1}{y^{(1)}} + {\alpha _2}{y^{(2)}} &#x3D; \sum\limits _{i &#x3D; 3}^m {\alpha _i}{y^{(i)}} \)，将\({\alpha _1}\)用\({\alpha _2}\)表达，然后得到关于\({\alpha _2}\)的二次函数，这样很容易取得极值。当所有样本满足KKT条件，且无法继续增加，我们就可以认为此刻取得最优值。</p>
<p>由于我们知道\(0 \leqslant {\alpha _i} \leqslant C\)，所以我们可以求的其取值范围，我们可以将二维变量表述为一个方格内，具体如下：</p>
<img src="/images/机器学习/监督学习-支持向量机图4.png" width=50% height=50% text-align=center/>

<p>最多四种情况代入，经过求截距等一系列操作，可以将的取值范围归纳为下面的公式，其中L表示上界，H表示上界。<br>同号时有：</p>
$$L = \max (0,\alpha _1^{old} + \alpha _2^{old} - C)$$
$$H = \min (C,\alpha _1^{old} + \alpha _2^{old})$$


<p>异号的时候有：</p>
$$L = \max (0,\alpha _2^{old} - \alpha _2^{old})$$
$$H = \min (C,C + \alpha _2^{old} - \alpha _1^{old})$$

<p>进一步求解二次规划问题：</p>

$$\psi ({\alpha _1},{\alpha _2}) = \frac{1}{2}\alpha _1^2k(1,1) + \frac{1}{2}\alpha _2^2k(2,2) + {y^{(1)}}{y^{(2)}}{\alpha _1}{\alpha _2}k(1,2) - {\alpha _1} - {\alpha _2} + {y^{(1)}}{\alpha _1}{v _1} + {y^{(2)}}{\alpha _2}{v _2} + M$$

<p>上式中\(k(i,j) &#x3D;  &lt; {x _i},{x _j} &gt; \)具体是核函数的简写，下节会介绍。\(M\)为与\(\alpha _1\),\(\alpha _2\)无关的参数。另外，\({v _1} &#x3D; \sum\limits _{i &#x3D; 3}^m {\alpha _i}{y^{(i)}}k(1,i) \),\({v _2} &#x3D; \sum\limits _{i &#x3D; 3}^m {\alpha _i}{y^{(i)}}k(2,i) \)。</p>
<p>我们设\(\zeta  &#x3D; {\alpha _1}{y^{(1)}} + {\alpha _2}{y^{(2)}}\)，所以有\({\alpha _1} &#x3D; {y^{(1)}}(\zeta  - {\alpha _2}{y^{(2)}})\),代入上式:</p>

$$\psi ({\alpha _2}) = \frac{1}{2}{(\zeta  - {\alpha _2}{y^{(2)}})^2}k(1,1) + \frac{1}{2}\alpha _2^2k(2,2) + {y^{(2)}}(\zeta  - {\alpha _2}{y^{(2)}}){\alpha _2}k(1,2) - (\zeta  - {\alpha _2}{y^{(2)}}){y^{(1)}} - {\alpha _2} + (\zeta  - {\alpha _2}{y^{(2)}}){v _1} + {y^{(2)}}{\alpha _2}{v _2} + M$$

<p>然后求导数:</p>
$$\frac{\partial }{{\partial {\alpha _2}}}\psi ({\alpha _2}) = ({\alpha _2}{y^{(2)}} - \zeta ){y^{(2)}}k(1,1) + {\alpha _2}k(2,2) + {y^{(2)}}\zeta k(1,2) - 2{\alpha _2}k(1,2) + {y^{(1)}}{y^{(2)}} - {y^{(1)}}{y^{(2)}} - {y^{(2)}}{v _1} + {y^{(2)}}{v _2} = 0$$$$\frac{\partial }{{\partial {\alpha _2}}}\psi ({\alpha _2}) = {\alpha _2}(k(1,1) + k(1,2) - 2k(1,2)) - \zeta {y^{(2)}}k(1,1) + \zeta {y^{(2)}}k(1,2) + {y^{(1)}}{y^{(2)}} - {y^{(1)}}{y^{(2)}} - {y^{(1)}}{v _1} + {y^{(2)}}{v _2} = 0$$

<p>我们设\(\eta  &#x3D; k(1,1) + k(1,2) - 2k(1,2)\)，对\(\eta\)大于0的情况，导数为0的极值点就是极小值。对于\(\eta\)小于等于0的情况，最小值点肯定取自于边界，我们需要比较函数在\(L\)和\(H\)的大小。</p>
<p>让我们继续简化上面的式子，对于\(\eta\)大于0的情况下，取得极值。由于我们轻易得到下面的关系。</p>

$${v_1} = \sum\limits _{i = 1}^m {{\alpha _i}{y^{(2)}}k(1,i)}  + b - b - \sum\limits _{i = 1}^2 {{\alpha _i}{y^{(2)}}k(1,i)}  = f({x^{(1)}}) - b - \sum\limits _{i = 1}^2 {{\alpha _i}{y^{(2)}}k(1,i)} $$

$${v_2} = \sum\limits _{i = 1}^m {{\alpha _i}{y^{(2)}}k(2,i)}  + b - b - \sum\limits _{i = 1}^2 {{\alpha _i}{y^{(2)}}k(2,i)}  = f({x^{(2)}}) - b - \sum\limits _{i = 1}^2 {{\alpha _i}{y^{(2)}}k(2,i)} $$

<p>将上面的关系代入的如下极值：</p>
$$\alpha _2^{new} = \frac{{\zeta {y^{(2)}}k(1,1) - \zeta {y^{(2)}}k(1,2) + {y^{(2)}}({y^{(1)}} - f({x^{(1)}}) - ({y^{(2)}} - f({x^{(2)}})) - {y^{(2)}}\sum\limits _{i = 1}^2 {\alpha _i^{old}{y^{(2)}}k(1,i)}  + {y^{(2)}}\sum\limits _{i = 1}^2 {\alpha _i^{old}{y^{(2)}}k(2,i)} }}{\eta }$$


<p>我们将\(\zeta  &#x3D; {\alpha _1}{y^{(1)}} + {\alpha _2}{y^{(2)}}\)代入上式，得:</p>
$$\alpha _2^{new} = \frac{{{\alpha _1}{y^{(1)}}{y^{(2)}}k(1,1) + {\alpha _2}k(1,1) - {\alpha _1}{y^{(1)}}{y^{(2)}}k(1,2) - {\alpha _2}k(1,2) - {\alpha _1}{y^{(1)}}{y^{(2)}}k(1,1) - {\alpha _2}k(1,2) + {\alpha _2}{y^{(1)}}{y^{(2)}}k(1,2) + {\alpha _2}k(2,2) + {y^{(2)}}({y^{(1)}} - f({x^{(1)}}) - ({y^{(2)}} - f({x^{(2)}}))}}{\eta }$$

<p>约掉部分选项，有:</p>
$$\alpha _2^{new} = \alpha _2^{old} + \frac{{{y^{(2)}}({y^{(1)}} - f({x^{(1)}}) - ({y^{(2)}} - f({x^{(2)}})))}}{\eta }$$
$$\alpha _2^{new} = \alpha _2^{old} + \frac{{{y^{(2)}}({e _1} - {e _2})}}{\eta }$$

<p>接下来就只剩下求\(b\)的问题了，根据上一节最后转化的KKT条件。对\(0 \leqslant {\alpha _1} \leqslant C\)的情况下，有\({y^{(1)}}({w^T}{x^{(1)}} + b) &#x3D; 1\)。所以有：</p>

$$b = {y^{(1)}} - \sum\limits _{i = 1}^m {{\alpha _i}} {y^{(i)}}k(1,i)$$


<p>进一步展开有：</p>

$$b = {y^{(1)}} - \sum\limits _{i = 3}^m {\alpha _i^{new}{y^{(i)}}k(1,i)}  - \alpha _1^{new}{y^{(1)}}k(1,1) - \alpha _2^{new}{y^{(2)}}k(1,2)$$


<p>我们知道上面的式子中\({\alpha _3}\)到\({\alpha _m}\)并没有发生变化，因此有：</p>

$$\sum\limits _{i = 3}^m {\alpha _i^{new}{y^{(i)}}k(1,i)}  = \sum\limits _{i = 3}^m {\alpha _i^{old}{y^{(i)}}k(1,i)}  = \sum\limits _{i = 1}^m {\alpha _i^{old}{y^{(i)}}k(1,i)}  - \alpha _1^{old}{y^{(1)}}k(1,1) - \alpha _2^{old}{y^{(2)}}k(1,2) = f({x^{(1)}}) - b - \alpha _1^{old}{y^{(1)}}k(1,1) - \alpha _2^{old}{y^{(2)}}k(1,2)$$

<p>代入如上式得:</p>
$${b^{new}} =  - {e_1} + (\alpha _1^{old} - \alpha _1^{new}){y^{(1)}}k(1,1) + (\alpha _1^{old} - \alpha _1^{new}){y^{(2)}}k(1,2) + {b^{old}}$$


<p>对于\({\alpha _2}\)有同样的道理。</p>
<p>综上，假如\(0 \leqslant {\alpha _1} \leqslant C\)，有:</p>
$${b^{new}} =  - {e_1} + (\alpha _1^{old} - \alpha _1^{new}){y^{(1)}}k(1,1) + (\alpha _1^{old} - \alpha _1^{new}){y^{(2)}}k(1,2) + {b^{old}}$$


<p>假如\(0 \leqslant {\alpha _2} \leqslant C\)，有：</p>
$${b^{new}} =  - {e_2} + (\alpha _2^{old} - \alpha _2^{new}){y^{(1)}}k(2,1) + (\alpha _2^{old} - \alpha _2^{new}){y^{(2)}}k(2,2) + {b^{old}}$$


<p>假如\(0 \leqslant {\alpha _1},{\alpha _2} \leqslant C\),事实上上面两个公司的出来的结果是一样的，因此不用特殊计算。<br>如果不满足在\(0\)和\(C\)的范围，则去两个公式的中间值。(笔者认为没有必要更新)</p>
<h2 id="8-SMO算法实践"><a href="#8-SMO算法实践" class="headerlink" title="8 SMO算法实践"></a>8 SMO算法实践</h2><p>在SMO论文中有具体的伪代码，算法的主要逻辑就是要保证每个样本都满足KKT条件，且直到所有\(\alpha \)达到极值，即不需要更新为止。</p>
<p>然后是关于\(\alpha \)的选择，第一个\(\alpha \)我们可以随机选择一个违反KKT条件的，第二个我们选择能够最大程度更新\(\alpha \)的值，看上一节的公式，实际会选择\(|e _1-e _2|\)最大的样本点作为第二个\(\alpha \)。具体逻辑可以参考SMO的论文，或者下面代码的注释。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.lines <span class="keyword">import</span> Line2D</span><br><span class="line"></span><br><span class="line"><span class="keyword">global</span> g_y_vec    <span class="comment"># g_y_vec为样本输出, 大小g_m</span></span><br><span class="line"><span class="keyword">global</span> g_x_mat    <span class="comment"># g_x_mat为样本的输入, 大小2*g_m</span></span><br><span class="line"><span class="keyword">global</span> g_m        <span class="comment"># g_m 为样本数目</span></span><br><span class="line"><span class="keyword">global</span> g_alpha    <span class="comment"># g_alpha 大小为g_m</span></span><br><span class="line"><span class="keyword">global</span> g_C</span><br><span class="line"><span class="keyword">global</span> g_w        <span class="comment"># 实际没有使用，而是通过alpha表示的</span></span><br><span class="line"><span class="keyword">global</span> g_b        <span class="comment"># y = wx+b</span></span><br><span class="line"><span class="keyword">global</span> g_y_now    <span class="comment"># 表示当前参数计算的y，即svmOutPut对应的g_y_now</span></span><br><span class="line"><span class="keyword">global</span> g_err      <span class="comment"># g_err 表示svmOutput - g_y_vec对应的序列</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># g_arr0 means classify of -1</span></span><br><span class="line">g_arr0 = np.array([[ <span class="number">0.88235916</span> , <span class="number">1.01511634</span>],[ <span class="number">0.75243817</span> , <span class="number">0.76520033</span>],[ <span class="number">0.95710848</span> , <span class="number">1.41894337</span>],[ <span class="number">1.48682891</span> , <span class="number">0.78885043</span>],[ <span class="number">1.24047011</span> , <span class="number">0.71984948</span>],[ <span class="number">0.67611276</span> , <span class="number">1.07909452</span>],[ <span class="number">1.03243669</span> , <span class="number">1.08929695</span>],[ <span class="number">1.0296548</span>  , <span class="number">1.25023769</span>],[ <span class="number">1.54134008</span> , <span class="number">0.39564824</span>],[ <span class="number">0.34645057</span> , <span class="number">1.61499636</span>],[ <span class="number">0.77206174</span> , <span class="number">1.23613698</span>],[ <span class="number">0.91446988</span> , <span class="number">1.38537765</span>],[ <span class="number">0.99982962</span> , <span class="number">1.34448471</span>],[ <span class="number">0.78745962</span> , <span class="number">0.9046565</span> ],[ <span class="number">0.74946602</span> , <span class="number">1.07424473</span>],[ <span class="number">1.09294839</span> , <span class="number">1.14711993</span>],[ <span class="number">0.39266844</span> , <span class="number">0.78788004</span>],[ <span class="number">0.83112357</span> , <span class="number">1.2762774</span> ],[ <span class="number">1.05056188</span> , <span class="number">1.13351562</span>],[ <span class="number">1.62101523</span> , <span class="number">1.15035562</span>],[ <span class="number">0.70377517</span> , <span class="number">1.1136416</span> ],[ <span class="number">1.03715472</span> , <span class="number">0.47905693</span>],[ <span class="number">0.94598381</span> , <span class="number">0.8874837</span> ],[ <span class="number">0.94447128</span> , <span class="number">2.02796925</span>],[ <span class="number">0.72442242</span> , <span class="number">1.09835206</span>],[ <span class="number">0.69046731</span> , <span class="number">1.46232182</span>],[ <span class="number">1.20744606</span> , <span class="number">1.10280041</span>],[ <span class="number">0.70665746</span> , <span class="number">0.82139503</span>],[ <span class="number">1.08803887</span> , <span class="number">1.4450361</span> ],[ <span class="number">0.88530961</span> , <span class="number">0.75727475</span>],[ <span class="number">0.98418545</span> , <span class="number">0.80248161</span>],[ <span class="number">0.74970386</span> , <span class="number">1.13205709</span>],[ <span class="number">0.72586454</span> , <span class="number">1.06058385</span>],[ <span class="number">0.9071812</span>  , <span class="number">1.09975063</span>],[ <span class="number">0.75182835</span> , <span class="number">0.93570147</span>],[ <span class="number">0.80052289</span> , <span class="number">1.08168507</span>],[ <span class="number">0.40180652</span> , <span class="number">0.9526211</span> ],[ <span class="number">0.62312617</span> , <span class="number">0.84385058</span>],[ <span class="number">0.68212516</span> , <span class="number">1.25912717</span>],[ <span class="number">1.19773245</span> , <span class="number">0.16399654</span>],[ <span class="number">0.96093132</span> , <span class="number">0.43932091</span>],[ <span class="number">1.25471657</span> , <span class="number">0.92371829</span>],[ <span class="number">1.12330272</span> , <span class="number">1.26968747</span>],[ <span class="number">1.30361985</span> , <span class="number">0.99862123</span>],[ <span class="number">1.23477665</span> , <span class="number">1.1742804</span> ],[ <span class="number">0.28471876</span> , <span class="number">0.5806044</span> ],[ <span class="number">1.89355099</span> , <span class="number">1.19928671</span>],[ <span class="number">1.09081369</span> , <span class="number">1.28467312</span>],[ <span class="number">1.40488635</span> , <span class="number">0.90034427</span>],[ <span class="number">1.11672364</span> , <span class="number">1.49070515</span>],[ <span class="number">1.35385212</span> , <span class="number">1.35767891</span>],[ <span class="number">0.92746374</span> , <span class="number">1.79096697</span>],[ <span class="number">1.89142562</span> , <span class="number">0.98228303</span>],[ <span class="number">1.0555218</span>  , <span class="number">0.86070833</span>],[ <span class="number">0.69001255</span> , <span class="number">1.12874741</span>],[ <span class="number">0.98137315</span> , <span class="number">1.3398852</span> ],[ <span class="number">1.02525371</span> , <span class="number">0.77572865</span>],[ <span class="number">1.1354295</span>  , <span class="number">1.07098552</span>],[ <span class="number">1.50829164</span> , <span class="number">1.43065998</span>],[ <span class="number">1.09928764</span> , <span class="number">1.55540292</span>],[ <span class="number">0.64695084</span> , <span class="number">0.79920395</span>],[ <span class="number">0.82059034</span> , <span class="number">0.97533491</span>],[ <span class="number">0.56345455</span> , <span class="number">1.08168272</span>],[ <span class="number">1.06673215</span> , <span class="number">1.19448556</span>],[ <span class="number">0.96512548</span> , <span class="number">1.5268577</span> ],[ <span class="number">0.96914451</span> , <span class="number">1.00902985</span>],[ <span class="number">0.72879413</span> , <span class="number">0.92476415</span>],[ <span class="number">1.0931483</span>  , <span class="number">1.13572242</span>],[ <span class="number">1.34765121</span> , <span class="number">0.83841006</span>],[ <span class="number">1.57813788</span> , <span class="number">0.65915892</span>],[ <span class="number">0.59032608</span> , <span class="number">0.82747946</span>],[ <span class="number">0.83838504</span> , <span class="number">0.67588473</span>],[ <span class="number">1.35101322</span> , <span class="number">1.21027851</span>],[ <span class="number">0.71762153</span> , <span class="number">0.41839038</span>],[ <span class="number">0.61295604</span> , <span class="number">0.66555018</span>],[ <span class="number">0.64379346</span> , <span class="number">0.92925228</span>],[ <span class="number">1.1194968</span>  , <span class="number">0.65876736</span>],[ <span class="number">0.39495437</span> , <span class="number">0.67246734</span>],[ <span class="number">1.05223282</span> , <span class="number">0.17889116</span>],[ <span class="number">0.97810984</span> , <span class="number">1.12794664</span>],[ <span class="number">0.98392719</span> , <span class="number">0.73590255</span>],[ <span class="number">1.25587405</span> , <span class="number">1.21853038</span>],[ <span class="number">1.01150226</span> , <span class="number">1.01835571</span>],[ <span class="number">1.02251614</span> , <span class="number">0.72704228</span>],[ <span class="number">1.00261519</span> , <span class="number">0.95347185</span>],[ <span class="number">0.96362523</span> , <span class="number">0.8607009</span> ],[ <span class="number">0.88034659</span> , <span class="number">1.2307104</span> ],[ <span class="number">0.75907236</span> , <span class="number">0.92799796</span>],[ <span class="number">0.54898709</span> , <span class="number">1.69882285</span>],[ <span class="number">0.55032649</span> , <span class="number">0.98831566</span>],[ <span class="number">1.33360789</span> , <span class="number">1.19793298</span>],[ <span class="number">0.83231239</span> , <span class="number">0.8946538</span> ],[ <span class="number">1.05173094</span> , <span class="number">1.26324289</span>],[ <span class="number">0.81482231</span> , <span class="number">0.56198584</span>],[ <span class="number">1.03854797</span> , <span class="number">1.0553811</span> ],[ <span class="number">1.32669227</span> , <span class="number">1.61115811</span>],[ <span class="number">1.13322152</span> , <span class="number">1.68151695</span>],[ <span class="number">0.39754618</span> , <span class="number">1.19392967</span>],[ <span class="number">0.61344185</span> , <span class="number">1.05281434</span>],[ <span class="number">1.18415366</span> , <span class="number">0.864884</span>  ]])</span><br><span class="line"><span class="comment"># g_arr1 means classify of +1</span></span><br><span class="line">g_arr1 = np.array([[ <span class="number">2.15366548</span> , <span class="number">1.88035458</span>],[ <span class="number">2.36978774</span> , <span class="number">1.76550283</span>],[ <span class="number">2.46261387</span> , <span class="number">2.10568262</span>],[ <span class="number">1.90475526</span> , <span class="number">1.95242885</span>],[ <span class="number">1.77712677</span> , <span class="number">1.96004856</span>],[ <span class="number">1.5995514</span>  , <span class="number">2.1323943</span> ],[ <span class="number">1.52727223</span> , <span class="number">1.50295551</span>],[ <span class="number">1.80330407</span> , <span class="number">1.57942301</span>],[ <span class="number">1.86487049</span> , <span class="number">1.87234414</span>],[ <span class="number">1.9586354</span>  , <span class="number">1.96279729</span>],[ <span class="number">2.59668134</span> , <span class="number">2.414423</span>  ],[ <span class="number">2.818419</span>   , <span class="number">1.76280366</span>],[ <span class="number">2.01511628</span> , <span class="number">2.10858546</span>],[ <span class="number">2.15907962</span> , <span class="number">1.81593012</span>],[ <span class="number">1.63966834</span> , <span class="number">2.2209023</span> ],[ <span class="number">2.47220599</span> , <span class="number">1.70482956</span>],[ <span class="number">2.08760748</span> , <span class="number">2.51601971</span>],[ <span class="number">1.50547722</span> , <span class="number">1.8487145</span> ],[ <span class="number">1.68125583</span> , <span class="number">2.64968501</span>],[ <span class="number">2.01924282</span> , <span class="number">2.0953572</span> ],[ <span class="number">2.22563534</span> , <span class="number">2.18266325</span>],[ <span class="number">2.2684291</span>  , <span class="number">2.23581599</span>],[ <span class="number">2.13787557</span> , <span class="number">1.9999382</span> ],[ <span class="number">1.02638695</span> , <span class="number">1.68134967</span>],[ <span class="number">2.35614619</span> , <span class="number">1.32072125</span>],[ <span class="number">2.20054871</span> , <span class="number">1.47401445</span>],[ <span class="number">1.99454827</span> , <span class="number">1.71658741</span>],[ <span class="number">1.83269065</span> , <span class="number">2.47662909</span>],[ <span class="number">2.40097251</span> , <span class="number">2.21823862</span>],[ <span class="number">2.54404652</span> , <span class="number">1.85742018</span>],[ <span class="number">1.84150027</span> , <span class="number">2.06350351</span>],[ <span class="number">1.69490855</span> , <span class="number">1.70169334</span>],[ <span class="number">1.44745704</span> , <span class="number">1.88295233</span>],[ <span class="number">2.24376639</span> , <span class="number">1.67530495</span>],[ <span class="number">1.42911921</span> , <span class="number">1.81854548</span>],[ <span class="number">1.33789289</span> , <span class="number">2.27686128</span>],[ <span class="number">2.43509821</span> , <span class="number">1.95032131</span>],[ <span class="number">1.9512447</span>  , <span class="number">1.4595415</span> ],[ <span class="number">2.13041192</span> , <span class="number">1.79372755</span>],[ <span class="number">2.2753866</span>  , <span class="number">2.23781951</span>],[ <span class="number">2.26753401</span> , <span class="number">1.78149305</span>],[ <span class="number">2.06505449</span> , <span class="number">2.01939606</span>],[ <span class="number">2.44426826</span> , <span class="number">2.1437101</span> ],[ <span class="number">2.16607141</span> , <span class="number">2.31077167</span>],[ <span class="number">1.96097237</span> , <span class="number">2.49100193</span>],[ <span class="number">1.37255424</span> , <span class="number">1.60735016</span>],[ <span class="number">1.63947758</span> , <span class="number">2.17852314</span>],[ <span class="number">2.13722666</span> , <span class="number">2.00559707</span>],[ <span class="number">1.222696</span>   , <span class="number">1.67075059</span>],[ <span class="number">2.56982685</span> , <span class="number">2.51218813</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcLH</span>(<span class="params">id1,id2</span>):</span><br><span class="line">  <span class="keyword">if</span> g_y_vec[id1] == g_y_vec[id2]:</span><br><span class="line">    L = <span class="built_in">max</span>(<span class="number">0</span>,g_alpha[id1]+g_alpha[id2]-g_C)</span><br><span class="line">    H = <span class="built_in">min</span>(g_C,g_alpha[id1]+g_alpha[id2])</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    L = <span class="built_in">max</span>(<span class="number">0</span>,g_alpha[id2]-g_alpha[id1])</span><br><span class="line">    H = <span class="built_in">min</span>(g_C,g_C+g_alpha[id2]-g_alpha[id1])</span><br><span class="line">  <span class="keyword">return</span> (L,H)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">svmOutput</span>(<span class="params">id1</span>):</span><br><span class="line">  <span class="comment"># 这个函数是svm的实际输出，计算当前参数(w,b)下, 计算得到的y</span></span><br><span class="line">  <span class="comment"># 由于w = sum (alpha*y*x), 对于第i个分量x_i所以输出结果应该为w*x_i+b, 也就是 sum (alpha*y*&lt;x_1-m,x_i&gt;)</span></span><br><span class="line">  <span class="comment"># 注: 待会分析下alpha的变化趋势与C的关系</span></span><br><span class="line">  <span class="keyword">global</span> g_b</span><br><span class="line">  <span class="built_in">sum</span> = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">    <span class="built_in">sum</span> += g_alpha[i]*g_y_vec[i]*kernel(i,id1)</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">sum</span>+g_b</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kernel</span>(<span class="params">id1,id2</span>):</span><br><span class="line">  <span class="comment"># 这里核函数为简单的内积</span></span><br><span class="line">  <span class="comment"># id1和id2为int类型，是g_x_mat中的索引</span></span><br><span class="line">  <span class="keyword">return</span> np.dot(g_x_mat[id1],g_x_mat[id2])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compareFun</span>(<span class="params">id1,id2,L,H</span>):</span><br><span class="line">  <span class="comment"># 如果返回1，表示L处去极小值。如果返回-1，H处去极小值。如果是0，表示这次不更新</span></span><br><span class="line">  <span class="comment"># 如果两者相等，这里略过, 说明无法有强力证据证明这个样本属于wx+b&gt;1还是wx+b&lt;1，所以等待下一轮迭代。因此，与L和H相等应该设置一个阀值，判断近似相等。</span></span><br><span class="line">  <span class="keyword">global</span> g_b</span><br><span class="line">  y_1 = g_y_vec[id1]</span><br><span class="line">  y_2 = g_y_vec[id2]</span><br><span class="line">  e_1 = g_err[id1]</span><br><span class="line">  e_2 = g_err[id2]</span><br><span class="line">  alpha_1 = g_alpha[id1]</span><br><span class="line">  alpha_2 = g_alpha[id2]</span><br><span class="line">  k11 = kernel(id1,id1)</span><br><span class="line">  k12 = kernel(id1,id2)</span><br><span class="line">  k22 = kernel(id2,id2)</span><br><span class="line">  s = y_1*y_2</span><br><span class="line">  f_1 = y_1*(e_1+g_b)-alpha_1*k11-s*alpha_2*k12</span><br><span class="line">  f_2 = y_2*(e_2+g_b)-s*alpha_1*k12-alpha_2*k22</span><br><span class="line">  L_1 = alpha_1+s*(alpha_2-L)</span><br><span class="line">  H_1 = alpha_1+s*(alpha_2-H)</span><br><span class="line">  phi_l = L_1*f_1+L*f_2+<span class="number">0.5</span>*L_1*L_1*k11+<span class="number">0.5</span>*L*L*k22+s*L*L_1*k12</span><br><span class="line">  phi_h = H_1*f_1+H*f_2+<span class="number">0.5</span>*H_1*H_1*k11+<span class="number">0.5</span>*H*H*k22+s*H*H_1*k12</span><br><span class="line">  <span class="keyword">if</span> L==H:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  <span class="keyword">if</span> phi_l &lt; phi_h:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">takeStep</span>(<span class="params">id1,id2,err</span>):</span><br><span class="line">  <span class="keyword">if</span> id1==id2:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  alpha_1 = g_alpha[id1]</span><br><span class="line">  alpha_2 = g_alpha[id2]</span><br><span class="line">  y_1 = g_y_vec[id1]</span><br><span class="line">  y_2 = g_y_vec[id2]</span><br><span class="line">  e1 = g_err[id1]</span><br><span class="line">  e2 = g_err[id2]</span><br><span class="line">  s = y_1*y_2</span><br><span class="line">  L,H=calcLH(id1,id2)</span><br><span class="line">  <span class="comment">#print(&quot;id1=&quot;,id1,&quot;, id2=&quot;,id2,&quot; L=&quot;,L,&quot;, H=&quot;,H)</span></span><br><span class="line">  <span class="keyword">if</span> L==H :</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  k11 = kernel(id1,id1)</span><br><span class="line">  k12 = kernel(id1,id2)</span><br><span class="line">  k22 = kernel(id2,id2)</span><br><span class="line">  eta = k11+k22-<span class="number">2</span>*k12</span><br><span class="line">  <span class="comment">#print(&quot;kernel:&quot;,k11,k12,k22,&quot;, eta=&quot;,eta,&quot;e1=&quot;,e1,&quot;e2=&quot;,e2)</span></span><br><span class="line"></span><br><span class="line">  alpha_2_new = alpha_2</span><br><span class="line">  <span class="comment"># 如果eta大于0, 我们可知最小值在边界或极小值点上。事实上，如果极小值不在范围内，必在距离极小值近的那个边界上。</span></span><br><span class="line">  <span class="comment"># 如果eta小于0, 我们可知最小值则必在边界上。我们只需要比较两个边界点函数的大小即可。</span></span><br><span class="line">  <span class="keyword">if</span> eta&gt;<span class="number">0</span> :</span><br><span class="line">    alpha_2_new = alpha_2+y_2*(e1-e2)/eta</span><br><span class="line">    alpha_2_new = <span class="built_in">max</span>(alpha_2_new,L)</span><br><span class="line">    alpha_2_new = <span class="built_in">min</span>(alpha_2_new,H)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># 由于我们知道可以直接这是一个关于alpha的二次函数，并且自变量的取值范围是[L,H], 事实上我们只需要比较alpha_2_new离L和H哪个远即可</span></span><br><span class="line">    <span class="comment"># 但是考虑到eta=0的一次函数特殊情况，我们还是老老实实的计算函数值吧。</span></span><br><span class="line">    ret = compareFun(id1, id2, L, H)</span><br><span class="line">    <span class="keyword">if</span> ret == <span class="number">0</span>:</span><br><span class="line">      alpha_2_new = alpha_2</span><br><span class="line">    <span class="keyword">elif</span> ret == <span class="number">1</span>:</span><br><span class="line">      alpha_2_new = L</span><br><span class="line">    <span class="keyword">elif</span> ret == <span class="number">1</span>:</span><br><span class="line">      alpha_2_new = H</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;----------------eta&lt;=0----------------&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 归整化alpha_2_new</span></span><br><span class="line">  <span class="keyword">if</span> alpha_2_new &lt; err:</span><br><span class="line">    alpha_2_new = <span class="number">0</span></span><br><span class="line">  <span class="keyword">elif</span> alpha_2_new &gt; g_C - err:</span><br><span class="line">    alpha_2_new = g_C</span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">abs</span>(alpha_2_new-alpha_2) &lt; err:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;alpha_2 is no need to update&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># update b</span></span><br><span class="line">  <span class="keyword">global</span> g_b</span><br><span class="line">  alpha_1_new = alpha_1 + s * (alpha_2 - alpha_2_new)      <span class="comment"># 不必担心alpha_1_new不在[0,C]范围内，之前的公式已经保证了</span></span><br><span class="line">  b1_new = -e1-y_1*k11*(alpha_1_new-alpha_1)-y_2*k12*(alpha_2_new-alpha_2) + g_b</span><br><span class="line">  b2_new = -e2-y_1*k12*(alpha_1_new-alpha_1)-y_2*k22*(alpha_2_new-alpha_2) + g_b</span><br><span class="line">  <span class="keyword">if</span> alpha_1_new&gt;<span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> alpha_1_new&lt;g_C:</span><br><span class="line">    g_b = b1_new</span><br><span class="line">  <span class="keyword">elif</span> alpha_2_new&gt;<span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> alpha_2_new&lt;g_C:</span><br><span class="line">    g_b = b2_new</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    g_b = <span class="number">0.5</span>*(b1_new+b2_new)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># update alpha</span></span><br><span class="line">  g_alpha[id1] = alpha_1_new</span><br><span class="line">  g_alpha[id2] = alpha_2_new</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;g_alpha[id1]=&quot;</span>,g_alpha[id1],<span class="string">&quot;g_alpha[id2]=&quot;</span>,g_alpha[id2],<span class="string">&quot;s=&quot;</span>,s,<span class="string">&quot;, alpha_1=&quot;</span>,alpha_1,<span class="string">&quot;, alpha_2=&quot;</span>,alpha_2)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># update err g_y_now</span></span><br><span class="line">  updateYAndErr()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updateYAndErr</span>():</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">    g_y_now[i] = svmOutput(i)</span><br><span class="line">    g_err[i] = svmOutput(i)-g_y_vec[i]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chooseBestAlphaIndex</span>(<span class="params">id2</span>):</span><br><span class="line">  maxIncr = <span class="number">0</span></span><br><span class="line">  maxIndex = -<span class="number">1</span>;</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">    incr = <span class="built_in">abs</span>(g_err[i]-g_alpha[i])</span><br><span class="line">    <span class="keyword">if</span> incr &gt;= maxIncr:</span><br><span class="line">      maxIndex = i</span><br><span class="line">      maxIncr = incr</span><br><span class="line">  <span class="keyword">return</span> maxIndex</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sizeOfNonZerorAndNonC</span>():</span><br><span class="line">  size=<span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">    <span class="keyword">if</span> g_alpha[i]!=<span class="number">0</span> <span class="keyword">and</span> g_alpha[i]!=g_C:</span><br><span class="line">      size= size+<span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> size</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chooseRandomIndex</span>(<span class="params">id2</span>):</span><br><span class="line">  ret = id2;</span><br><span class="line">  <span class="keyword">while</span> ret==id2:</span><br><span class="line">    ret = random.randint(<span class="number">0</span>,g_m-<span class="number">1</span>)</span><br><span class="line">  <span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于examineExample函数,我们一次进选择id2样本对应的alpha与如下规则选择的id1对应的alpha,然后相应跟新其值。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">examineExample</span>(<span class="params">id2</span>):</span><br><span class="line">  y_2 = g_y_vec[id2]</span><br><span class="line">  tol = <span class="number">1e-2</span>        <span class="comment"># 是一个正数</span></span><br><span class="line">  alpha_2 = g_alpha[id2]</span><br><span class="line">  e_2 = svmOutput(id2)-g_y_vec[id2]</span><br><span class="line">  r_2 = e_2 * y_2</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 我们只对当前违反kkt条件的样本对应的alpha进行更新</span></span><br><span class="line">  <span class="comment"># 关于违反kkt条件的说明:</span></span><br><span class="line">  <span class="comment"># (1) r_2 &lt; -tol 表示r_2小于0，即表示输出的预测结果与样本的y符号相反，因此应该属于误差案例的。所以，根据公式，alpha = C。如果alpha &lt; C ,比违反kkt条件</span></span><br><span class="line">  <span class="comment"># (2) r_2 &lt; -tol 表示r_2大于0，即表示输出的预测结果与样本的y符号相同，当误差大于一定的</span></span><br><span class="line">  <span class="keyword">if</span> r_2 &lt; -tol <span class="keyword">and</span> alpha_2 &lt; g_C <span class="keyword">or</span> r_2 &gt; tol <span class="keyword">and</span> alpha_2 &gt; <span class="number">0</span> :</span><br><span class="line">    <span class="comment"># 下面的程序逻辑是这样的:</span></span><br><span class="line">    <span class="comment"># 先遍历alpha非0或非C, 因为我们对于alpha为0和alpha为C的情况, 认为是处于非支持向量和处于误差样本的情况。我们只有根据支持向量下，找到最优的||w||才有意义</span></span><br><span class="line">    <span class="comment"># 首先，我们找到|e1-e2|最大的alpha, 从这里优化。如果优化结果不理想，我们就随机找一个alpha一起计算。如果还不行，就在整个范围alpha范围内计算</span></span><br><span class="line">    <span class="keyword">if</span> sizeOfNonZerorAndNonC()&gt;<span class="number">0</span>:</span><br><span class="line">      id1=chooseBestAlphaIndex(id2)</span><br><span class="line">      <span class="keyword">if</span> takeStep(id1,id2,<span class="number">1e-3</span>):</span><br><span class="line">        <span class="comment">#print(&quot;takeStep1, alpha[&quot;,id1,&quot;]=&quot;,g_alpha[id1],&quot;, alphapp[&quot;,id2,&quot;]=&quot;,g_alpha[id2])</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    r=chooseRandomIndex(id2)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">      id1 = (r+i)%g_m</span><br><span class="line">      <span class="keyword">if</span> id1!=id2 <span class="keyword">and</span> g_alpha[id1]!=<span class="number">0</span> <span class="keyword">and</span> g_alpha[id1]!=g_C:</span><br><span class="line">        <span class="keyword">if</span> takeStep(id1,id2,<span class="number">1e-3</span>):</span><br><span class="line">          <span class="comment">#print(&quot;takeStep2, alpha[&quot;, id1, &quot;]=&quot;, g_alpha[id1], &quot;, alphapp[&quot;, id2, &quot;]=&quot;, g_alpha[id2])</span></span><br><span class="line">          <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    r = chooseRandomIndex(id2)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">      id1 = (r + i) % g_m</span><br><span class="line">      <span class="keyword">if</span> id1 != id2:</span><br><span class="line">        <span class="keyword">if</span> takeStep(id1,id2,<span class="number">1e-3</span>):</span><br><span class="line">          <span class="comment">#print(&quot;takeStep3, alpha[&quot;, id1, &quot;]=&quot;, g_alpha[id1], &quot;, alphapp[&quot;, id2, &quot;]=&quot;, g_alpha[id2])</span></span><br><span class="line">          <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">showPic</span>(<span class="params">w,b</span>):</span><br><span class="line">  <span class="comment"># draw wx+b, x1为横轴，x2为纵轴</span></span><br><span class="line">  k = -w[<span class="number">0</span>]/w[<span class="number">1</span>]</span><br><span class="line">  b = -g_b/w[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">  figure, ax = plt.subplots()</span><br><span class="line">  ax.set_xlim(left=-<span class="number">1</span>, right=<span class="number">4</span>)</span><br><span class="line">  ax.set_ylim(bottom=-<span class="number">1</span>, top=<span class="number">4</span>)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">    x = g_x_mat[i]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">abs</span>(g_alpha[i]-<span class="number">0</span>)&lt;<span class="number">1e-3</span>:</span><br><span class="line">      plt.plot(x[<span class="number">0</span>], x[<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;+&#x27;</span>, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">abs</span>(g_alpha[i]-g_C)&lt;<span class="number">1e-3</span>:</span><br><span class="line">      plt.plot(x[<span class="number">0</span>], x[<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, color=<span class="string">&#x27;g&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      plt.plot(x[<span class="number">0</span>], x[<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, color=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">  <span class="comment"># draw y = -x+3</span></span><br><span class="line">  (line1_xs, line1_ys) = [(<span class="number">0</span>, <span class="number">3</span>), (b, <span class="number">3</span>*k+b)]</span><br><span class="line">  ax.add_line(Line2D(line1_xs, line1_ys, linewidth=<span class="number">1</span>, color=<span class="string">&#x27;blue&#x27;</span>))</span><br><span class="line">  plt.xlabel(<span class="string">&quot;x1&quot;</span>)</span><br><span class="line">  plt.ylabel(<span class="string">&quot;x2&quot;</span>)</span><br><span class="line">  plt.plot()</span><br><span class="line">  plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">  <span class="comment"># 0 make data</span></span><br><span class="line">  g_y_vec = np.array([])</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(g_arr0)):</span><br><span class="line">    g_y_vec=np.append(g_y_vec,-<span class="number">1</span>)</span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(g_arr1)):</span><br><span class="line">    g_y_vec=np.append(g_y_vec,+<span class="number">1</span>)</span><br><span class="line">  g_x_mat = np.vstack((g_arr0,g_arr1))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 1 training</span></span><br><span class="line">  g_m = <span class="built_in">len</span>(g_x_mat)</span><br><span class="line">  g_alpha = np.zeros(g_m)</span><br><span class="line">  g_y_now = np.zeros(g_m)</span><br><span class="line">  g_err = np.zeros(g_m)</span><br><span class="line">  <span class="keyword">global</span> g_b</span><br><span class="line">  g_b = <span class="number">0</span></span><br><span class="line">  numChanged = <span class="number">0</span></span><br><span class="line">  examineAll = <span class="number">1</span></span><br><span class="line">  g_C = <span class="number">10</span></span><br><span class="line">  err = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  updateYAndErr()       <span class="comment"># 实现更新下缓冲，即当前输出与误差值</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> numChanged&gt;<span class="number">0</span> <span class="keyword">or</span> examineAll:</span><br><span class="line">    numChanged = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 循环处理，第一次对所有的样本进行一次处理。</span></span><br><span class="line">    <span class="comment"># 然后对所有非边界的数值进行处理。因为在当前参数下，非边界的样本，我们认为其是支持向量。对于优化||w||的大小，支持向量才有意义。</span></span><br><span class="line">    <span class="keyword">if</span> examineAll:</span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">        numChanged += examineExample(i)</span><br><span class="line">        <span class="comment">#print(&quot;examineAll=1, numChanged=&quot;,numChanged)</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">        <span class="keyword">if</span> g_alpha[i]!=<span class="number">0</span> <span class="keyword">and</span> g_alpha[i]!=g_C:</span><br><span class="line">          numChanged += examineExample(i)</span><br><span class="line">    examineAll = <span class="built_in">abs</span>(examineAll-<span class="number">1</span>)</span><br><span class="line">  <span class="comment">#print(g_alpha)</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 3 show</span></span><br><span class="line">  <span class="comment"># 计算w</span></span><br><span class="line">  w = np.array([<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">    <span class="keyword">if</span> g_alpha[i]!=<span class="number">0</span>:</span><br><span class="line">      w=np.add(w,g_y_vec[i]*g_alpha[i]*g_x_mat[i])</span><br><span class="line"></span><br><span class="line">  showPic(w,g_b)</span><br><span class="line">  <span class="comment">#print(g_alpha)</span></span><br></pre></td></tr></table></figure>

<p>经过数轮迭代得到如下结果，其中直线为得到的分割曲线，蓝色点为支持向量，红色点是那些有良好分类的样本，绿色点为可容忍的误差样本。</p>
<img src="/images/机器学习/监督学习-支持向量机计算结果1.png" width=50% height=50% text-align=center/>


<h2 id="9-核函数"><a href="#9-核函数" class="headerlink" title="9 核函数"></a>9 核函数</h2><p>SMO另个非常强大的地方上，它能够很好的解决非线性问题。我们之前的公式中有\(&lt;{x _i},{x _j}&gt;\)，他是两个向量的內积，代表着两个样本的相关性。我们把这个叫做核函数。核函数不仅仅是可以为简单的內积，还可以对样本进行多维展开，映射到高维地址空间。这样在低维地址空间线性不可分的样本，在高维空间就变得线性可分了。譬如,\(|x| &lt; 1\)不是线性可分的，但是对其进行\(x \to (x,{x^2})\)映射后，也就的到一个二次曲线，我们可以使用\(y &#x3D; 1\)进行线性分割。</p>
<p>下面直接引入高斯核函数，它可以对函数进行无线维空间的映射。具体定义如下：</p>
$$k(x,z) = \exp ( - \frac{{||x - z|{|^2}}}{{2{\sigma ^2}}})$$

<blockquote>
<p>对于核函数的数学理论和几何意义，以及高斯核为啥可以向无限维空间映射，之后有时间需要详细研究。</p>
</blockquote>
<p>我们制造一组\(x _1^2 + x _2^2 &#x3D; 1\)分割的样本，然后尝试对其进行分割，实际上与上一节程序的区别仅仅在于核函数的选取，这里我们使用高斯核函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.patches <span class="keyword">import</span> Circle</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">global</span> g_y_vec    <span class="comment"># g_y_vec为样本输出, 大小g_m</span></span><br><span class="line"><span class="keyword">global</span> g_x_mat    <span class="comment"># g_x_mat为样本的输入, 大小2*g_m</span></span><br><span class="line"><span class="keyword">global</span> g_m        <span class="comment"># g_m 为样本数目</span></span><br><span class="line"><span class="keyword">global</span> g_alpha    <span class="comment"># g_alpha 大小为g_m</span></span><br><span class="line"><span class="keyword">global</span> g_C</span><br><span class="line"><span class="keyword">global</span> g_w        <span class="comment"># 实际没有使用，而是通过alpha表示的</span></span><br><span class="line"><span class="keyword">global</span> g_b        <span class="comment"># y = wx+b</span></span><br><span class="line"><span class="keyword">global</span> g_y_now    <span class="comment"># 表示当前参数计算的y，即svmOutPut对应的g_y_now</span></span><br><span class="line"><span class="keyword">global</span> g_err      <span class="comment"># g_err 表示svmOutput - g_y_vec对应的序列</span></span><br><span class="line"></span><br><span class="line">g_x_mat = np.array([[<span class="number">0.602</span>, <span class="number">0.732</span>], [-<span class="number">0.599</span>, <span class="number">0.945</span>], [-<span class="number">0.337</span>, -<span class="number">0.677</span>], [<span class="number">0.459</span>, -<span class="number">0.486</span>], [<span class="number">1.056</span>, <span class="number">0.137</span>], [<span class="number">0.838</span>, <span class="number">0.623</span>], [<span class="number">1.022</span>, -<span class="number">0.685</span>], [<span class="number">0.504</span>, <span class="number">0.821</span>], [-<span class="number">0.977</span>, -<span class="number">0.724</span>], [<span class="number">1.116</span>, <span class="number">0.56</span>], [<span class="number">0.969</span>, <span class="number">0.151</span>], [-<span class="number">0.693</span>, <span class="number">0.077</span>], [<span class="number">1.042</span>, -<span class="number">0.146</span>], [<span class="number">0.705</span>, -<span class="number">0.215</span>], [<span class="number">1.024</span>, -<span class="number">0.322</span>], [<span class="number">1.025</span>, -<span class="number">0.172</span>], [<span class="number">0.306</span>, -<span class="number">1.12</span>], [-<span class="number">0.131</span>, <span class="number">0.008</span>], [-<span class="number">1.157</span>, -<span class="number">1.081</span>], [<span class="number">0.452</span>, -<span class="number">0.865</span>], [-<span class="number">1.117</span>, -<span class="number">0.533</span>], [-<span class="number">1.083</span>, -<span class="number">0.355</span>], [-<span class="number">0.982</span>, <span class="number">0.572</span>], [-<span class="number">1.053</span>, <span class="number">1.003</span>], [-<span class="number">0.553</span>, -<span class="number">0.434</span>], [-<span class="number">0.115</span>, <span class="number">0.283</span>], [<span class="number">0.785</span>, <span class="number">0.233</span>], [-<span class="number">0.926</span>, -<span class="number">0.299</span>], [-<span class="number">1.039</span>, <span class="number">0.581</span>], [<span class="number">0.869</span>, -<span class="number">1.033</span>], [<span class="number">0.754</span>, -<span class="number">1.091</span>], [-<span class="number">1.096</span>, -<span class="number">0.311</span>], [<span class="number">0.537</span>, <span class="number">0.508</span>], [-<span class="number">0.38</span>, -<span class="number">0.565</span>], [<span class="number">1.165</span>, <span class="number">0.219</span>], [-<span class="number">0.123</span>, <span class="number">0.431</span>], [<span class="number">1.048</span>, -<span class="number">0.896</span>], [-<span class="number">0.409</span>, <span class="number">0.299</span>], [<span class="number">0.537</span>, -<span class="number">0.126</span>], [<span class="number">0.985</span>, -<span class="number">0.577</span>], [-<span class="number">1.135</span>, <span class="number">1.025</span>], [-<span class="number">0.779</span>, <span class="number">0.81</span>], [<span class="number">0.547</span>, <span class="number">0.697</span>], [<span class="number">0.424</span>, -<span class="number">1.015</span>], [<span class="number">0.421</span>, -<span class="number">0.904</span>], [<span class="number">0.151</span>, -<span class="number">0.149</span>], [<span class="number">0.77</span>, -<span class="number">1.011</span>], [-<span class="number">0.401</span>, <span class="number">1.113</span>], [<span class="number">0.817</span>, <span class="number">0.573</span>], [<span class="number">0.87</span>, -<span class="number">0.266</span>], [-<span class="number">0.731</span>, <span class="number">0.418</span>], [-<span class="number">0.651</span>, <span class="number">0.063</span>], [<span class="number">0.731</span>, <span class="number">0.04</span>], [<span class="number">0.649</span>, <span class="number">0.677</span>], [-<span class="number">0.084</span>, -<span class="number">0.568</span>], [<span class="number">0.391</span>, -<span class="number">0.171</span>], [-<span class="number">1.07</span>, <span class="number">0.738</span>], [-<span class="number">0.307</span>, <span class="number">0.702</span>], [<span class="number">0.854</span>, <span class="number">1.125</span>], [<span class="number">0.093</span>, -<span class="number">0.148</span>], [-<span class="number">0.82</span>, <span class="number">0.969</span>], [<span class="number">0.11</span>, -<span class="number">1.011</span>], [<span class="number">0.672</span>, -<span class="number">0.261</span>], [<span class="number">0.6</span>, -<span class="number">0.262</span>], [<span class="number">0.28</span>, <span class="number">0.001</span>], [-<span class="number">0.005</span>, -<span class="number">0.544</span>], [-<span class="number">0.666</span>, <span class="number">0.046</span>], [-<span class="number">0.457</span>, -<span class="number">0.129</span>], [-<span class="number">1.02</span>, <span class="number">1.071</span>], [<span class="number">1.191</span>, <span class="number">0.121</span>], [<span class="number">0.665</span>, -<span class="number">0.884</span>], [<span class="number">0.412</span>, <span class="number">0.665</span>], [-<span class="number">0.992</span>, -<span class="number">1.165</span>], [-<span class="number">0.726</span>, -<span class="number">1.178</span>], [-<span class="number">0.886</span>, <span class="number">1.08</span>], [<span class="number">0.263</span>, <span class="number">0.481</span>], [-<span class="number">0.051</span>, <span class="number">0.668</span>], [<span class="number">0.933</span>, -<span class="number">0.008</span>], [-<span class="number">0.896</span>, -<span class="number">0.637</span>], [-<span class="number">0.605</span>, <span class="number">0.287</span>], [<span class="number">0.03</span>, -<span class="number">0.232</span>], [<span class="number">0.749</span>, <span class="number">0.012</span>], [<span class="number">1.175</span>, <span class="number">0.632</span>], [<span class="number">0.968</span>, <span class="number">1.106</span>], [-<span class="number">1.19</span>, <span class="number">0.82</span>], [<span class="number">0.641</span>, <span class="number">0.129</span>], [-<span class="number">0.375</span>, -<span class="number">1.079</span>], [-<span class="number">0.267</span>, -<span class="number">0.442</span>], [<span class="number">0.361</span>, -<span class="number">0.741</span>], [-<span class="number">0.475</span>, <span class="number">0.473</span>], [<span class="number">0.133</span>, <span class="number">1.18</span>], [<span class="number">1.146</span>, <span class="number">1.185</span>], [-<span class="number">0.293</span>, <span class="number">0.172</span>], [<span class="number">0.78</span>, -<span class="number">0.805</span>], [<span class="number">0.186</span>, -<span class="number">0.089</span>], [-<span class="number">0.068</span>, <span class="number">0.829</span>], [-<span class="number">0.621</span>, -<span class="number">0.778</span>], [<span class="number">0.407</span>, -<span class="number">0.523</span>], [<span class="number">0.415</span>, -<span class="number">0.01</span>], [-<span class="number">0.229</span>, <span class="number">0.002</span>], [-<span class="number">0.997</span>, -<span class="number">0.891</span>], [<span class="number">1.011</span>, -<span class="number">1.186</span>], [<span class="number">0.19</span>, -<span class="number">0.437</span>], [<span class="number">0.958</span>, <span class="number">0.669</span>], [-<span class="number">0.888</span>, -<span class="number">0.217</span>], [<span class="number">0.444</span>, <span class="number">0.05</span>], [-<span class="number">0.54</span>, -<span class="number">1.041</span>], [-<span class="number">0.314</span>, <span class="number">0.296</span>], [<span class="number">0.879</span>, -<span class="number">0.898</span>], [<span class="number">0.127</span>, -<span class="number">0.008</span>], [<span class="number">0.995</span>, -<span class="number">1.11</span>], [-<span class="number">0.878</span>, -<span class="number">0.843</span>], [-<span class="number">0.109</span>, <span class="number">0.189</span>], [<span class="number">0.859</span>, <span class="number">0.564</span>], [-<span class="number">0.023</span>, <span class="number">0.945</span>], [-<span class="number">0.878</span>, <span class="number">0.899</span>], [-<span class="number">0.062</span>, -<span class="number">1.051</span>], [<span class="number">0.394</span>, <span class="number">0.519</span>], [-<span class="number">1.139</span>, <span class="number">0.282</span>], [-<span class="number">0.494</span>, -<span class="number">0.075</span>], [-<span class="number">0.922</span>, <span class="number">1.11</span>], [<span class="number">0.753</span>, -<span class="number">1.018</span>], [<span class="number">0.816</span>, -<span class="number">1.106</span>], [<span class="number">0.03</span>, <span class="number">0.569</span>], [-<span class="number">1.11</span>, -<span class="number">0.289</span>], [<span class="number">0.777</span>, <span class="number">0.025</span>], [<span class="number">0.892</span>, <span class="number">0.784</span>], [<span class="number">0.91</span>, <span class="number">0.176</span>], [<span class="number">0.692</span>, <span class="number">0.099</span>], [<span class="number">0.97</span>, <span class="number">0.58</span>], [<span class="number">0.034</span>, <span class="number">1.151</span>], [-<span class="number">0.606</span>, -<span class="number">0.775</span>], [<span class="number">0.873</span>, -<span class="number">0.579</span>], [<span class="number">0.833</span>, -<span class="number">1.042</span>], [-<span class="number">0.251</span>, <span class="number">0.102</span>], [<span class="number">0.436</span>, -<span class="number">0.585</span>], [<span class="number">0.86</span>, -<span class="number">1.06</span>], [-<span class="number">1.118</span>, <span class="number">1.094</span>], [<span class="number">0.598</span>, -<span class="number">0.129</span>], [<span class="number">0.694</span>, <span class="number">0.281</span>], [<span class="number">1.048</span>, -<span class="number">1.036</span>], [-<span class="number">0.348</span>, <span class="number">0.639</span>], [<span class="number">1.046</span>, -<span class="number">1.124</span>], [-<span class="number">0.333</span>, -<span class="number">0.463</span>], [-<span class="number">0.447</span>, -<span class="number">0.009</span>], [<span class="number">0.344</span>, -<span class="number">0.852</span>], [-<span class="number">1.174</span>, <span class="number">0.196</span>], [<span class="number">0.701</span>, <span class="number">0.695</span>], [-<span class="number">0.916</span>, -<span class="number">0.128</span>], [-<span class="number">0.597</span>, -<span class="number">0.934</span>]])</span><br><span class="line">g_y_vec = np.array([<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcLH</span>(<span class="params">id1,id2</span>):</span><br><span class="line">  <span class="keyword">if</span> g_y_vec[id1] == g_y_vec[id2]:</span><br><span class="line">    L = <span class="built_in">max</span>(<span class="number">0</span>,g_alpha[id1]+g_alpha[id2]-g_C)</span><br><span class="line">    H = <span class="built_in">min</span>(g_C,g_alpha[id1]+g_alpha[id2])</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    L = <span class="built_in">max</span>(<span class="number">0</span>,g_alpha[id2]-g_alpha[id1])</span><br><span class="line">    H = <span class="built_in">min</span>(g_C,g_C+g_alpha[id2]-g_alpha[id1])</span><br><span class="line">  <span class="keyword">return</span> (L,H)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">svmOutput</span>(<span class="params">id1</span>):</span><br><span class="line">  <span class="comment"># 这个函数是svm的实际输出，计算当前参数(w,b)下, 计算得到的y</span></span><br><span class="line">  <span class="comment"># 由于w = sum (alpha*y*x), 对于第i个分量x_i所以输出结果应该为w*x_i+b, 也就是 sum (alpha*y*&lt;x_1-m,x_i&gt;)</span></span><br><span class="line">  <span class="comment"># 注: 待会分析下alpha的变化趋势与C的关系</span></span><br><span class="line">  <span class="keyword">global</span> g_b</span><br><span class="line">  <span class="built_in">sum</span> = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">    <span class="built_in">sum</span> += g_alpha[i]*g_y_vec[i]*kernel(i,id1)</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">sum</span>+g_b</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kernel</span>(<span class="params">id1,id2</span>):</span><br><span class="line">  <span class="comment"># 这里核函数为简单的内积</span></span><br><span class="line">  <span class="comment"># id1和id2为int类型，是g_x_mat中的索引</span></span><br><span class="line">  x_1 = g_x_mat[id1]</span><br><span class="line">  x_2 = g_x_mat[id2]</span><br><span class="line">  val = np.subtract(x_1,x_2)</span><br><span class="line">  val = np.dot(val,val)</span><br><span class="line">  <span class="keyword">return</span> np.exp(-val)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compareFun</span>(<span class="params">id1,id2,L,H</span>):</span><br><span class="line">  <span class="comment"># 如果返回1，表示L处去极小值。如果返回-1，H处去极小值。如果是0，表示这次不更新</span></span><br><span class="line">  <span class="comment"># 如果两者相等，这里略过, 说明无法有强力证据证明这个样本属于wx+b&gt;1还是wx+b&lt;1，所以等待下一轮迭代。因此，与L和H相等应该设置一个阀值，判断近似相等。</span></span><br><span class="line">  <span class="keyword">global</span> g_b</span><br><span class="line">  y_1 = g_y_vec[id1]</span><br><span class="line">  y_2 = g_y_vec[id2]</span><br><span class="line">  e_1 = g_err[id1]</span><br><span class="line">  e_2 = g_err[id2]</span><br><span class="line">  alpha_1 = g_alpha[id1]</span><br><span class="line">  alpha_2 = g_alpha[id2]</span><br><span class="line">  k11 = kernel(id1,id1)</span><br><span class="line">  k12 = kernel(id1,id2)</span><br><span class="line">  k22 = kernel(id2,id2)</span><br><span class="line">  s = y_1*y_2</span><br><span class="line">  f_1 = y_1*(e_1+g_b)-alpha_1*k11-s*alpha_2*k12</span><br><span class="line">  f_2 = y_2*(e_2+g_b)-s*alpha_1*k12-alpha_2*k22</span><br><span class="line">  L_1 = alpha_1+s*(alpha_2-L)</span><br><span class="line">  H_1 = alpha_1+s*(alpha_2-H)</span><br><span class="line">  phi_l = L_1*f_1+L*f_2+<span class="number">0.5</span>*L_1*L_1*k11+<span class="number">0.5</span>*L*L*k22+s*L*L_1*k12</span><br><span class="line">  phi_h = H_1*f_1+H*f_2+<span class="number">0.5</span>*H_1*H_1*k11+<span class="number">0.5</span>*H*H*k22+s*H*H_1*k12</span><br><span class="line">  <span class="keyword">if</span> L==H:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  <span class="keyword">if</span> phi_l &lt; phi_h:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">takeStep</span>(<span class="params">id1,id2,err</span>):</span><br><span class="line">  <span class="keyword">if</span> id1==id2:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  alpha_1 = g_alpha[id1]</span><br><span class="line">  alpha_2 = g_alpha[id2]</span><br><span class="line">  y_1 = g_y_vec[id1]</span><br><span class="line">  y_2 = g_y_vec[id2]</span><br><span class="line">  e1 = g_err[id1]</span><br><span class="line">  e2 = g_err[id2]</span><br><span class="line">  s = y_1*y_2</span><br><span class="line">  L,H=calcLH(id1,id2)</span><br><span class="line">  <span class="comment">#print(&quot;id1=&quot;,id1,&quot;, id2=&quot;,id2,&quot; L=&quot;,L,&quot;, H=&quot;,H)</span></span><br><span class="line">  <span class="keyword">if</span> L==H :</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  k11 = kernel(id1,id1)</span><br><span class="line">  k12 = kernel(id1,id2)</span><br><span class="line">  k22 = kernel(id2,id2)</span><br><span class="line">  eta = k11+k22-<span class="number">2</span>*k12</span><br><span class="line">  <span class="comment">#print(&quot;kernel:&quot;,k11,k12,k22,&quot;, eta=&quot;,eta,&quot;e1=&quot;,e1,&quot;e2=&quot;,e2)</span></span><br><span class="line"></span><br><span class="line">  alpha_2_new = alpha_2</span><br><span class="line">  <span class="comment"># 如果eta大于0, 我们可知最小值在边界或极小值点上。事实上，如果极小值不在范围内，必在距离极小值近的那个边界上。</span></span><br><span class="line">  <span class="comment"># 如果eta小于0, 我们可知最小值则必在边界上。我们只需要比较两个边界点函数的大小即可。</span></span><br><span class="line">  <span class="keyword">if</span> eta&gt;<span class="number">0</span> :</span><br><span class="line">    alpha_2_new = alpha_2+y_2*(e1-e2)/eta</span><br><span class="line">    alpha_2_new = <span class="built_in">max</span>(alpha_2_new,L)</span><br><span class="line">    alpha_2_new = <span class="built_in">min</span>(alpha_2_new,H)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># 由于我们知道可以直接这是一个关于alpha的二次函数，并且自变量的取值范围是[L,H], 事实上我们只需要比较alpha_2_new离L和H哪个远即可</span></span><br><span class="line">    <span class="comment"># 但是考虑到eta=0的一次函数特殊情况，我们还是老老实实的计算函数值吧。</span></span><br><span class="line">    ret = compareFun(id1, id2, L, H)</span><br><span class="line">    <span class="keyword">if</span> ret == <span class="number">0</span>:</span><br><span class="line">      alpha_2_new = alpha_2</span><br><span class="line">    <span class="keyword">elif</span> ret == <span class="number">1</span>:</span><br><span class="line">      alpha_2_new = L</span><br><span class="line">    <span class="keyword">elif</span> ret == <span class="number">1</span>:</span><br><span class="line">      alpha_2_new = H</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;----------------eta&lt;=0----------------&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 归整化alpha_2_new</span></span><br><span class="line">  <span class="keyword">if</span> alpha_2_new &lt; err:</span><br><span class="line">    alpha_2_new = <span class="number">0</span></span><br><span class="line">  <span class="keyword">elif</span> alpha_2_new &gt; g_C - err:</span><br><span class="line">    alpha_2_new = g_C</span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">abs</span>(alpha_2_new-alpha_2) &lt; err:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;alpha_2 is no need to update&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># update b</span></span><br><span class="line">  <span class="keyword">global</span> g_b</span><br><span class="line">  alpha_1_new = alpha_1 + s * (alpha_2 - alpha_2_new)      <span class="comment"># 不必担心alpha_1_new不在[0,C]范围内，之前的公式已经保证了</span></span><br><span class="line">  b1_new = -e1-y_1*k11*(alpha_1_new-alpha_1)-y_2*k12*(alpha_2_new-alpha_2) + g_b</span><br><span class="line">  b2_new = -e2-y_1*k12*(alpha_1_new-alpha_1)-y_2*k22*(alpha_2_new-alpha_2) + g_b</span><br><span class="line">  <span class="keyword">if</span> alpha_1_new&gt;<span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> alpha_1_new&lt;g_C:</span><br><span class="line">    g_b = b1_new</span><br><span class="line">  <span class="keyword">elif</span> alpha_2_new&gt;<span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> alpha_2_new&lt;g_C:</span><br><span class="line">    g_b = b2_new</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    g_b = <span class="number">0.5</span>*(b1_new+b2_new)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># update alpha</span></span><br><span class="line">  g_alpha[id1] = alpha_1_new</span><br><span class="line">  g_alpha[id2] = alpha_2_new</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;g_alpha[id1]=&quot;</span>,g_alpha[id1],<span class="string">&quot;g_alpha[id2]=&quot;</span>,g_alpha[id2],<span class="string">&quot;s=&quot;</span>,s,<span class="string">&quot;, alpha_1=&quot;</span>,alpha_1,<span class="string">&quot;, alpha_2=&quot;</span>,alpha_2)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># update err g_y_now</span></span><br><span class="line">  updateYAndErr()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updateYAndErr</span>():</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">    g_y_now[i] = svmOutput(i)</span><br><span class="line">    g_err[i] = svmOutput(i)-g_y_vec[i]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chooseBestAlphaIndex</span>(<span class="params">id2</span>):</span><br><span class="line">  maxIncr = <span class="number">0</span></span><br><span class="line">  maxIndex = -<span class="number">1</span>;</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">    incr = <span class="built_in">abs</span>(g_err[i]-g_alpha[i])</span><br><span class="line">    <span class="keyword">if</span> incr &gt;= maxIncr:</span><br><span class="line">      maxIndex = i</span><br><span class="line">      maxIncr = incr</span><br><span class="line">  <span class="keyword">return</span> maxIndex</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sizeOfNonZerorAndNonC</span>():</span><br><span class="line">  size=<span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">    <span class="keyword">if</span> g_alpha[i]!=<span class="number">0</span> <span class="keyword">and</span> g_alpha[i]!=g_C:</span><br><span class="line">      size= size+<span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> size</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chooseRandomIndex</span>(<span class="params">id2</span>):</span><br><span class="line">  ret = id2;</span><br><span class="line">  <span class="keyword">while</span> ret==id2:</span><br><span class="line">    ret = random.randint(<span class="number">0</span>,g_m-<span class="number">1</span>)</span><br><span class="line">  <span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于examineExample函数,我们一次进选择id2样本对应的alpha与如下规则选择的id1对应的alpha,然后相应跟新其值。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">examineExample</span>(<span class="params">id2</span>):</span><br><span class="line">  y_2 = g_y_vec[id2]</span><br><span class="line">  tol = <span class="number">1e-2</span>        <span class="comment"># 是一个正数</span></span><br><span class="line">  alpha_2 = g_alpha[id2]</span><br><span class="line">  e_2 = svmOutput(id2)-g_y_vec[id2]</span><br><span class="line">  r_2 = e_2 * y_2</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 我们只对当前违反kkt条件的样本对应的alpha进行更新</span></span><br><span class="line">  <span class="comment"># 关于违反kkt条件的说明:</span></span><br><span class="line">  <span class="comment"># (1) r_2 &lt; -tol 表示r_2小于0，即表示输出的预测结果与样本的y符号相反，因此应该属于误差案例的。所以，根据公式，alpha = C。如果alpha &lt; C ,比违反kkt条件</span></span><br><span class="line">  <span class="comment"># (2) r_2 &lt; -tol 表示r_2大于0，即表示输出的预测结果与样本的y符号相同，当误差大于一定的</span></span><br><span class="line">  <span class="keyword">if</span> r_2 &lt; -tol <span class="keyword">and</span> alpha_2 &lt; g_C <span class="keyword">or</span> r_2 &gt; tol <span class="keyword">and</span> alpha_2 &gt; <span class="number">0</span> :</span><br><span class="line">    <span class="comment"># 下面的程序逻辑是这样的:</span></span><br><span class="line">    <span class="comment"># 先遍历alpha非0或非C, 因为我们对于alpha为0和alpha为C的情况, 认为是处于非支持向量和处于误差样本的情况。我们只有根据支持向量下，找到最优的||w||才有意义</span></span><br><span class="line">    <span class="comment"># 首先，我们找到|e1-e2|最大的alpha, 从这里优化。如果优化结果不理想，我们就随机找一个alpha一起计算。如果还不行，就在整个范围alpha范围内计算</span></span><br><span class="line">    <span class="keyword">if</span> sizeOfNonZerorAndNonC()&gt;<span class="number">0</span>:</span><br><span class="line">      id1=chooseBestAlphaIndex(id2)</span><br><span class="line">      <span class="keyword">if</span> takeStep(id1,id2,<span class="number">1e-3</span>):</span><br><span class="line">        <span class="comment">#print(&quot;takeStep1, alpha[&quot;,id1,&quot;]=&quot;,g_alpha[id1],&quot;, alphapp[&quot;,id2,&quot;]=&quot;,g_alpha[id2])</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    r=chooseRandomIndex(id2)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">      id1 = (r+i)%g_m</span><br><span class="line">      <span class="keyword">if</span> id1!=id2 <span class="keyword">and</span> g_alpha[id1]!=<span class="number">0</span> <span class="keyword">and</span> g_alpha[id1]!=g_C:</span><br><span class="line">        <span class="keyword">if</span> takeStep(id1,id2,<span class="number">1e-3</span>):</span><br><span class="line">          <span class="comment">#print(&quot;takeStep2, alpha[&quot;, id1, &quot;]=&quot;, g_alpha[id1], &quot;, alphapp[&quot;, id2, &quot;]=&quot;, g_alpha[id2])</span></span><br><span class="line">          <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    r = chooseRandomIndex(id2)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">      id1 = (r + i) % g_m</span><br><span class="line">      <span class="keyword">if</span> id1 != id2:</span><br><span class="line">        <span class="keyword">if</span> takeStep(id1,id2,<span class="number">1e-3</span>):</span><br><span class="line">          <span class="comment">#print(&quot;takeStep3, alpha[&quot;, id1, &quot;]=&quot;, g_alpha[id1], &quot;, alphapp[&quot;, id2, &quot;]=&quot;, g_alpha[id2])</span></span><br><span class="line">          <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kernel_test</span>(<span class="params">id1,x</span>):</span><br><span class="line">  <span class="comment"># 这里核函数为简单的内积</span></span><br><span class="line">  <span class="comment"># id1和id2为int类型，是g_x_mat中的索引</span></span><br><span class="line">  x_1 = g_x_mat[id1]</span><br><span class="line">  val = np.subtract(x_1,x)</span><br><span class="line">  val = np.dot(val,val)</span><br><span class="line">  <span class="keyword">return</span> np.exp(-val)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">svmOutput_test</span>(<span class="params">alpha,b,x</span>):</span><br><span class="line">  <span class="comment"># 这个函数是svm的实际输出，计算当前参数(w,b)下, 计算得到的y</span></span><br><span class="line">  <span class="comment"># 由于w = sum (alpha*y*x), 对于第i个分量x_i所以输出结果应该为w*x_i+b, 也就是 sum (alpha*y*&lt;x_1-m,x_i&gt;)</span></span><br><span class="line">  <span class="comment"># 注: 待会分析下alpha的变化趋势与C的关系</span></span><br><span class="line">  <span class="keyword">global</span> g_m</span><br><span class="line">  <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">    <span class="keyword">if</span> alpha[i]!=<span class="number">0</span>:</span><br><span class="line">      <span class="built_in">sum</span> += alpha[i]*g_y_vec[i]*kernel_test(i,x)</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">sum</span>+\</span><br><span class="line">         b</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">showPic</span>(<span class="params">alpha,b</span>):</span><br><span class="line">  figure, ax = plt.subplots()</span><br><span class="line">  ax.set_xlim(left=-<span class="number">2</span>, right=<span class="number">2</span>)</span><br><span class="line">  ax.set_ylim(bottom=-<span class="number">2</span>, top=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 生成100组测试数据</span></span><br><span class="line">  x=[]</span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    x_0 = random.randint(-<span class="number">1200</span>,<span class="number">1200</span>)/<span class="number">1000</span></span><br><span class="line">    x_1 = random.randint(-<span class="number">1200</span>,<span class="number">1200</span>)/<span class="number">1000</span></span><br><span class="line">    x.append([x_0,x_1])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">if</span> svmOutput_test(alpha,b,x[i])&gt;<span class="number">0</span>:</span><br><span class="line">      plt.plot(x[i][<span class="number">0</span>], x[i][<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;+&#x27;</span>, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      plt.plot(x[i][<span class="number">0</span>], x[i][<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, color=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># draw x_0*x_0+x_1*x_1=0</span></span><br><span class="line">  cir1 = Circle(xy=(<span class="number">0.0</span>, <span class="number">0.0</span>), radius=<span class="number">1</span>)</span><br><span class="line">  ax.add_patch(cir1)</span><br><span class="line"></span><br><span class="line">  plt.xlabel(<span class="string">&quot;x1&quot;</span>)</span><br><span class="line">  plt.ylabel(<span class="string">&quot;x2&quot;</span>)</span><br><span class="line">  plt.plot()</span><br><span class="line">  plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">  SHOW_PIC = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">  g_m = <span class="built_in">len</span>(g_x_mat)</span><br><span class="line">  <span class="keyword">if</span> SHOW_PIC == <span class="literal">False</span>:</span><br><span class="line">    <span class="comment"># 1 training</span></span><br><span class="line">    g_alpha = np.zeros(g_m)</span><br><span class="line">    g_y_now = np.zeros(g_m)</span><br><span class="line">    g_err = np.zeros(g_m)</span><br><span class="line">    <span class="keyword">global</span> g_b</span><br><span class="line">    g_b = <span class="number">0</span></span><br><span class="line">    numChanged = <span class="number">0</span></span><br><span class="line">    examineAll = <span class="number">1</span></span><br><span class="line">    g_C = <span class="number">10</span></span><br><span class="line">    err = <span class="number">0</span></span><br><span class="line">    updateYAndErr()       <span class="comment"># 实现更新下缓冲，即当前输出与误差值</span></span><br><span class="line">    <span class="keyword">while</span> numChanged&gt;<span class="number">0</span> <span class="keyword">or</span> examineAll:</span><br><span class="line">      numChanged = <span class="number">0</span></span><br><span class="line">      <span class="comment"># 循环处理，第一次对所有的样本进行一次处理。</span></span><br><span class="line">      <span class="comment"># 然后对所有非边界的数值进行处理。因为在当前参数下，非边界的样本，我们认为其是支持向量。对于优化||w||的大小，支持向量才有意义。</span></span><br><span class="line">      <span class="keyword">if</span> examineAll:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">          numChanged += examineExample(i)</span><br><span class="line">          <span class="comment">#print(&quot;examineAll=1, numChanged=&quot;,numChanged)</span></span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">          <span class="keyword">if</span> g_alpha[i]!=<span class="number">0</span> <span class="keyword">and</span> g_alpha[i]!=g_C:</span><br><span class="line">            numChanged += examineExample(i)</span><br><span class="line">      examineAll = <span class="built_in">abs</span>(examineAll-<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(g_alpha, g_b)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># 3 show</span></span><br><span class="line">    alpha= [  <span class="number">0.00000000e+00</span>,   <span class="number">7.59410972e-01</span>,  -<span class="number">2.22044605e-16</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">1.00000000e+01</span>,   <span class="number">1.00000000e+01</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">1.00000000e+01</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">5.67018243e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">1.00000000e+01</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">1.00000000e+01</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">2.45788486e+00</span>,   <span class="number">1.33286169e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">1.11022302e-16</span>,   <span class="number">1.00000000e+01</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">7.22541284e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,  -<span class="number">1.38777878e-17</span>,   <span class="number">0.00000000e+00</span>,  -<span class="number">2.77555756e-17</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">8.66273771e-04</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">8.33442222e+00</span>,</span><br><span class="line">    <span class="number">1.00000000e+01</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">1.00000000e+01</span>,   <span class="number">1.00000000e+01</span>,  -<span class="number">2.77555756e-17</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">1.00000000e+01</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">4.39461998e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">1.00000000e+01</span>,   <span class="number">1.00000000e+01</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,  -<span class="number">2.22044605e-16</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">1.00000000e+01</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,  -<span class="number">2.77555756e-17</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    -<span class="number">5.55111512e-17</span>,  <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">6.59194921e-17</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">1.00000000e+01</span>,   <span class="number">1.00000000e+01</span>,  -<span class="number">1.38777878e-17</span>,</span><br><span class="line">    <span class="number">5.21868376e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,  -<span class="number">2.22044605e-16</span>,   <span class="number">6.56986459e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">5.19744490e+00</span>,   <span class="number">9.69510083e+00</span>,   <span class="number">1.00000000e+01</span>,</span><br><span class="line">    <span class="number">1.00000000e+01</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">8.69782724e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">1.00000000e+01</span>,</span><br><span class="line">    <span class="number">3.24960991e+00</span>,   <span class="number">8.23041098e+00</span>]</span><br><span class="line">    b = -<span class="number">2.68518972087</span></span><br><span class="line">    <span class="comment">#w = np.array([0, 0])</span></span><br><span class="line">    <span class="comment">#for i in range(g_m):</span></span><br><span class="line">    <span class="comment">#  if g_alpha[i] != 0:</span></span><br><span class="line">    <span class="comment">#    w = np.add(w, g_y_vec[i] * g_alpha[i] * g_x_mat[i])</span></span><br><span class="line">    showPic(alpha,b)</span><br></pre></td></tr></table></figure>

<p>经过数轮迭代之后，得到参数。然后在随机产生一些样本，通过训练集得到参数对随机测试样本进行分割，结果如下，发现分割效果还是很理想的。</p>
<img src="/images/机器学习/监督学习-支持向量机计算结果2.png" width=50% height=50% text-align=center/>

<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>手写公式和word版博客地址:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/zhengchenyu/MyBlog/tree/master/doc/mlearning/支持向量机</span><br></pre></td></tr></table></figure>

<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>cs229-notes3</li>
<li>机器学习 周志华版</li>
<li>Fast Training of Support Vector Machines Using Sequential Minimal Optimization</li>
<li><a target="_blank" rel="noopener" href="http://blog.csdn.net/luoshixian099/article/details/51227754">http://blog.csdn.net/luoshixian099/article/details/51227754</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2017/09/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-7-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" data-id="cuidlluekJvJsBA0Osrh_l-Wb" data-title="机器学习-2.7-监督学习之支持向量机" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-机器学习-2-6-监督学习之朴素贝叶斯算法" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2017/08/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-6-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/" class="article-date">
  <time class="dt-published" datetime="2017-08-05T13:11:31.000Z" itemprop="datePublished">2017-08-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2017/08/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-6-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/">机器学习-2.6-监督学习之朴素贝叶斯算法</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="朴素贝叶斯算法"><a href="#朴素贝叶斯算法" class="headerlink" title="朴素贝叶斯算法"></a>朴素贝叶斯算法</h1><h2 id="1-朴素贝叶斯算法"><a href="#1-朴素贝叶斯算法" class="headerlink" title="1 朴素贝叶斯算法"></a>1 朴素贝叶斯算法</h2><h3 id="1-1-垃圾邮件处理模型"><a href="#1-1-垃圾邮件处理模型" class="headerlink" title="1.1 垃圾邮件处理模型"></a>1.1 垃圾邮件处理模型</h3><p>以预测垃圾邮件为例子，介绍贝叶斯算法。词库中有5000个单词。我们有输入\(x\)，\(x \in {\{ 0,1\} ^{50000}}\)，是一个50000维的向量，其中\(x _i\)表示词库中第\(i\)个单次在该邮件中出现，为\(0\)表示词库中第\(i\)个单次在该邮件中没有出现。</p>
<p>已经邮件是否为邮件,样本\(({x _1},{x _2},…,{x _i})\)出现的概率为\(p({x _1},{x _2},…,{x _i}|y)\)，可以定义为下式:</p>

$$p({x _1},{x _2},...,{x _i}|y) = p({x _1}|y)p({x _2}|y,{x _1})...p({x _{50000}}|y,{x _1},...,{x _{49999}})$$


<p>注: 上式很好理解。右侧第一个式子为已知\(y\)，\(x _1\)出现的概率。第二个为一直\(y\)和\(x _1\)出现\(x _2\)的概率。因此两个式子相乘表示一直\(y\)初选\((x1,x2)\)的概率。依次类推，可以得到这个公式。</p>

$$p({x _1},{x _2},...,{x _i}|y) = p({x _1}|y)p({x _2}|y)...p({x _{50000}}|y) = \prod\limits _{i = 1}^{50000} {p({x _i}|y)}$$



<h3 id="1-2-公式推导"><a href="#1-2-公式推导" class="headerlink" title="1.2 公式推导"></a>1.2 公式推导</h3><p>关于最大释然函数如何设置，我们需要知道我们的目标是找到参数\(\theta\)，使得训练时候在给定\(x\)的情况下，得到最准确\(y\)的概率最大，最大释然函数就是这些概率的连乘，具体是需要保证下面的式子最大:</p>

$$\ln \prod\limits _{i = 1}^m {p(y = {y^i}|x = {x^i};\theta )p(x = {x^i})} $$


<p>在很多教材中都介绍最大释然函数是如下的表达，实际上是一样的。下面的式子的直接意义是在\(\theta\)已知情况下，出现\((x^i,y^i)\)的概率，实际上面的分析过程是一样的。</p>

$$\ln \prod\limits _{i = 1}^m {p(y = {y^i},x = {x^i};\theta )} $$


<blockquote>
<p>后面为了简写后面省略了\(\theta\)。</p>
</blockquote>
<p>我们做如下设置:</p>

$${\phi _y} = p(y = 1)$$
$${\phi _{i|y = 1}} = p({x _i} = 1|y = 1)$$
$${\phi _{i|y = 0}} = p({x _i} = 1|y = 0)$$


<p>然后得到最大释然函数:</p>

$$\ell ({\phi _y},{\phi _{i|y = 1}},{\phi _{i|y = 0}}) = \ln \prod\limits _{i = 1}^m {p(y = {y^i},x = {x^i})}  = \ln \prod\limits _{i = 1}^m {p({x^i}|{y^i})p({y^i})}$$
$$\ell ({\phi _y},{\phi _{i|y = 1}},{\phi _{i|y = 0}}) = \ln \prod\limits _{i = 1}^m {[[1\{ {y^i} = 1\} \ln ((\prod\limits _{j = 1}^n {p(x _j^i|{y^i})p({y^i} = 1)} ){\phi _y})][1\{ {y^i} = 1\} \ln ((\prod\limits _{j = 1}^n {p(x _j^i|{y^i})p({y^i} = 0)} )(1 - {\phi _y}))]} ]$$
$$\ell ({\phi _y},{\phi _{i|y = 1}},{\phi _{i|y = 0}}) = \sum\limits _{i = 1}^m {[[1\{ {y^i} = 1\} (\ln {\phi _y} + \ln \prod\limits _{j = 1}^n {({{({\phi _{j|y = 1}})}^{1\{ {x _j} = 1\} }}{{(1 - {\phi _{j|y = 1}})}^{1\{ {x _j} = 0\} }})} )][1\{ {y^i} = 0\} (\ln (1 - {\phi _y}) + \ln \prod\limits _{j = 1}^n {({{({\phi _{j|y = 0}})}^{1\{ {x _j} = 1\} }}{{(1 - {\phi _{j|y = 0}})}^{1\{ {x _j} = 0\} }})} )]]}$$
$$\ell ({\phi _y},{\phi _{i|y = 1}},{\phi _{i|y = 0}}) = \sum\limits  _{i = 1}^m {[[1\{ {y^i} = 1\} (\ln {\phi _y} + \sum\limits _{j = 1}^n {(1\{ x _j^i = 1\} \ln {\phi _{j|y = 1}} + 1\{ x _j^i = 0\} \ln (1 - {\phi _{j|y = 1}}))} )][1\{ {y^i} = 0\} (\ln (1 - {\phi _y}) + \sum\limits _{j = 1}^n {(1\{ x _j^i = 1\} \ln {\phi _{j|y = 0}} + 1\{ x _j^i = 0\} \ln (1 - {\phi _{j|y = 0}}))} )]]}$$


<p>然后我们对\(\phi _y\)求偏导数：</p>

$$\frac{{\partial \ell ({\phi _y},{\phi _{i|y = 1}},{\phi _{i|y = 0}})}}{\partial {\phi _y}} = \sum\limits _{i = 1}^m {(1\{ {y^i} = 1\} \frac{1}{\phi _y} + 1\{ {y^i} = 0\} \frac{ - 1}{1 - {\phi _y}})}  = \sum\limits _{i = 1}^m {\frac{1\{ {y^i} = 1\}  - {\phi _y}}{\phi _y}(1 - {\phi _y})} = 0$$


<p>所以有:</p>

$${\phi _y} = \frac{1}{m}\sum\limits _{i = 1}^m {1\{ {y^i} = 1\} }$$


<p>然后继续求偏导数:</p>

$$\frac{{\partial \ell ({\phi _y},{\phi _{i|y = 1}},{\phi _{i|y = 0}})}}{{\partial {\phi _{i|y = 1}}}} = \sum\limits _{i = 1}^m {1\{ {y^i} = 1\} (1\{ x _j^i = 1\} \frac{1}{{{\phi _{x _j^i = 1|y = 1}}}} + 1\{ x _j^i = 0\} \frac{{ - 1}}{{1 - {\phi _{x _j^i = 1|y = 1}}}})}=0$$

$$\frac{{\partial \ell ({\phi _y},{\phi _{i|y = 1}},{\phi _{i|y = 0}})}}{{\partial {\phi _{i|y = 1}}}} = \sum\limits _{i = 1}^m {1\{ {y^i} = 1\} (\frac{{1\{ x _j^i = 1\}  - {\phi _{i|y = 1}}}}{{{\phi _{i|y = 1}}(1 - {\phi _{i|y = 1}})}})}=0$$


<p>如果一个单词不再训练样本中，就会出现0&#x2F;0的现象。避免这个事件发生，引入拉普拉斯平滑。所以有:</p>

$${\phi _{i|y = 1}} = \frac{{\sum\limits _{i = 1}^m {1\{ x _j^i = 1,{y^i} = 1\} }  + 1}}{{\sum\limits _{i = 1}^m {1\{ {y^i} = 1\} }  + 2}}$$


<p>同理有:</p>

$${\phi _{i|y = 0}} = \frac{{\sum\limits _{i = 1}^m {1\{ x _j^i = 1,{y^i} = 0\} }  + 1}}{{\sum\limits _{i = 1}^m {1\{ {y^i} = 1\} }  + 2}}$$


<p>注:试分析这样的公式，\(\phi _y\)就是总的垃圾邮件数目除以从样本数。\({\phi _{i|y &#x3D; 1}}\)就是所有垃圾邮件中出现\(x _j^i\)对应的单词的数目除以所有垃圾邮件的数目。实际上这是个很容理解的道理。当然这就是数学的魅力，即便很简单的公司都是有着其理论依据的。</p>
<h3 id="1-3-算法实现"><a href="#1-3-算法实现" class="headerlink" title="1.3 算法实现"></a>1.3 算法实现</h3><p>我们使用样本得到各个单词的概率分布，然后预测邮件是否为垃圾邮件。运行下面的程序，可以得到了正确的垃圾邮件分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">global</span> DEBUG</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">constructDic</span>():</span><br><span class="line">  file = <span class="built_in">open</span>(<span class="string">&quot;../res/NaiveBayes/dict.txt&quot;</span>)</span><br><span class="line">  <span class="comment"># key is the word, value is the sequence number</span></span><br><span class="line">  <span class="built_in">dict</span> = &#123;&#125;           <span class="comment"># In fact, treeMap is better than dict</span></span><br><span class="line">  count = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">    line = file.readline()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> line.split():</span><br><span class="line">      <span class="keyword">if</span> word.lower() <span class="keyword">not</span> <span class="keyword">in</span> <span class="built_in">dict</span>:</span><br><span class="line">        <span class="built_in">dict</span>[word.lower()]=count</span><br><span class="line">        count = count+<span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">dict</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">findIndexInDict</span>(<span class="params">word,<span class="built_in">dict</span></span>):</span><br><span class="line">  <span class="keyword">if</span> word <span class="keyword">in</span> <span class="built_in">dict</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>[word]</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parseSpamOrHam</span>(<span class="params">words</span>):</span><br><span class="line">  <span class="keyword">if</span> words[<span class="number">0</span>] == <span class="string">&quot;spam&quot;</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">  <span class="keyword">elif</span> words[<span class="number">0</span>] == <span class="string">&quot;ham&quot;</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">if</span> DEBUG:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&quot;error email type in training sample!&quot;</span>)</span><br><span class="line">  <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">splitEmail</span>(<span class="params">line</span>):</span><br><span class="line">  regEx = re.<span class="built_in">compile</span>(<span class="string">r&#x27;[^a-zA-Z]|\d&#x27;</span>)</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> word: word!=<span class="string">&quot;&quot;</span>, regEx.split(line)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># spam is 1, ham is 0</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parseEmails</span>(<span class="params">x_arr,y_arr,<span class="built_in">dict</span>,dictLen</span>):</span><br><span class="line">  file = <span class="built_in">open</span>(<span class="string">&quot;../res/NaiveBayes/emails.txt&quot;</span>)</span><br><span class="line">  <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">    line = file.readline()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">    words = splitEmail(line)</span><br><span class="line">    y = parseSpamOrHam(words)</span><br><span class="line">    x = np.zeros(dictLen)</span><br><span class="line">    <span class="keyword">if</span> y == -<span class="number">1</span>:</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words[<span class="number">1</span>:]:</span><br><span class="line">      <span class="keyword">if</span> word.lower() <span class="keyword">in</span> <span class="built_in">dict</span>:</span><br><span class="line">        index = <span class="built_in">dict</span>[word.lower()]</span><br><span class="line">        x[index]=<span class="number">1</span></span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> DEBUG:</span><br><span class="line">          <span class="built_in">print</span>(word,<span class="string">&quot; is not in dic!&quot;</span>)</span><br><span class="line">    x_arr.append(x)</span><br><span class="line">    y_arr.append(y)</span><br><span class="line">    <span class="comment">#if DE<span class="doctag">BUG:</span></span></span><br><span class="line">    <span class="comment"># for i in range(len(x)):</span></span><br><span class="line">    <span class="comment">#  if x[i] == 1:</span></span><br><span class="line">    <span class="comment">#    print(i)</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">dict</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calPhiY</span>(<span class="params">y_arr</span>):</span><br><span class="line">  <span class="comment"># p(y=1)</span></span><br><span class="line">  <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> y_arr:</span><br><span class="line">    <span class="built_in">sum</span> = <span class="built_in">sum</span> + i</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">sum</span>/<span class="built_in">len</span>(y_arr)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calPhiXY</span>(<span class="params">y_arr,x_arr,knownY,dictLen</span>):</span><br><span class="line">  <span class="comment"># return a vector</span></span><br><span class="line">  <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">  ret = np.zeros(dictLen)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(y_arr)):</span><br><span class="line">    <span class="keyword">if</span> y_arr[i]!=knownY:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    <span class="built_in">sum</span>=<span class="built_in">sum</span>+<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(dictLen):</span><br><span class="line">      <span class="keyword">if</span> x_arr[i][j]==<span class="number">1</span>:</span><br><span class="line">       ret[j]=ret[j]+<span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> (ret+<span class="number">1</span>)/(<span class="built_in">sum</span>+<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">classify</span>(<span class="params">phi,phi_y0,phi_y1,dictLen</span>):</span><br><span class="line">  file = <span class="built_in">open</span>(<span class="string">&quot;../res/NaiveBayes/testEmails.txt&quot;</span>)</span><br><span class="line">  <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">    line = file.readline()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">    words = splitEmail(line)[<span class="number">1</span>:]</span><br><span class="line">    x = np.zeros(dictLen)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">      <span class="keyword">if</span> word.lower() <span class="keyword">in</span> <span class="built_in">dict</span>:</span><br><span class="line">        index = <span class="built_in">dict</span>[word.lower()]</span><br><span class="line">        x[index]=<span class="number">1</span></span><br><span class="line">    res = calcPro(phi,phi_y1,x)/(calcPro(phi,phi_y1,x)+calcPro(phi,phi_y0,x))</span><br><span class="line">    <span class="keyword">if</span> res &gt; <span class="number">0.5</span>:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&quot;spam :&quot;</span> + line)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&quot;ham :&quot;</span> + line)</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">dict</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcPro</span>(<span class="params">phi,phi_y,x</span>):</span><br><span class="line">  <span class="comment"># p(x|y=phi_y)</span></span><br><span class="line">  ret = <span class="number">1</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)):</span><br><span class="line">    <span class="keyword">if</span> x[i] == <span class="number">1</span>:</span><br><span class="line">      ret = ret * phi_y[i]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      ret = ret * (<span class="number">1</span>-phi_y[i])</span><br><span class="line">  <span class="keyword">return</span> ret*phi</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">  <span class="comment"># 0. debug options</span></span><br><span class="line">  DEBUG = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 1. construct Dict</span></span><br><span class="line">  <span class="built_in">dict</span> = constructDic()</span><br><span class="line">  dictLen = <span class="built_in">len</span>(<span class="built_in">dict</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 2. parse the email to generate the sample, x and y</span></span><br><span class="line">  x_arr=[]</span><br><span class="line">  y_arr=[]</span><br><span class="line">  parseEmails(x_arr,y_arr,<span class="built_in">dict</span>,dictLen)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 3. learn from the sample</span></span><br><span class="line">  phi = calPhiY(y_arr)            <span class="comment"># p(y)=1</span></span><br><span class="line">  phi_y1 = calPhiXY(y_arr, x_arr, <span class="number">1</span>, dictLen)</span><br><span class="line">  phi_y0 = calPhiXY(y_arr, x_arr, <span class="number">0</span>, dictLen)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 4. test classify</span></span><br><span class="line">  <span class="comment">#p(y=1|x)=p(x|y=1)p(y=1)/(p(x|y=1)p(y=1)+p(x|y=0)p(y=0))</span></span><br><span class="line">  <span class="comment">#p(x|y=1) = Pe(p(x=x^i|y=1))</span></span><br><span class="line">  classify(phi,phi_y0,phi_y1,dictLen)</span><br></pre></td></tr></table></figure>

<h2 id="2-多元伯努利事件模型"><a href="#2-多元伯努利事件模型" class="headerlink" title="2 多元伯努利事件模型"></a>2 多元伯努利事件模型</h2><h3 id="2-1-公式推导"><a href="#2-1-公式推导" class="headerlink" title="2.1 公式推导"></a>2.1 公式推导</h3><p>下面采用另外一种方式进行建模。对于\({x^T} &#x3D; [{x _1},{x _2},…,{x _n}]\)，其中\({x _1} &#x3D; 1\)代表邮件的第一个单词在词库中的索引为1。仍然有如下公式：</p>

$$p({x _1},{x _2},...,{x _i}|y) = \prod\limits _{i = 1}^n {p({x _i}|y)}$$


<p>我们设置\({\phi _y} &#x3D; p(y &#x3D; 1)\)，\({\phi _{i &#x3D; k|y &#x3D; 1}} &#x3D; p({x _i} &#x3D; k|y &#x3D; 1)\)。</p>
<p>然后求最大释然函数：</p>

$$\ell ({\phi _y},{\phi _{i = k|y = 1}},{\phi _{i = k|y = 0}}) = \ln \prod\limits _{i = 1}^m {p({y^i}|{x^i})}  = \ln \prod\limits _{i = 1}^m {p({x^i}|{y^i})p({y^i})}$$
$$\ell ({\phi _y},{\phi _{i = k|y = 1}},{\phi _{i = k|y = 0}}) = \ln \prod\limits _{i = 1}^m {{{(p({x^i}|{y^i} = 1){\phi _y})}^{1\{ {y^i} = 1\} }}p({x^i}|{y^i} = 0)(1 - {\phi _y}){)^{1\{ {y^i} = 0\} }})}$$
$$\ell ({\phi _y},{\phi _{i = k|y = 1}},{\phi _{i = k|y = 0}}) = \ln \prod\limits _{i = 1}^m {{{((\ln (\prod\limits _{j = 1}^n {p(} x _j^i|{y^i} = 1)){\phi _y})}^{1\{ {y^i} = 1\} }}(\ln (\prod\limits _{j = 1}^n {p(} x _j^i|{y^i} = 0))(1 - {\phi _y}){)^{1\{ {y^i} = 0\} }})}$$
$$\ell ({\phi _y},{\phi _{i = k|y = 1}},{\phi _{i = k|y = 0}}) = \sum\limits _{i = 1}^m {[1\{ {y^i} = 1\} (\ln {\phi _y} + \sum\limits _{j = 1}^n {\ln p(x _j^i|{y^i} = 1)} ) + 1\{ {y^i} = 0\} (\ln (1 - {\phi _y}) + \sum\limits _{j = 1}^n {\ln p(x _j^i|{y^i} = 0)} )]}$$


<p>我们对\({\phi _y}\)求偏导数，与之前相同，这里直接写出：</p>

$${\phi _y} = \frac{1}{m}\sum\limits _{i = 1}^m {1\{ {y^i} = 1\} }$$


<p>我们对\({\phi _{i &#x3D; k|y &#x3D; 0}}\)求偏导数，如下：</p>

$$\frac{{\partial \ell ({\phi _y},{\phi _{i = k|y = 1}},{\phi _{i = k|y = 0}})}}{{\partial {\phi _{i = k|y = 1}}}} = \frac{{\partial \sum\limits _{i = 1}^m {1\{ {y^i} = 1\} \sum\limits _{j = 1}^n {\ln p({x^i}|{y^i} = 1)} } }}{{\partial {\phi _{i|y = 1}}}}$$


<p>其中有：</p>

$$\ln p({x^i}|{y^i} = 1) = \ln [p{(x _j^i = k|{y^i} = 1)^{1\{ x _j^i = k\} }}p{(x _j^i \ne k|{y^i} = 1)^{1\{ x _j^i \ne k\} }}]$$
$$\ln p({x^i}|{y^i} = 1) = 1\{ x _j^i = k\} \ln {\phi _{i = k|y = 1}} + (1 - 1\{ x _j^i = k\} )\ln (1 - {\phi _{i = k|y = 1}})$$


<p>所以有：</p>

$$\frac{{\partial \ell ({\phi _y},{\phi _{i = k|y = 1}},{\phi _{i = k|y = 0}})}}{{\partial {\phi _{i = k|y = 1}}}} = \sum\limits _{i = 1}^m {1\{ {y^i} = 1\} \sum\limits _{j = 1}^n {\frac{{1\{ x _j^i = k\}  - {\phi _{i = k|y = 1}}}}{{{\phi _{i = k|y = 1}}(1 - {\phi _{i = k|y = 1}})}}} }  = 0$$


<p>根据拉普拉斯平滑，所以最后得到下面的公式，其中V为词库中单词的数目。</p>

$${\phi _{i = k|y = 1}} = \frac{{\sum\limits _{i = 1}^m {\sum\limits _{j = 1}^n {1\{ {y^i} = 1,x _j^i = k\} } }  + 1}}{{n\sum\limits _{i = 1}^m {1\{ {y^i} = 1\} }  + V}}$$


<p>同理有:</p>

$${\phi _{i = k|y = 0}} = \frac{{\sum\limits _{i = 1}^m {\sum\limits _{j = 1}^n {1\{ {y^i} = 0,x _j^i = k\} } }  + 1}}{{n\sum\limits _{i = 1}^m {1\{ {y^i} = 0\} }  + V}}$$


<h3 id="2-2-算法的实现"><a href="#2-2-算法的实现" class="headerlink" title="2.2 算法的实现"></a>2.2 算法的实现</h3><p>我们使用样本得到各个单词的概率分布，然后预测邮件是否为垃圾邮件。运行下面的程序，可以得到了正确的垃圾邮件分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">global</span> DEBUG</span><br><span class="line"><span class="keyword">global</span> SCALE</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">constructDic</span>():</span><br><span class="line">  file = <span class="built_in">open</span>(<span class="string">&quot;../res/NaiveBayes/dict.txt&quot;</span>)</span><br><span class="line">  <span class="comment"># key is the word, value is the sequence number</span></span><br><span class="line">  <span class="built_in">dict</span> = &#123;&#125;           <span class="comment"># In fact, treeMap is better than dict</span></span><br><span class="line">  count = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">    line = file.readline()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> line.split():</span><br><span class="line">      <span class="keyword">if</span> word.lower() <span class="keyword">not</span> <span class="keyword">in</span> <span class="built_in">dict</span>:</span><br><span class="line">        <span class="built_in">dict</span>[word.lower()]=count</span><br><span class="line">        count = count+<span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">dict</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">findIndexInDict</span>(<span class="params">word,<span class="built_in">dict</span></span>):</span><br><span class="line">  <span class="keyword">if</span> word <span class="keyword">in</span> <span class="built_in">dict</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>[word]</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parseSpamOrHam</span>(<span class="params">words</span>):</span><br><span class="line">  <span class="keyword">if</span> words[<span class="number">0</span>] == <span class="string">&quot;spam&quot;</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">  <span class="keyword">elif</span> words[<span class="number">0</span>] == <span class="string">&quot;ham&quot;</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">if</span> DEBUG:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&quot;error email type in training sample!&quot;</span>)</span><br><span class="line">  <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">splitEmail</span>(<span class="params">line</span>):</span><br><span class="line">  regEx = re.<span class="built_in">compile</span>(<span class="string">r&#x27;[^a-zA-Z]|\d&#x27;</span>)</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> word: word!=<span class="string">&quot;&quot;</span>, regEx.split(line)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># spam is 1, ham is 0</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parseEmails</span>(<span class="params">x_arr,y_arr,<span class="built_in">dict</span>,dictLen</span>):</span><br><span class="line">  file = <span class="built_in">open</span>(<span class="string">&quot;../res/NaiveBayes/emails.txt&quot;</span>)</span><br><span class="line">  <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">    line = file.readline()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">    words = splitEmail(line)</span><br><span class="line">    y = <span class="built_in">int</span>(parseSpamOrHam(words))</span><br><span class="line">    words = words[<span class="number">1</span>:]</span><br><span class="line">    x = np.zeros(<span class="built_in">len</span>(words),<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">if</span> y == -<span class="number">1</span>:</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(words)):</span><br><span class="line">      <span class="keyword">if</span> words[i].lower() <span class="keyword">in</span> <span class="built_in">dict</span>:</span><br><span class="line">        index = <span class="built_in">dict</span>[words[i].lower()]</span><br><span class="line">        x[i]=index</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        x[i]=dictLen+<span class="number">1</span></span><br><span class="line">    x_arr.append(x)</span><br><span class="line">    y_arr.append(y)</span><br><span class="line">    <span class="comment">#if DE<span class="doctag">BUG:</span></span></span><br><span class="line">    <span class="comment"># for i in range(len(x)):</span></span><br><span class="line">    <span class="comment">#    print(x[i])</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">dict</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calPhiY</span>(<span class="params">y_arr</span>):</span><br><span class="line">  <span class="comment"># p(y=1)</span></span><br><span class="line">  <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> y_arr:</span><br><span class="line">    <span class="built_in">sum</span> = <span class="built_in">sum</span> + i</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">sum</span>/<span class="built_in">len</span>(y_arr)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calPhiXY</span>(<span class="params">y_arr,x_arr,knownY,dictLen</span>):</span><br><span class="line">  <span class="comment"># return a vector</span></span><br><span class="line">  <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">  ret = np.zeros(dictLen)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(y_arr)):</span><br><span class="line">    <span class="keyword">if</span> y_arr[i]!=knownY:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    <span class="built_in">sum</span>=<span class="built_in">sum</span>+<span class="built_in">len</span>(x_arr[i])</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_arr[i])):</span><br><span class="line">      index = x_arr[i][j]</span><br><span class="line">      ret[index]=ret[index]+<span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> SCALE*(ret+<span class="number">1</span>)/(<span class="built_in">sum</span>+dictLen)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">classify</span>(<span class="params">phi,phi_y0,phi_y1,dictLen</span>):</span><br><span class="line">  file = <span class="built_in">open</span>(<span class="string">&quot;../res/NaiveBayes/testEmails.txt&quot;</span>)</span><br><span class="line">  <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">    line = file.readline()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">    words = splitEmail(line)[<span class="number">1</span>:]</span><br><span class="line">    x = np.zeros(<span class="built_in">len</span>(words),<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(words)):</span><br><span class="line">      <span class="keyword">if</span> words[i].lower() <span class="keyword">in</span> <span class="built_in">dict</span>:</span><br><span class="line">        index = <span class="built_in">dict</span>[words[i].lower()]</span><br><span class="line">        x[i]=index</span><br><span class="line">    res = calcPro(phi,phi_y1,x)/(calcPro(phi,phi_y1,x)+calcPro(phi,phi_y0,x))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> res &gt; <span class="number">0.5</span>:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&quot;spam :&quot;</span> + line)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&quot;ham :&quot;</span> + line)</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">dict</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcPro</span>(<span class="params">phi,phi_y,x</span>):</span><br><span class="line">  <span class="comment"># p(x|y=phi_y)</span></span><br><span class="line">  ret = <span class="number">1</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)):</span><br><span class="line">    ret = ret*phi_y[x[i]]</span><br><span class="line">    <span class="comment">#print(ret)</span></span><br><span class="line">  <span class="keyword">return</span> ret*phi</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">  <span class="comment"># 0. debug options</span></span><br><span class="line">  DEBUG = <span class="literal">True</span></span><br><span class="line">  SCALE = <span class="number">10e3</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 1. construct Dict</span></span><br><span class="line">  <span class="built_in">dict</span> = constructDic()</span><br><span class="line">  dictLen = <span class="built_in">len</span>(<span class="built_in">dict</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 2. parse the email to generate the sample, x and y</span></span><br><span class="line">  x_arr=[]</span><br><span class="line">  y_arr=[]</span><br><span class="line">  parseEmails(x_arr,y_arr,<span class="built_in">dict</span>,dictLen)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 3. learn from the sample</span></span><br><span class="line">  phi = calPhiY(y_arr)            <span class="comment"># p(y)=1</span></span><br><span class="line">  phi_y1 = calPhiXY(y_arr, x_arr, <span class="number">1</span>, dictLen)</span><br><span class="line">  phi_y0 = calPhiXY(y_arr, x_arr, <span class="number">0</span>, dictLen)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 4. test classify</span></span><br><span class="line">  <span class="comment">#p(y=1|x)=p(x|y=1)p(y=1)/(p(x|y=1)p(y=1)+p(x|y=0)p(y=0))</span></span><br><span class="line">  <span class="comment">#p(x|y=1) = Pe(p(x=x^i|y=1))</span></span><br><span class="line">  classify(phi,phi_y0,phi_y1,dictLen)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>由于概率值过小，多次乘积会超过浮点数精度范围，所以程序这里乘以一个固定的比例系数</p>
</blockquote>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>手写公式和word版博客地址:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/zhengchenyu/MyBlog/tree/master/doc/mlearning/贝叶斯算法</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2017/08/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-6-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/" data-id="cuidcIB_RMSTeApCb3T37F2Fs" data-title="机器学习-2.6-监督学习之朴素贝叶斯算法" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-机器学习-2-5-监督学习之生成型学习算法" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2017/08/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-5-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%94%9F%E6%88%90%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" class="article-date">
  <time class="dt-published" datetime="2017-08-05T08:41:15.000Z" itemprop="datePublished">2017-08-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2017/08/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-5-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%94%9F%E6%88%90%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/">机器学习-2.5-监督学习之生成型学习算法</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="生成型学习算法"><a href="#生成型学习算法" class="headerlink" title="生成型学习算法"></a>生成型学习算法</h1><h2 id="1-生成型学习算法简述"><a href="#1-生成型学习算法简述" class="headerlink" title="1 生成型学习算法简述"></a>1 生成型学习算法简述</h2><p>我们之前讲到的算法。譬如逻辑回归，试图从\(x\)直接学习得到一组映射关系到\(y\)，即通过样本\((x,y)\)学习得到,使得其能够准确的预测\(y\)。这类算法叫做判别性学习算法。</p>
<p>本节我们将介绍生成学习型算法。举个例子，对于一个大象，我们可以为大象构造一个关于其特征的模型。对于一条狗，同样可以为狗构造一个关于关于其特征的模型。对于一个新动物，我们可以根据前述构建的模型，判断这个动物是像大象多一点还是像狗多一点，来确定这动物是大象还是狗。</p>
<p>我们可以根据样本得到大象的特征模型\(p(x|y&#x3D;0)\)和狗的特征模型\(p(x|y&#x3D;1)\)，同时也可以得到\(p(y)\)。根据贝叶斯公式，有如下：</p>
$$\max (p(y|x)) = \max (\frac{p(x|y)p(y)}{p(x)}) = \max (\frac{p(x|y)p(y)}{p(x|y = 1)p(y = 1) + p(x|y = 0)p(y = 0)})$$


<p>对于我们做预测的时候，可以不考虑分母。</p>
$$\max (p(y|x)) = \max (p(x|y)p(y))$$


<blockquote>
<p>为什么不考虑分母?我们可以将\(p(x|y)p(y)\)简化为\(f(y)\),因此分母就是\(f(1)+f(0)\)。如果我们通过样本对完成了建模，因此分母就是一个常数了，因此没有计算的必要了。</p>
</blockquote>
<h2 id="2-高斯判别分析"><a href="#2-高斯判别分析" class="headerlink" title="2 高斯判别分析"></a>2 高斯判别分析</h2><h3 id="2-1-多维高斯分布介绍"><a href="#2-1-多维高斯分布介绍" class="headerlink" title="2.1 多维高斯分布介绍"></a>2.1 多维高斯分布介绍</h3><p>略，详见cs229-note2.pdf</p>
<h3 id="2-2-高斯判别分析模型"><a href="#2-2-高斯判别分析模型" class="headerlink" title="2.2 高斯判别分析模型"></a>2.2 高斯判别分析模型</h3><p>对于一个分类问题,\(y \in (0,1)\)。假设\(x\)是连续的随机变量，服从高斯分布。具体描述如下：</p>
$$y \sim Bernoulli(\phi)$$$$x|y = 0 \sim N({\mu _0},\Sigma)$$$$x|y = 1 \sim N({\mu _1},\Sigma)$$
$$p(x|y = 0) = \frac{1}{{{(2\pi )}^{\frac{n}{2}}}|\Sigma {|^{\frac{n}{2}}}}\exp ( - \frac{1}{2}{(x - {\mu _0})^T}{\Sigma ^{ - 1}}(x - {\mu _0}))$$
$$p(x|y = 1) = \frac{1}{{{(2\pi )}^{\frac{n}{2}}}|\Sigma {|^{\frac{n}{2}}}}\exp ( - \frac{1}{2}{(x - {\mu _1})^T}{\Sigma ^{ - 1}}(x - {\mu _1}))$$


<h3 id="2-3-公式推导"><a href="#2-3-公式推导" class="headerlink" title="2.3 公式推导"></a>2.3 公式推导</h3><p>我们设置\(\phi  &#x3D; p(y &#x3D; 0)\)。根据前面的分析，在给定样本的情况下，我们需要保证下式最大。</p>
$$\prod\limits _{i = 1}^m {p({y^i}|{x^i})}  = \prod\limits _{i = 1}^m {p({x^i}|{y^i})p({y^i})}$$


<p>这里为了简写，我们设置\({z _0} &#x3D; x - {\mu _0}\) ，\({z _1} &#x3D; x - {\mu _1}\)。取自然对数后，得到最大释然函数:</p>

$$\ell ({\mu _1},{\mu _1},\Sigma ,\phi ) = \sum\limits _{i = 1}^m {\ln \{ {{[\frac{1}{{{{(2\pi )}^{\frac{n}{2}}}|\Sigma {|^{\frac{n}{2}}}}}\exp ( - \frac{1}{2}z _0^T{\Sigma ^{ - 1}}{z _0})]}^{1\{ {y^i} = 0\} }}{{[\frac{1}{{{{(2\pi )}^{\frac{n}{2}}}|\Sigma {|^{\frac{n}{2}}}}}\exp ( - \frac{1}{2}z _1^T{\Sigma ^{ - 1}}{z _1})]}^{1\{ {y^i} = 1\} }}\} } $$
$$\ell ({\mu _0},{\mu _1},\Sigma ,\phi ) = \sum\limits _{i = 1}^m {\{ 1\{ {y^i} = 0\} [ - \frac{1}{2}z _0^T{\Sigma ^{ - 1}}{z _0} - \frac{n}{2}\ln 2\pi  - \frac{n}{2}\ln |\Sigma | + \ln \phi ] + 1\{ {y^i} = 1\} } [ - \frac{1}{2}z _1^T{\Sigma ^{ - 1}}{z _1} - \frac{n}{2}\ln 2\pi  - \frac{n}{2}\ln |\Sigma | + \ln (1 - \phi )]\}$$


<p>然后我们分别对各个参数求偏导数:</p>

$$\frac{{\partial \ell ({\mu _0},{\mu _1},\sum ,\phi )}}{{\partial \phi }} = \sum\limits _{i = 1}^m {(\frac{{1\{ {y^i} = 1\} }}{\phi } - \frac{{1\{ {y^i} = 0\} }}{{1 - \phi }})}  = \sum\limits _{i = 1}^n {(\frac{{1\{ {y^i} = 1\} }}{\phi } - \frac{{1 - 1\{ {y^i} = 1\} }}{{1 - \phi }})}  = 0$$


<p>所以有:</p>

$$\phi  = \frac{1}{m}\sum\limits _{i = 1}^n {1\{ {y^i} = 1\} }$$


<p>然后对\(\mu _0\)求导：</p>

$${\nabla _{{\mu _0}}}\ell ({\mu _0},{\mu _1},\Sigma ,\phi ) = \sum\limits _{i = 1}^m {{\nabla _{{\mu _0}}}( - \frac{1}{2}{{({x^i} - {\mu _0})}^T}{\Sigma ^{ - 1}}({x^i} - {\mu _0}))1\{ {y^i} = 0\} }$$$${\nabla _{{\mu _0}}}\ell ({\mu _0},{\mu _1},\Sigma ,\phi ) =  - \frac{1}{2}\sum\limits _{i = 1}^m {{\nabla _{{\mu _0}}}( - \mu _0^T{\Sigma ^{ - 1}}{x^i} - {{({x^i})}^T}{\Sigma ^{ - 1}}{\mu _0} + \mu _0^T{\Sigma ^{ - 1}}{\mu _0})1\{ {y^i} = 0\} }$$

<p>我们对上面的式子分头计算。</p>
<blockquote>
<p>\(\Sigma\)为对角阵。由于求偏微分的数为一个常数，因此该值与其迹的相同。另外，这里使用了一些关于迹的公式。具体详见附录。</p>
</blockquote>

$${\nabla _{{\mu _0}}}\mu _0^T{\Sigma ^{ - 1}}x = {\nabla _{{\mu _0}}}tr(\mu _0^T{\Sigma ^{ - 1}}{x^i}) = {[{\nabla _{\mu _0^T}}tr(\mu _0^T{\Sigma ^{ - 1}}{x^i})]^T} = {[{({\Sigma ^{ - 1}}{x^i})^T}]^T} = {\Sigma ^{ - 1}}{x^i}$$
$${\nabla _{{\mu _0}}}{({x^i})^T}{\Sigma ^{ - 1}}{\mu _0} = {\nabla _{{\mu _0}}}tr({({x^i})^T}{\Sigma ^{ - 1}}{\mu _0}) = {\nabla _{{\mu _0}}}{\mu _0}{({x^i})^T}{\Sigma ^{ - 1}} = {({({x^i})^T}{\Sigma ^{ - 1}})^T} = {\Sigma ^{ - 1}}{x^i}$$
$${\nabla _{{\mu _0}}}\mu _0^T{\Sigma ^{ - 1}}{\mu _0} = {[{\nabla _{\mu _0^T}}\mu _0^T{\Sigma ^{ - 1}}{\mu _0}E]^T} = [E\mu _0^T{\Sigma ^{ - 1}} + {E^T}\mu _0^T{({\Sigma ^{ - 1}})^T}] = 2{\Sigma ^{ - 1}}{\mu _0}$$


<p>所以有:</p>

$${\nabla _{{\mu _0}}}\ell ({\mu _0},{\mu _1},\Sigma ,\phi ) =  - \sum\limits _{i = 1}^m {{\nabla _{{\mu _0}}}({\Sigma ^{ - 1}}{\mu _0} - {\Sigma ^{ - 1}}{x^i})1\{ {y^i} = 0\} }  = 0$$
$${\mu _0} = \frac{{\sum\limits _{i = 1}^m {1\{ {y^i} = 0\} {x^i}} }}{{\sum\limits _{i = 1}^m {1\{ {y^i} = 0\} } }}$$


<p>同理有:</p>

$${\mu _1} = \frac{{\sum\limits _{i = 1}^m {1\{ {y^i} = 1\} {x^i}} }}{{\sum\limits _{i = 1}^m {1\{ {y^i} = 1\} } }}$$


<p>然后对\(\Sigma^{-1}\)求偏导数。</p>
$${\nabla _{{\Sigma ^{ - 1}}}}\ell ({\mu _0},{\mu _1},\Sigma ,\phi ) = \sum\limits _{i = 1}^m {{\nabla _{{\Sigma ^{ - 1}}}}[( - \frac{1}{2}z _0^T{\Sigma ^{ - 1}}{z _0} + \frac{1}{2}\ln \frac{1}{{|\Sigma |}})1\{ {y^i} = 0\}  + ( - \frac{1}{2}z _1^T{\Sigma ^{ - 1}}{z _1} + \frac{1}{2}\ln \frac{1}{{|\Sigma |}})1\{ {y^i} = 1\} ]}$$


<p>这里分开计算:</p>

$${\nabla _{\Sigma ^{ - 1}}}(z _0^T{\Sigma ^{ - 1}}{z _0}) = {\nabla _{\sum ^{ - 1}}}tr(z _0^T{\Sigma ^{ - 1}}{z _0}) = {\nabla _{\sum ^{ - 1}}}tr({\Sigma ^{ - 1}}{z _0}z _0^T) = {({z _0}z _0^T)^T} = {z _0}z _0^T$$
$${\nabla _{\Sigma ^{ - 1}}}\ln \frac{1}{|\Sigma |} = {\nabla _{\Sigma ^{ - 1}}}\ln |{\Sigma ^{ - 1}}| = \frac{1}{|{\Sigma ^{ - 1}}|}{\nabla _{\Sigma ^{ - 1}}}{\Sigma ^{ - 1}} = {({({\Sigma ^{ - 1}})^{ - 1}})^T}$$


<p>所以有:</p>

$${\nabla _{{\sum ^{ - 1}}}}\ell ({\mu _0},{\mu _1},\Sigma ,\phi ) = \sum\limits _{i = 1}^m {[( - \frac{1}{2}{z _0}z _0^T + \frac{1}{2}\Sigma )1\{ {y^i} = 0\}  + ( - \frac{1}{2}{z _1}z _1^T + \frac{1}{2}\Sigma )1\{ {y^i} = 1\} ]}  = 0$$
$$\Sigma  = \frac{1}{m}\sum\limits _{i = 1}^m {[({x^i} - {\mu _0}){{({x^i} - {\mu _0})}^T}1\{ {y^i} = 0\}  + ({x^i} - {\mu _1}){{({x^i} - {\mu _1})}^T}1\{ {y^i} = 1\} ]}$$
$$\Sigma  = \frac{1}{m}\sum\limits _{i = 1}^m {[({x^i} - {\mu _{{y^i}}}){{({x^i} - {\mu _{{y^i}}})}^T}]}$$


<h2 id="2-3-高斯型算法实例"><a href="#2-3-高斯型算法实例" class="headerlink" title="2.3 高斯型算法实例"></a>2.3 高斯型算法实例</h2><p>我们分别以 \((1,1)\)和\((2,2)\)为均值生成一组高斯分布。下面图是生成的样本。</p>
<img src="/images/机器学习/监督学习-生成型算法GDA样本.png" width=50% height=50% text-align=center/>

<p>前提假设是我们知道两组分类是符合高斯分布的，切假定协方差相同。但我们不知道两组数据的均值和协方差。根据前面的公式有如下代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> multivariate_normal</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> matplotlib.lines <span class="keyword">import</span> Line2D</span><br><span class="line"></span><br><span class="line"><span class="comment">## this is a program about GDA</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">global</span> MAKE_DATA</span><br><span class="line"><span class="keyword">global</span> SHOW_PIC</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_data</span>(<span class="params">x</span>):</span><br><span class="line">  <span class="keyword">if</span> MAKE_DATA == <span class="literal">True</span>:</span><br><span class="line">    mean_1 = [<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">    mean_2 = [<span class="number">2</span>,<span class="number">2</span>]</span><br><span class="line">    cov = [[<span class="number">0.1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.1</span>]]</span><br><span class="line">    arr1 = np.random.multivariate_normal(mean_1, cov, <span class="number">100</span>)</span><br><span class="line">    arr2 = np.random.multivariate_normal(mean_2, cov, <span class="number">50</span>)</span><br><span class="line">    <span class="built_in">print</span>(arr1)</span><br><span class="line">    <span class="built_in">print</span>(arr2)</span><br><span class="line">    x.append(arr1)</span><br><span class="line">    x.append(arr2)</span><br><span class="line">  <span class="keyword">if</span> MAKE_DATA == <span class="literal">False</span>:</span><br><span class="line">    arr1 = np.array([[ <span class="number">0.88235916</span> , <span class="number">1.01511634</span>],[ <span class="number">0.75243817</span> , <span class="number">0.76520033</span>],[ <span class="number">0.95710848</span> , <span class="number">1.41894337</span>],[ <span class="number">1.48682891</span> , <span class="number">0.78885043</span>],[ <span class="number">1.24047011</span> , <span class="number">0.71984948</span>],[ <span class="number">0.67611276</span> , <span class="number">1.07909452</span>],[ <span class="number">1.03243669</span> , <span class="number">1.08929695</span>],[ <span class="number">1.0296548</span>  , <span class="number">1.25023769</span>],[ <span class="number">1.54134008</span> , <span class="number">0.39564824</span>],[ <span class="number">0.34645057</span> , <span class="number">1.61499636</span>],[ <span class="number">0.77206174</span> , <span class="number">1.23613698</span>],[ <span class="number">0.91446988</span> , <span class="number">1.38537765</span>],[ <span class="number">0.99982962</span> , <span class="number">1.34448471</span>],[ <span class="number">0.78745962</span> , <span class="number">0.9046565</span> ],[ <span class="number">0.74946602</span> , <span class="number">1.07424473</span>],[ <span class="number">1.09294839</span> , <span class="number">1.14711993</span>],[ <span class="number">0.39266844</span> , <span class="number">0.78788004</span>],[ <span class="number">0.83112357</span> , <span class="number">1.2762774</span> ],[ <span class="number">1.05056188</span> , <span class="number">1.13351562</span>],[ <span class="number">1.62101523</span> , <span class="number">1.15035562</span>],[ <span class="number">0.70377517</span> , <span class="number">1.1136416</span> ],[ <span class="number">1.03715472</span> , <span class="number">0.47905693</span>],[ <span class="number">0.94598381</span> , <span class="number">0.8874837</span> ],[ <span class="number">0.94447128</span> , <span class="number">2.02796925</span>],[ <span class="number">0.72442242</span> , <span class="number">1.09835206</span>],[ <span class="number">0.69046731</span> , <span class="number">1.46232182</span>],[ <span class="number">1.20744606</span> , <span class="number">1.10280041</span>],[ <span class="number">0.70665746</span> , <span class="number">0.82139503</span>],[ <span class="number">1.08803887</span> , <span class="number">1.4450361</span> ],[ <span class="number">0.88530961</span> , <span class="number">0.75727475</span>],[ <span class="number">0.98418545</span> , <span class="number">0.80248161</span>],[ <span class="number">0.74970386</span> , <span class="number">1.13205709</span>],[ <span class="number">0.72586454</span> , <span class="number">1.06058385</span>],[ <span class="number">0.9071812</span>  , <span class="number">1.09975063</span>],[ <span class="number">0.75182835</span> , <span class="number">0.93570147</span>],[ <span class="number">0.80052289</span> , <span class="number">1.08168507</span>],[ <span class="number">0.40180652</span> , <span class="number">0.9526211</span> ],[ <span class="number">0.62312617</span> , <span class="number">0.84385058</span>],[ <span class="number">0.68212516</span> , <span class="number">1.25912717</span>],[ <span class="number">1.19773245</span> , <span class="number">0.16399654</span>],[ <span class="number">0.96093132</span> , <span class="number">0.43932091</span>],[ <span class="number">1.25471657</span> , <span class="number">0.92371829</span>],[ <span class="number">1.12330272</span> , <span class="number">1.26968747</span>],[ <span class="number">1.30361985</span> , <span class="number">0.99862123</span>],[ <span class="number">1.23477665</span> , <span class="number">1.1742804</span> ],[ <span class="number">0.28471876</span> , <span class="number">0.5806044</span> ],[ <span class="number">1.89355099</span> , <span class="number">1.19928671</span>],[ <span class="number">1.09081369</span> , <span class="number">1.28467312</span>],[ <span class="number">1.40488635</span> , <span class="number">0.90034427</span>],[ <span class="number">1.11672364</span> , <span class="number">1.49070515</span>],[ <span class="number">1.35385212</span> , <span class="number">1.35767891</span>],[ <span class="number">0.92746374</span> , <span class="number">1.79096697</span>],[ <span class="number">1.89142562</span> , <span class="number">0.98228303</span>],[ <span class="number">1.0555218</span>  , <span class="number">0.86070833</span>],[ <span class="number">0.69001255</span> , <span class="number">1.12874741</span>],[ <span class="number">0.98137315</span> , <span class="number">1.3398852</span> ],[ <span class="number">1.02525371</span> , <span class="number">0.77572865</span>],[ <span class="number">1.1354295</span>  , <span class="number">1.07098552</span>],[ <span class="number">1.50829164</span> , <span class="number">1.43065998</span>],[ <span class="number">1.09928764</span> , <span class="number">1.55540292</span>],[ <span class="number">0.64695084</span> , <span class="number">0.79920395</span>],[ <span class="number">0.82059034</span> , <span class="number">0.97533491</span>],[ <span class="number">0.56345455</span> , <span class="number">1.08168272</span>],[ <span class="number">1.06673215</span> , <span class="number">1.19448556</span>],[ <span class="number">0.96512548</span> , <span class="number">1.5268577</span> ],[ <span class="number">0.96914451</span> , <span class="number">1.00902985</span>],[ <span class="number">0.72879413</span> , <span class="number">0.92476415</span>],[ <span class="number">1.0931483</span>  , <span class="number">1.13572242</span>],[ <span class="number">1.34765121</span> , <span class="number">0.83841006</span>],[ <span class="number">1.57813788</span> , <span class="number">0.65915892</span>],[ <span class="number">0.59032608</span> , <span class="number">0.82747946</span>],[ <span class="number">0.83838504</span> , <span class="number">0.67588473</span>],[ <span class="number">1.35101322</span> , <span class="number">1.21027851</span>],[ <span class="number">0.71762153</span> , <span class="number">0.41839038</span>],[ <span class="number">0.61295604</span> , <span class="number">0.66555018</span>],[ <span class="number">0.64379346</span> , <span class="number">0.92925228</span>],[ <span class="number">1.1194968</span>  , <span class="number">0.65876736</span>],[ <span class="number">0.39495437</span> , <span class="number">0.67246734</span>],[ <span class="number">1.05223282</span> , <span class="number">0.17889116</span>],[ <span class="number">0.97810984</span> , <span class="number">1.12794664</span>],[ <span class="number">0.98392719</span> , <span class="number">0.73590255</span>],[ <span class="number">1.25587405</span> , <span class="number">1.21853038</span>],[ <span class="number">1.01150226</span> , <span class="number">1.01835571</span>],[ <span class="number">1.02251614</span> , <span class="number">0.72704228</span>],[ <span class="number">1.00261519</span> , <span class="number">0.95347185</span>],[ <span class="number">0.96362523</span> , <span class="number">0.8607009</span> ],[ <span class="number">0.88034659</span> , <span class="number">1.2307104</span> ],[ <span class="number">0.75907236</span> , <span class="number">0.92799796</span>],[ <span class="number">0.54898709</span> , <span class="number">1.69882285</span>],[ <span class="number">0.55032649</span> , <span class="number">0.98831566</span>],[ <span class="number">1.33360789</span> , <span class="number">1.19793298</span>],[ <span class="number">0.83231239</span> , <span class="number">0.8946538</span> ],[ <span class="number">1.05173094</span> , <span class="number">1.26324289</span>],[ <span class="number">0.81482231</span> , <span class="number">0.56198584</span>],[ <span class="number">1.03854797</span> , <span class="number">1.0553811</span> ],[ <span class="number">1.32669227</span> , <span class="number">1.61115811</span>],[ <span class="number">1.13322152</span> , <span class="number">1.68151695</span>],[ <span class="number">0.39754618</span> , <span class="number">1.19392967</span>],[ <span class="number">0.61344185</span> , <span class="number">1.05281434</span>],[ <span class="number">1.18415366</span> , <span class="number">0.864884</span>  ]])</span><br><span class="line">    arr2 = np.array([[ <span class="number">2.15366548</span> , <span class="number">1.88035458</span>],[ <span class="number">2.36978774</span> , <span class="number">1.76550283</span>],[ <span class="number">2.46261387</span> , <span class="number">2.10568262</span>],[ <span class="number">1.90475526</span> , <span class="number">1.95242885</span>],[ <span class="number">1.77712677</span> , <span class="number">1.96004856</span>],[ <span class="number">1.5995514</span>  , <span class="number">2.1323943</span> ],[ <span class="number">1.52727223</span> , <span class="number">1.50295551</span>],[ <span class="number">1.80330407</span> , <span class="number">1.57942301</span>],[ <span class="number">1.86487049</span> , <span class="number">1.87234414</span>],[ <span class="number">1.9586354</span>  , <span class="number">1.96279729</span>],[ <span class="number">2.59668134</span> , <span class="number">2.414423</span>  ],[ <span class="number">2.818419</span>   , <span class="number">1.76280366</span>],[ <span class="number">2.01511628</span> , <span class="number">2.10858546</span>],[ <span class="number">2.15907962</span> , <span class="number">1.81593012</span>],[ <span class="number">1.63966834</span> , <span class="number">2.2209023</span> ],[ <span class="number">2.47220599</span> , <span class="number">1.70482956</span>],[ <span class="number">2.08760748</span> , <span class="number">2.51601971</span>],[ <span class="number">1.50547722</span> , <span class="number">1.8487145</span> ],[ <span class="number">1.68125583</span> , <span class="number">2.64968501</span>],[ <span class="number">2.01924282</span> , <span class="number">2.0953572</span> ],[ <span class="number">2.22563534</span> , <span class="number">2.18266325</span>],[ <span class="number">2.2684291</span>  , <span class="number">2.23581599</span>],[ <span class="number">2.13787557</span> , <span class="number">1.9999382</span> ],[ <span class="number">1.02638695</span> , <span class="number">1.68134967</span>],[ <span class="number">2.35614619</span> , <span class="number">1.32072125</span>],[ <span class="number">2.20054871</span> , <span class="number">1.47401445</span>],[ <span class="number">1.99454827</span> , <span class="number">1.71658741</span>],[ <span class="number">1.83269065</span> , <span class="number">2.47662909</span>],[ <span class="number">2.40097251</span> , <span class="number">2.21823862</span>],[ <span class="number">2.54404652</span> , <span class="number">1.85742018</span>],[ <span class="number">1.84150027</span> , <span class="number">2.06350351</span>],[ <span class="number">1.69490855</span> , <span class="number">1.70169334</span>],[ <span class="number">1.44745704</span> , <span class="number">1.88295233</span>],[ <span class="number">2.24376639</span> , <span class="number">1.67530495</span>],[ <span class="number">1.42911921</span> , <span class="number">1.81854548</span>],[ <span class="number">1.33789289</span> , <span class="number">2.27686128</span>],[ <span class="number">2.43509821</span> , <span class="number">1.95032131</span>],[ <span class="number">1.9512447</span>  , <span class="number">1.4595415</span> ],[ <span class="number">2.13041192</span> , <span class="number">1.79372755</span>],[ <span class="number">2.2753866</span>  , <span class="number">2.23781951</span>],[ <span class="number">2.26753401</span> , <span class="number">1.78149305</span>],[ <span class="number">2.06505449</span> , <span class="number">2.01939606</span>],[ <span class="number">2.44426826</span> , <span class="number">2.1437101</span> ],[ <span class="number">2.16607141</span> , <span class="number">2.31077167</span>],[ <span class="number">1.96097237</span> , <span class="number">2.49100193</span>],[ <span class="number">1.37255424</span> , <span class="number">1.60735016</span>],[ <span class="number">1.63947758</span> , <span class="number">2.17852314</span>],[ <span class="number">2.13722666</span> , <span class="number">2.00559707</span>],[ <span class="number">1.222696</span>   , <span class="number">1.67075059</span>],[ <span class="number">2.56982685</span> , <span class="number">2.51218813</span>]])</span><br><span class="line">    x.append(arr1)</span><br><span class="line">    x.append(arr2)</span><br><span class="line">  <span class="keyword">if</span> SHOW_PIC == <span class="number">1</span>:</span><br><span class="line">    figure, ax = plt.subplots()</span><br><span class="line">    ax.set_xlim(left=-<span class="number">1</span>, right=<span class="number">4</span>)</span><br><span class="line">    ax.set_ylim(bottom=-<span class="number">1</span>, top=<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(arr1)):</span><br><span class="line">      plt.plot(arr1[i][<span class="number">0</span>], arr1[i][<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;+&#x27;</span>, color=<span class="string">&#x27;g&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(arr2)):</span><br><span class="line">      plt.plot(arr2[i][<span class="number">0</span>], arr2[i][<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, color=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;x1&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;x2&quot;</span>)</span><br><span class="line">    plt.plot()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcPhi</span>(<span class="params">x</span>):</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">len</span>(x[<span class="number">1</span>])/(<span class="built_in">len</span>(x[<span class="number">0</span>])+<span class="built_in">len</span>(x[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcMu</span>(<span class="params">x,i</span>):</span><br><span class="line">  <span class="keyword">return</span> x[i].mean(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcSigma</span>(<span class="params">x,mu_0,mu_1</span>):</span><br><span class="line">  <span class="built_in">sum</span>=np.array([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>]])</span><br><span class="line">  x0=x[<span class="number">0</span>]</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x0)):</span><br><span class="line">    z0=np.array([x0[i]-mu_0])</span><br><span class="line">    z0T = np.array([x0[i]-mu_0]).transpose()</span><br><span class="line">    <span class="built_in">sum</span> = <span class="built_in">sum</span> + np.dot(z0T, z0)</span><br><span class="line">    <span class="built_in">print</span>(np.dot(z0T, z0))</span><br><span class="line">  x1=x[<span class="number">1</span>]</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x1)):</span><br><span class="line">    z1=np.array([x1[i]-mu_1])</span><br><span class="line">    z1T = np.array([x1[i]-mu_1]).transpose()</span><br><span class="line">    <span class="built_in">sum</span> = <span class="built_in">sum</span> + np.dot(z1T, z1)</span><br><span class="line">    <span class="built_in">print</span>(np.dot(z1T, z1))</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">sum</span>/(<span class="built_in">len</span>(x0)+<span class="built_in">len</span>(x1))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">classify</span>(<span class="params">x,mu_0,mu_1,sigma,phi</span>):</span><br><span class="line">  var0 = multivariate_normal(mean=mu_0.tolist(), cov=sigma.tolist())</span><br><span class="line">  var1 = multivariate_normal(mean=mu_1.tolist(), cov=sigma.tolist())</span><br><span class="line">  <span class="comment"># p(y=1|x) = p(x|y=1)/p(y=1) / p(x) = p(x|y=1)p(y=1) / (p(x|y=1)p(y=1)+p(x|y=0)p(y=0))</span></span><br><span class="line">  <span class="keyword">return</span> var1.pdf(x)*phi/(var1.pdf(x)*phi+var0.pdf(x)*(<span class="number">1</span>-phi)) &gt; <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">testClassify</span>(<span class="params">mu_0,mu_1,sigma,phi</span>):</span><br><span class="line">  <span class="keyword">if</span> SHOW_PIC != <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">  figure, ax = plt.subplots()</span><br><span class="line">  ax.set_xlim(left=-<span class="number">1</span>, right=<span class="number">4</span>)</span><br><span class="line">  ax.set_ylim(bottom=-<span class="number">1</span>, top=<span class="number">4</span>)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    x1 = random.randint(<span class="number">0</span>,<span class="number">3000</span>)/<span class="number">1000</span></span><br><span class="line">    x2 = random.randint(<span class="number">0</span>,<span class="number">3000</span>)/<span class="number">1000</span></span><br><span class="line">    <span class="keyword">if</span>( classify([x1,x2],mu_0,mu_1,sigma,phi)) == <span class="literal">True</span>:</span><br><span class="line">      plt.plot(x1, x2, <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;+&#x27;</span>, color=<span class="string">&#x27;g&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      plt.plot(x1, x2, <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, color=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">  <span class="comment"># draw y = -x+3</span></span><br><span class="line">  (line1_xs, line1_ys) = [(<span class="number">0</span>, <span class="number">3</span>), (<span class="number">3</span>, <span class="number">0</span>)]</span><br><span class="line">  ax.add_line(Line2D(line1_xs, line1_ys, linewidth=<span class="number">1</span>, color=<span class="string">&#x27;blue&#x27;</span>))</span><br><span class="line">  plt.xlabel(<span class="string">&quot;x1&quot;</span>)</span><br><span class="line">  plt.ylabel(<span class="string">&quot;x2&quot;</span>)</span><br><span class="line">  plt.plot()</span><br><span class="line">  plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">  <span class="comment"># 0 debug option</span></span><br><span class="line">  MAKE_DATA = <span class="literal">False</span></span><br><span class="line">  SHOW_PIC = <span class="number">1</span>        <span class="comment"># 0 - do not show   1 - show sample data    2 - show test data</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 1 make the sample</span></span><br><span class="line">  <span class="comment"># the format of x is [ndarray0,ndarray1] , ndarray1 is the set of the first class, we set y=0.</span></span><br><span class="line">  <span class="comment"># ndarray1 is the set of the second class, we set y=1</span></span><br><span class="line">  x = []</span><br><span class="line">  make_data(x)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> MAKE_DATA == <span class="literal">True</span>:</span><br><span class="line">    exit(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 2 learn from the sample</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;hello&quot;</span>)</span><br><span class="line">  phi = calcPhi(x)                <span class="comment"># phi  = p(y=1)  means close (2,2)</span></span><br><span class="line">  mu_0 = calcMu(x,<span class="number">0</span>)</span><br><span class="line">  mu_1 = calcMu(x,<span class="number">1</span>)</span><br><span class="line">  sigma = calcSigma(x,mu_0,mu_1)</span><br><span class="line">  <span class="built_in">print</span>(phi,<span class="string">&quot;, &quot;</span>,mu_0,<span class="string">&quot;, &quot;</span>,mu_1)</span><br><span class="line">  <span class="built_in">print</span>(sigma.tolist())</span><br><span class="line"></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;---------&quot;</span>)</span><br><span class="line">  <span class="comment"># 3 test classify</span></span><br><span class="line">  testClassify(mu_0,mu_1,sigma,phi)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>对于两个协方差相同的样本，我们知道两组数据有相同的概率分布曲线，具体都应该是一个个同心圆。因此，我们可以确定一条垂直于两点连线的曲线是对该样本的理论上正确的分割。因此，我们得到如下结果，并与理论分割的曲线，即\(y&#x3D;-x+3\)比较，发现对我们随机产生的样本分类效果非常好。</p>
<img src="/images/机器学习/监督学习-生成型算法GDA结果.png" width=50% height=50% text-align=center/>


<h2 id="附录-A-相关公式"><a href="#附录-A-相关公式" class="headerlink" title="附录 A 相关公式"></a>附录 A 相关公式</h2><p>Hessian矩阵定义:</p>
$${\nabla _A}f(A) = \left( {\begin{array}{*{20}{c}}  {\frac{{\partial f}}{{\partial {A _{11}}}}}& \ldots &{\frac{{\partial f}}{{\partial {A _{1n}}}}} \\\    \vdots & \ddots & \vdots  \\\   {\frac{{\partial f}}{{\partial {A _{n1}}}}}& \cdots &{\frac{{\partial f}}{{\partial {A _{nn}}}}} \end{array}} \right)$$



<p>矩阵的迹的定义:</p>

$$trA = \sum\limits _{i = 1}^n {{A _{ii}}}$$


<p>关于矩阵迹的公式:</p>

$$tr(a) = a$$
$$tr(aA) = atr(A)$$
$$tr(aA) = atr(A)$$
$$tr(ABC) = tr(CAB) + tr(BCA)$$
$$trA = tr{A^T}$$
$$tr(A + B) = trA + trB$$
$${\nabla _A}tr(AB) = {B^T}$$
$${\nabla _{A^T}}f(A) = {({\nabla _A}f(A))^T}$$
$${\nabla _{A^T}}trAB{A^T}C = CAB + {C^T}A{B^T}$$
$${\nabla _{A^T}}trAB{A^T}C = CAB + {C^T}A{B^T}$$


<blockquote>
<p>对矩阵做展开能证明上述大部分公式，这里暂时略。</p>
</blockquote>
<h2 id="附录B"><a href="#附录B" class="headerlink" title="附录B"></a>附录B</h2><p>手写公式和word版博客地址:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/zhengchenyu/MyBlog/tree/master/doc/mlearning/生成型学习算法</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2017/08/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-5-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%94%9F%E6%88%90%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" data-id="cuidxlAK0gHKDmDQAb_8lkInN" data-title="机器学习-2.5-监督学习之生成型学习算法" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-机器学习-2-4-监督学习之softmax" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2017/07/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-4-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8Bsoftmax/" class="article-date">
  <time class="dt-published" datetime="2017-07-24T10:35:50.000Z" itemprop="datePublished">2017-07-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2017/07/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-4-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8Bsoftmax/">机器学习-2.4-监督学习之softmax</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="Softmax-Regression"><a href="#Softmax-Regression" class="headerlink" title="Softmax Regression"></a>Softmax Regression</h1><p>下面介绍一下指数族分布的另外一个例子。之前的逻辑回归中，可以用来解决二分类的问题。对于有k中可能结果的时候，问题就转化为多分类问题了，也就是接下来要说明的softmax regression问题。<br>s</p>
<h2 id="1-函数定义"><a href="#1-函数定义" class="headerlink" title="1 函数定义"></a>1 函数定义</h2><p>在进行下一步推导前，我们先定义部分辅助函数。我们这里事先定义一个k-1维向量\(T(y)\)，这里具体对应于指数族分布的\(T(y)\),具体定义如下:</p>

$$T(1) = \left[ \begin{gathered}  1 \hfill \\\  0 \hfill \\\  0 \hfill \\\  ... \hfill \\\  0 \hfill \\ \end{gathered}  \right],T(2) = \left[ \begin{gathered}  0 \hfill \\\  1 \hfill \\\  0 \hfill \\\  ... \hfill \\\  0 \hfill \\ \end{gathered}  \right],T(3) = \left[ \begin{gathered}  0 \hfill \\\  0 \hfill \\\  1 \hfill \\\  ... \hfill \\\  0 \hfill \\\ \end{gathered}  \right],...,T(k - 1) = \left[ \begin{gathered}  0 \hfill \\\  0 \hfill \\\  0 \hfill \\\  ... \hfill \\\  1 \hfill \\ \end{gathered}  \right],T(k) = \left[ \begin{gathered}  0 \hfill \\\  0 \hfill \\\  0 \hfill \\\  ... \hfill \\\  0 \hfill \\ \end{gathered}  \right]$$


<p>我们定义\(T(y) _1\)表示的\(T(y)\)第一个元素，其他依次类推。</p>
<p>然后再引入一个函数，具体定义如下:</p>
$$1\{ true\} = 1$$
$$1\{ false\} = 0$$

<blockquote>
<p>可以通过带入值的方式验证上式。</p>
</blockquote>
<h2 id="2模型推导"><a href="#2模型推导" class="headerlink" title="2	模型推导"></a>2	模型推导</h2><p>在softmax中,我们知道\(y \in \{ 1,2,…,k\}\)。我们设置\(y&#x3D;1\)的概率为\(\phi _1\),<br>类似的有\(y&#x3D;k\)的概率为\(\phi _k\)，并且我们轻易得到如下公式：</p>
$$\sum\limits _{i = 1}^k {\phi _i}  = 1$$$${\phi _k} = \sum\limits _{i = 1}^{k - 1}{\phi _i}$$


<p>由于我们已经知道\(p(y &#x3D; i) &#x3D; {\phi _i}\)，综上可以得到:</p>

$$p(y) = \phi _1^{1\{ y = 1\} }\phi _2^{1\{ y = 2\} }...\phi _k^{1\{ y = k\} } = \phi _1^{1\{ y = 1\} }\phi _2^{1\{ y = 2\} }...\phi _k^{1 - \sum\limits _{i = 1}^{k - 1} {1\{ y = i\} } } = \phi _1^{T{(y)} _1}\phi _2^{T{{(y)} _2}}...\phi _k^{1 - \sum\limits _{i = 1}^{k - 1} {T{(y)} _i} }$$$$p(y) = \exp (T{(y) _1}\ln {\phi _1} + T{(y) _2}\ln {\phi _2} + ... + (1 - \sum\limits _{i = 1}^{k - 1} {T{{(y)} _i}} )\ln {\phi _k})$$$$p(y) = \exp (T{(y) _1}\ln \frac{\phi _1}{\phi _k} + T{(y) _2}\ln \frac{\phi _2}{\phi _k} + ... + T{(y) _{k - 1}}\ln \frac{\phi _{k - 1}}{\phi _k} + \ln {\phi _k})$$


<p>我们对照指数族分布，其中\(T(y)\)我们已经定义，可以得到其他参数如下:</p>

$$b(y) = 1$$
$$a(\eta ) =  - \ln {\phi _k}$$
$$\eta  = \left[ \begin{gathered}  \ln \frac{\phi _1}{\phi _k} \hfill \\\  \ln \frac{\phi _2}{\phi _k} \hfill \\\  ... \hfill \\\  \ln \frac{\phi _{k - 1}}{\phi _k} \hfill \\\ \end{gathered}  \right]$$


<p>我们用\(\eta\)来表示\(\phi\)，目标是将我们的问题归整为一个变量的问题，从而更容易计算。对上面向量拆开计算得到:</p>

$${\eta _i} = \ln \frac{\phi _i}{\phi _k}$$


<p>继而可以转化为:</p>

$${\eta _i} = \ln \frac{\phi _i}{\phi _k}$$


<p>又由于我们已知\(\sum\limits _{j &#x3D; 1}^k {\phi _j}  &#x3D; 1\)，所以有:</p>

$$\sum\limits _{i = 1}^k {e^{\eta _i}{\phi _k}}  = 1$$


<p>继而有:</p>

$${\phi _k} = \frac{1}{\sum\limits _{j = 1}^k {e^{\eta _j}}}$$


<p>最后我们可以得到:</p>

$${\phi _i} = \frac{e^{\eta _i}}{\sum\limits _{j = 1}^k {e^{\eta _j}}}$$


<p>由于指数组分布假设是关于输入的线性函数，所以得到在已知\(x\)和\(\theta\)的情况下\(y&#x3D;i\)的概率的公式，如下：</p>
$$p(y = i|x,\theta ) = {\phi _i} = \frac{e^{\eta _i}}{\sum\limits _{j = 1}^k {e^{\eta _j}}} = \frac{e^{\theta _i^Tx}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}}$$


<p>这里我们可以得到\({h_\theta }(x)\),如下:</p>
$${h_\theta }(x) = E[T(y)|x,\theta ] = E[\left( \begin{gathered}  1\{ y = 1\}  \hfill \\\  1\{ y = 2\}  \hfill \\\  ... \hfill \\\  1\{ y = k - 1\}  \hfill \\\ \end{gathered}  \right)|x,\theta ]$$

$${h_ \theta }(x) = E[\left( \begin{gathered}  {\phi _1} \hfill \\\  {\phi _2} \hfill \\\  ... \hfill \\\  {\phi _{k - 1}} \hfill \\\ \end{gathered}  \right)|x,\theta ] = \left( \begin{gathered}  \frac{e^{\theta _1^Tx}}{\sum\limits _{j = 1}^k {e^{\theta  _j^Tx}}} \hfill \\\  \frac{e^{\theta _2^Tx}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}} \hfill \\\  ... \hfill \\\  \frac{e^{\theta _{k - 1}^Tx}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}} \hfill \\\\end{gathered}  \right)$$


<blockquote>
<p>其中,\(k\)为取值的可能集合的大小，\(\theta\)为一个\(k*n\)的矩阵。</p>
</blockquote>
<p>到这里我们可以定义我们的损失函数了，如下:</p>
$$J(x) = \frac{1}{2}\sum\limits _{i = 1}^m {{(T(y) - {h _\theta }(x))}^2}$$


<p>上式是一个向量，我们可以对这个向量继续求平方和，来衡量准确度，如下：</p>
$$J(x) = \frac{1}{2}\sum\limits _{i = 1}^m {|{{(T(y) - {h _\theta }(x))}^2}|}$$


<p>损失函数已经定义完成，我们就有了算法的截止条件了。接下来就是找到算法的最优迭代方向，也就是计算其偏导数了。</p>
<p>我们使用最大释然性来计算迭代方向。建模的原则是：对于一个已知\(y&#x3D;i\)的样本\((x,y)\)，我们需要找到一个方向去迭代\(\theta\)，使得\(\phi _i\)尽可能大，使得其他\(\phi\)尽可能小。当然由于所有\(\phi\)之和为1,所以我们只需要保证\(\phi _i\)尽可能大即可。因此,在已经的\(m\)个样本的情况下，我们只需要保证下面的式子最大:</p>

$$\sum\limits _{i = 1}^m {p(y = {y^i}|x,\theta )}  = \sum\limits _{i = 1}^m {\prod\limits _{l = 1}^k {p{(y = l|x,\theta )}^{1\{ {y^i} = l\}}} }$$


<p>取自然对数后，我们定义最大似然函数:</p>

$$l(\theta ) = \sum\limits _{i = 1}^m {\ln \prod\limits _{l = 1}^k {(\frac{e^{\theta _l^Tx}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}})^{1\{{y^i} = l\}}}}$$

<p>当且仅当\({y^i} &#x3D; l\)的时候, \(^{1{y^i&#x3D;l\} &#x3D; 1})。其他值为0，我们可以将连乘转化，如下：</p>
$$l(\theta ) = \sum\limits _{i = 1}^m {\ln (\frac{e^{\theta _{y^i}^Tx}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}})}  = \sum\limits _{i = 1}^m {\ln [\theta _{y^i}^Tx - \ln \sum\limits _{j = 1}^k {e^{\theta _j^Tx}}]} $$


<p>然后对其求偏导数,对\(y _i&#x3D;f\)的时候后有：</p>
$$\frac{\partial l(\theta )}{\partial {\theta _f}} = \sum\limits _{i = 1}^m {\ln [{x _f} - \frac{e^{\theta _f^Tx}{x _f}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}}]}$$


<p>对\({y_i} \ne f\)的时候，有：</p>
$$\frac{\partial l(\theta )}{\partial {\theta _f}} = \sum\limits _{i = 1}^m {\ln [0 - \frac{e^{\theta _f^Tx}{x _f}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}}]}$$


<p>综上，可以得到:</p>
$$\frac{\partial l(\theta )}{\partial {\theta _f}} = \sum\limits _{i = 1}^m {\ln [1\{ {y^i} = f\}  - \frac{e^{\theta _f^Tx}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}}]{x _f}}  = \sum\limits _{i = 1}^m {\ln [1\{{y^i} = f\}  - {\phi _f}]{x _f}}$$


<blockquote>
<p>\(x _f\)是一个向量，因为公式一次迭代更新一个维度下的一组\(\theta\)。事实上，这里是一个向量求微分。如果我们对的某个元素\(\theta\)进行微分，我们依然能够得到这个公式。</p>
</blockquote>
<h2 id="3-实际问题的解决"><a href="#3-实际问题的解决" class="headerlink" title="3 实际问题的解决"></a>3 实际问题的解决</h2><p>我们随机制造一组数据，在\(2*2\)的空间内使用直线\({x _2} &#x3D; 0.5*{x _1}\)和直线\({x _2} &#x3D; -0.5*{x _1}+2\)具体的分类如下:</p>
<img src="/images/机器学习/监督学习-softmax样本.png" width=50% height=50% text-align=center/>

<p>根据之前的推导，编写如下代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.lines <span class="keyword">import</span> Line2D</span><br><span class="line"></span><br><span class="line">g_x_arr=[[<span class="number">1</span>, <span class="number">0.035</span>, <span class="number">1.344</span>], [<span class="number">1</span>, <span class="number">0.662</span>, <span class="number">0.598</span>], [<span class="number">1</span>, <span class="number">1.791</span>, <span class="number">1.889</span>], [<span class="number">1</span>, <span class="number">0.158</span>, <span class="number">0.12</span>], [<span class="number">1</span>, <span class="number">1.55</span>, <span class="number">1.835</span>], [<span class="number">1</span>, <span class="number">2.0</span>, <span class="number">0.613</span>], [<span class="number">1</span>, <span class="number">1.176</span>, <span class="number">0.368</span>], [<span class="number">1</span>, <span class="number">0.564</span>, <span class="number">0.043</span>], [<span class="number">1</span>, <span class="number">1.559</span>, <span class="number">1.507</span>], [<span class="number">1</span>, <span class="number">1.998</span>, <span class="number">0.988</span>], [<span class="number">1</span>, <span class="number">0.082</span>, <span class="number">0.941</span>], [<span class="number">1</span>, <span class="number">0.542</span>, <span class="number">1.371</span>], [<span class="number">1</span>, <span class="number">0.542</span>, <span class="number">0.671</span>], [<span class="number">1</span>, <span class="number">1.34</span>, <span class="number">1.856</span>], [<span class="number">1</span>, <span class="number">0.049</span>, <span class="number">0.089</span>], [<span class="number">1</span>, <span class="number">1.933</span>, <span class="number">0.871</span>], [<span class="number">1</span>, <span class="number">1.753</span>, <span class="number">1.024</span>], [<span class="number">1</span>, <span class="number">0.315</span>, <span class="number">1.341</span>], [<span class="number">1</span>, <span class="number">0.829</span>, <span class="number">1.26</span>], [<span class="number">1</span>, <span class="number">0.686</span>, <span class="number">1.721</span>], [<span class="number">1</span>, <span class="number">1.222</span>, <span class="number">1.129</span>], [<span class="number">1</span>, <span class="number">0.55</span>, <span class="number">0.075</span>], [<span class="number">1</span>, <span class="number">0.767</span>, <span class="number">0.346</span>], [<span class="number">1</span>, <span class="number">1.516</span>, <span class="number">1.752</span>], [<span class="number">1</span>, <span class="number">1.347</span>, <span class="number">0.905</span>], [<span class="number">1</span>, <span class="number">0.127</span>, <span class="number">0.782</span>], [<span class="number">1</span>, <span class="number">1.169</span>, <span class="number">1.272</span>], [<span class="number">1</span>, <span class="number">1.301</span>, <span class="number">0.273</span>], [<span class="number">1</span>, <span class="number">0.081</span>, <span class="number">0.739</span>], [<span class="number">1</span>, <span class="number">0.203</span>, <span class="number">0.658</span>], [<span class="number">1</span>, <span class="number">0.347</span>, <span class="number">1.064</span>], [<span class="number">1</span>, <span class="number">0.793</span>, <span class="number">1.193</span>], [<span class="number">1</span>, <span class="number">1.428</span>, <span class="number">0.326</span>], [<span class="number">1</span>, <span class="number">0.509</span>, <span class="number">0.983</span>], [<span class="number">1</span>, <span class="number">0.12</span>, <span class="number">0.884</span>], [<span class="number">1</span>, <span class="number">0.251</span>, <span class="number">0.282</span>], [<span class="number">1</span>, <span class="number">0.73</span>, <span class="number">0.445</span>], [<span class="number">1</span>, <span class="number">1.889</span>, <span class="number">1.323</span>], [<span class="number">1</span>, <span class="number">1.314</span>, <span class="number">1.795</span>], [<span class="number">1</span>, <span class="number">1.297</span>, <span class="number">1.467</span>], [<span class="number">1</span>, <span class="number">1.669</span>, <span class="number">0.613</span>], [<span class="number">1</span>, <span class="number">0.753</span>, <span class="number">0.114</span>], [<span class="number">1</span>, <span class="number">0.94</span>, <span class="number">1.972</span>], [<span class="number">1</span>, <span class="number">0.738</span>, <span class="number">1.603</span>], [<span class="number">1</span>, <span class="number">1.508</span>, <span class="number">1.237</span>], [<span class="number">1</span>, <span class="number">0.979</span>, <span class="number">0.572</span>], [<span class="number">1</span>, <span class="number">0.128</span>, <span class="number">1.254</span>], [<span class="number">1</span>, <span class="number">0.569</span>, <span class="number">0.155</span>], [<span class="number">1</span>, <span class="number">0.88</span>, <span class="number">0.211</span>], [<span class="number">1</span>, <span class="number">0.405</span>, <span class="number">0.603</span>], [<span class="number">1</span>, <span class="number">1.02</span>, <span class="number">1.9</span>], [<span class="number">1</span>, <span class="number">0.438</span>, <span class="number">1.535</span>], [<span class="number">1</span>, <span class="number">1.506</span>, <span class="number">1.638</span>], [<span class="number">1</span>, <span class="number">1.712</span>, <span class="number">0.394</span>], [<span class="number">1</span>, <span class="number">0.556</span>, <span class="number">0.124</span>], [<span class="number">1</span>, <span class="number">0.444</span>, <span class="number">0.115</span>], [<span class="number">1</span>, <span class="number">0.595</span>, <span class="number">1.009</span>], [<span class="number">1</span>, <span class="number">0.165</span>, <span class="number">0.089</span>], [<span class="number">1</span>, <span class="number">1.57</span>, <span class="number">0.634</span>], [<span class="number">1</span>, <span class="number">1.429</span>, <span class="number">1.181</span>], [<span class="number">1</span>, <span class="number">0.8</span>, <span class="number">0.671</span>], [<span class="number">1</span>, <span class="number">1.914</span>, <span class="number">1.091</span>], [<span class="number">1</span>, <span class="number">0.594</span>, <span class="number">0.569</span>], [<span class="number">1</span>, <span class="number">0.935</span>, <span class="number">0.277</span>], [<span class="number">1</span>, <span class="number">0.47</span>, <span class="number">0.522</span>], [<span class="number">1</span>, <span class="number">0.94</span>, <span class="number">1.924</span>], [<span class="number">1</span>, <span class="number">0.194</span>, <span class="number">1.933</span>], [<span class="number">1</span>, <span class="number">0.612</span>, <span class="number">0.613</span>], [<span class="number">1</span>, <span class="number">0.236</span>, <span class="number">0.894</span>], [<span class="number">1</span>, <span class="number">1.888</span>, <span class="number">0.251</span>], [<span class="number">1</span>, <span class="number">1.548</span>, <span class="number">0.191</span>], [<span class="number">1</span>, <span class="number">1.543</span>, <span class="number">0.603</span>], [<span class="number">1</span>, <span class="number">1.521</span>, <span class="number">0.02</span>], [<span class="number">1</span>, <span class="number">0.923</span>, <span class="number">0.856</span>], [<span class="number">1</span>, <span class="number">0.649</span>, <span class="number">1.31</span>], [<span class="number">1</span>, <span class="number">0.379</span>, <span class="number">1.746</span>], [<span class="number">1</span>, <span class="number">1.345</span>, <span class="number">0.902</span>], [<span class="number">1</span>, <span class="number">0.937</span>, <span class="number">0.524</span>], [<span class="number">1</span>, <span class="number">1.018</span>, <span class="number">0.68</span>], [<span class="number">1</span>, <span class="number">1.738</span>, <span class="number">1.623</span>], [<span class="number">1</span>, <span class="number">1.534</span>, <span class="number">1.9</span>], [<span class="number">1</span>, <span class="number">0.139</span>, <span class="number">1.911</span>], [<span class="number">1</span>, <span class="number">1.508</span>, <span class="number">1.173</span>], [<span class="number">1</span>, <span class="number">0.798</span>, <span class="number">0.865</span>], [<span class="number">1</span>, <span class="number">0.451</span>, <span class="number">1.186</span>], [<span class="number">1</span>, <span class="number">1.63</span>, <span class="number">1.123</span>], [<span class="number">1</span>, <span class="number">0.82</span>, <span class="number">0.848</span>], [<span class="number">1</span>, <span class="number">1.213</span>, <span class="number">1.48</span>], [<span class="number">1</span>, <span class="number">0.894</span>, <span class="number">0.664</span>], [<span class="number">1</span>, <span class="number">1.456</span>, <span class="number">0.934</span>], [<span class="number">1</span>, <span class="number">0.59</span>, <span class="number">1.525</span>], [<span class="number">1</span>, <span class="number">0.522</span>, <span class="number">1.329</span>], [<span class="number">1</span>, <span class="number">1.179</span>, <span class="number">1.396</span>], [<span class="number">1</span>, <span class="number">0.527</span>, <span class="number">0.273</span>], [<span class="number">1</span>, <span class="number">1.399</span>, <span class="number">1.215</span>], [<span class="number">1</span>, <span class="number">0.966</span>, <span class="number">1.514</span>], [<span class="number">1</span>, <span class="number">1.341</span>, <span class="number">0.028</span>], [<span class="number">1</span>, <span class="number">0.479</span>, <span class="number">0.191</span>], [<span class="number">1</span>, <span class="number">1.193</span>, <span class="number">0.724</span>], [<span class="number">1</span>, <span class="number">0.714</span>, <span class="number">1.285</span>]]</span><br><span class="line">g_Ty_arr=[[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>]]</span><br><span class="line">g_y_arr=[<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">phi</span>(<span class="params">i,theta,x</span>):</span><br><span class="line">	theta_n = <span class="built_in">len</span>(theta)</span><br><span class="line">	numerator = math.exp(np.dot(theta[i], x))</span><br><span class="line">	denominator = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(theta_n):                      <span class="comment"># maybe need to optimize</span></span><br><span class="line">		denominator = denominator + math.exp(np.dot(theta[j],x))</span><br><span class="line">	<span class="keyword">return</span> numerator/denominator</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">h_theta</span>(<span class="params">theta,x</span>):</span><br><span class="line">	theta_n = <span class="built_in">len</span>(theta)</span><br><span class="line">	ret = []</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(theta_n):</span><br><span class="line">		ret.append(phi(i,theta,x))</span><br><span class="line">	<span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">one</span>(<span class="params">y,i</span>):</span><br><span class="line">	<span class="keyword">if</span> y == i:</span><br><span class="line">		<span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		<span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">derl</span>(<span class="params">f,x_arr,y_arr,theta</span>):</span><br><span class="line">	<span class="comment"># 0.</span></span><br><span class="line">	<span class="comment"># f is the dimension to calc the partial derivative</span></span><br><span class="line">	<span class="comment"># 1. calc the size</span></span><br><span class="line">	<span class="comment"># theta_n is the size of theta, equals to the length of Ty&#x27; result set -1 .Here, equals lenght of &#123;0,1,2&#125; = 3-1 =2</span></span><br><span class="line">	<span class="comment"># m is the sum of samples</span></span><br><span class="line">	<span class="comment"># x_n is the dimension of the input variable &#x27;x&#x27; + 1 (for x_0 =1). Here is 2 + 1 = 3</span></span><br><span class="line">	theta_n = <span class="built_in">len</span>(theta)</span><br><span class="line">	m = <span class="built_in">len</span>(x_arr)</span><br><span class="line">	x_n = <span class="built_in">len</span>(x_arr[<span class="number">0</span>])</span><br><span class="line">	<span class="comment"># initial the output variable sum.Here is a vector, and the length of this vector is x_n</span></span><br><span class="line">	<span class="built_in">sum</span> = []</span><br><span class="line">	<span class="keyword">for</span> x_dim_index <span class="keyword">in</span> <span class="built_in">range</span>(x_n):</span><br><span class="line">			<span class="built_in">sum</span>.append(<span class="number">0</span>)</span><br><span class="line">	<span class="comment"># 2. calc the partial derivative</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(x_n):</span><br><span class="line">		<span class="built_in">sum</span>[i] = <span class="number">0</span></span><br><span class="line">		<span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">			<span class="built_in">sum</span>[i]=<span class="built_in">sum</span>[i]+(one(y_arr[j],f)-phi(f,theta,x_arr[j]))*x_arr[j][i]</span><br><span class="line">		<span class="built_in">sum</span>[i]=<span class="built_in">sum</span>[i]/m</span><br><span class="line">	<span class="comment">#sum=plus_vector(sum,theta[f],0.1)</span></span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">sum</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plus_vector</span>(<span class="params">arr1,arr2,a</span>):</span><br><span class="line">	<span class="keyword">return</span> [x + a * y <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(arr1, arr2)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">theta,a,x_arr,y_arr</span>):</span><br><span class="line">	theta_n = <span class="built_in">len</span>(theta)</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(theta_n):</span><br><span class="line">		theta[i]=plus_vector(theta[i],derl(i, x_arr,y_arr,theta),a)</span><br><span class="line">	<span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">judge1</span>(<span class="params">theta,x_arr,y_arr_vector,limit,debug</span>):</span><br><span class="line">	j_theta = J(theta,x_arr,y_arr_vector)</span><br><span class="line">	<span class="keyword">if</span> j_theta &lt; limit:</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">	<span class="keyword">if</span> debug:</span><br><span class="line">		<span class="built_in">print</span>(<span class="string">&quot;|J_theta(x)| = &quot;</span>, j_theta,<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">J</span>(<span class="params">theta,x_arr,y_arr_vector</span>):</span><br><span class="line">	<span class="built_in">sum</span>=<span class="number">0</span></span><br><span class="line">	m = <span class="built_in">len</span>(x_arr)</span><br><span class="line">	y_len = <span class="built_in">len</span>(y_arr_vector[<span class="number">0</span>])</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">		<span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(y_len):</span><br><span class="line">			<span class="built_in">sum</span> = <span class="built_in">sum</span> + (phi(j,theta,x_arr[i]) - y_arr_vector[i][j])**<span class="number">2</span></span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">sum</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calTheVaule</span>():</span><br><span class="line">	<span class="comment">#theta=[[theta_1_1,theta_1_2],[theta_2_1,theta_2_2],[theta_3_1,theta_3_2]]</span></span><br><span class="line">	theta = [[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]]</span><br><span class="line">	a = <span class="number">1</span></span><br><span class="line">	count = <span class="number">0</span></span><br><span class="line">	<span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">		<span class="keyword">if</span> judge1(theta,g_x_arr,g_Ty_arr,<span class="number">1</span>,<span class="literal">True</span>):</span><br><span class="line">			<span class="keyword">break</span></span><br><span class="line">		theta=update(theta,a,g_x_arr,g_y_arr)</span><br><span class="line">		count = count + <span class="number">1</span></span><br><span class="line">		<span class="built_in">print</span>(<span class="string">&quot;count=&quot;</span>,count,<span class="string">&quot;theta=&quot;</span>,theta)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcIndex</span>(<span class="params">theta,x</span>):</span><br><span class="line">  phi0 = phi(<span class="number">0</span>,theta,x)</span><br><span class="line">  phi1 = phi(<span class="number">1</span>,theta,x)</span><br><span class="line">  phi2 = phi(<span class="number">2</span>,theta,x)</span><br><span class="line">  <span class="keyword">if</span> phi0 &gt;= phi1 <span class="keyword">and</span> phi0 &gt;=phi1:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  <span class="keyword">elif</span> phi1&gt;= phi0 <span class="keyword">and</span> phi1 &gt;= phi2:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">testValue</span>():</span><br><span class="line">  <span class="comment"># limit =1， count = 33844</span></span><br><span class="line">  theta = [[<span class="number">17.418683742474045</span>, <span class="number">13.989348587756815</span>, -<span class="number">37.731307869980014</span>],[-<span class="number">33.56035031194091</span>, <span class="number">1.3797244978249763</span>, <span class="number">34.095843086377855</span>],[<span class="number">19.97548491920147</span>, -<span class="number">11.65484983882828</span>, <span class="number">7.628008093823735</span>]]</span><br><span class="line">  samples = <span class="number">100</span></span><br><span class="line">  figure, ax = plt.subplots()</span><br><span class="line">  <span class="comment"># 设置x，y值域</span></span><br><span class="line">  ax.set_xlim(left=<span class="number">0</span>, right=<span class="number">2</span>)</span><br><span class="line">  ax.set_ylim(bottom=<span class="number">0</span>, top=<span class="number">2</span>)</span><br><span class="line">  <span class="comment"># 两条line的数据</span></span><br><span class="line">  (line1_xs, line1_ys) = [(<span class="number">0</span>, <span class="number">2</span>), (<span class="number">0</span>, <span class="number">1</span>)]</span><br><span class="line">  (line2_xs, line2_ys) = [(<span class="number">0</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">1</span>)]</span><br><span class="line">  <span class="comment"># 创建两条线，并添加</span></span><br><span class="line">  ax.add_line(Line2D(line1_xs, line1_ys, linewidth=<span class="number">1</span>, color=<span class="string">&#x27;blue&#x27;</span>))</span><br><span class="line">  ax.add_line(Line2D(line2_xs, line2_ys, linewidth=<span class="number">1</span>, color=<span class="string">&#x27;blue&#x27;</span>))</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(samples):</span><br><span class="line">    x_0 = random.randint(<span class="number">0</span>,<span class="number">2000</span>)/<span class="number">1000</span></span><br><span class="line">    x_1 = random.randint(<span class="number">0</span>,<span class="number">2000</span>)/<span class="number">1000</span></span><br><span class="line">    index = calcIndex(theta,[<span class="number">1</span>,x_0,x_1])</span><br><span class="line">    <span class="keyword">if</span> index == <span class="number">0</span>:</span><br><span class="line">      plt.plot(x_0, x_1, <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;x&#x27;</span>, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> index == <span class="number">1</span>:</span><br><span class="line">      plt.plot(x_0, x_1, <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;+&#x27;</span>, color=<span class="string">&#x27;g&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      plt.plot(x_0, x_1, <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, color=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">  plt.xlabel(<span class="string">&quot;x1&quot;</span>)</span><br><span class="line">  plt.ylabel(<span class="string">&quot;x2&quot;</span>)</span><br><span class="line">  plt.plot()</span><br><span class="line">  plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__== <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">  <span class="comment">#calTheVaule()</span></span><br><span class="line">  testValue()</span><br></pre></td></tr></table></figure>

<p>经过33844次迭代之后，我们得到\(\theta\) &#x3D; [[17.418683742474045, 13.989348587756815, -37.731307869980014],[-33.56035031194091, 1.3797244978249763, 34.095843086377855],[19.97548491920147, -11.65484983882828, 7.628008093823735]]</p>
<blockquote>
<p>迭代的次数越多数据会越准确，这里迭代了上三万次，实际上可以通过牛顿法来减少迭代的次数，这里就不再重新代码了，牛顿法可以参见前面的文章。</p>
</blockquote>
<p>然后，我们随机制造一组值，看看分类效果，具体如下：</p>
<img src="/images/机器学习/监督学习-softmax计算结果.png" width=50% height=50% text-align=center/>

<h2 id="附录-公式推导手写版"><a href="#附录-公式推导手写版" class="headerlink" title="附录 公式推导手写版"></a>附录 公式推导手写版</h2><img src="/images/机器学习/监督学习-softmax公式推导手写版1.png" width=50% height=50% text-align=center/>

<img src="/images/机器学习/监督学习-softmax公式推导手写版2.png" width=50% height=50% text-align=center/>

<img src="/images/机器学习/监督学习-softmax公式推导手写版3.png" width=50% height=50% text-align=center/>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2017/07/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-4-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8Bsoftmax/" data-id="cuid-vK-j9KStRBwBDYmt-_G-" data-title="机器学习-2.4-监督学习之softmax" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/02/">February 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/02/07/ErasuceCode%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/12/25/RSS-%E8%BF%9C%E7%A8%8BMerge%E7%9A%84%E8%AE%BE%E8%AE%A1/">RSS-远程Merge的设计</a>
          </li>
        
          <li>
            <a href="/2023/12/25/RSS-%E8%BF%9C%E7%A8%8BMerge%E7%9A%84%E8%AE%BE%E8%AE%A1EN/">RSS-Remote Merge Design</a>
          </li>
        
          <li>
            <a href="/2017/10/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-3-2-%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/">机器学习-3.2-非监督学习之主成分分析.md</a>
          </li>
        
          <li>
            <a href="/2017/10/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-8-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8Bk%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/">机器学习-2.8-监督学习之k近邻算法</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 zhengchenyu<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>