<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>机器学习-2.5-监督学习之生成型学习算法 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="生成型学习算法1 生成型学习算法简述我们之前讲到的算法。譬如逻辑回归，试图从\(x\)直接学习得到一组映射关系到\(y\)，即通过样本\((x,y)\)学习得到,使得其能够准确的预测\(y\)。这类算法叫做判别性学习算法。 本节我们将介绍生成学习型算法。举个例子，对于一个大象，我们可以为大象构造一个关于其特征的模型。对于一条狗，同样可以为狗构造一个关于关于其特征的模型。对于一个新动物，我们可以">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习-2.5-监督学习之生成型学习算法">
<meta property="og:url" content="http://example.com/2017/08/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-5-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%94%9F%E6%88%90%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="生成型学习算法1 生成型学习算法简述我们之前讲到的算法。譬如逻辑回归，试图从\(x\)直接学习得到一组映射关系到\(y\)，即通过样本\((x,y)\)学习得到,使得其能够准确的预测\(y\)。这类算法叫做判别性学习算法。 本节我们将介绍生成学习型算法。举个例子，对于一个大象，我们可以为大象构造一个关于其特征的模型。对于一条狗，同样可以为狗构造一个关于关于其特征的模型。对于一个新动物，我们可以">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E5%9E%8B%E7%AE%97%E6%B3%95GDA%E6%A0%B7%E6%9C%AC.png">
<meta property="og:image" content="http://example.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E5%9E%8B%E7%AE%97%E6%B3%95GDA%E7%BB%93%E6%9E%9C.png">
<meta property="article:published_time" content="2017-08-05T08:41:15.000Z">
<meta property="article:modified_time" content="2025-02-07T08:06:18.199Z">
<meta property="article:author" content="zhengchenyu">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E5%9E%8B%E7%AE%97%E6%B3%95GDA%E6%A0%B7%E6%9C%AC.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-机器学习-2-5-监督学习之生成型学习算法" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2017/08/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-5-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%94%9F%E6%88%90%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" class="article-date">
  <time class="dt-published" datetime="2017-08-05T08:41:15.000Z" itemprop="datePublished">2017-08-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      机器学习-2.5-监督学习之生成型学习算法
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="生成型学习算法"><a href="#生成型学习算法" class="headerlink" title="生成型学习算法"></a>生成型学习算法</h1><h2 id="1-生成型学习算法简述"><a href="#1-生成型学习算法简述" class="headerlink" title="1 生成型学习算法简述"></a>1 生成型学习算法简述</h2><p>我们之前讲到的算法。譬如逻辑回归，试图从\(x\)直接学习得到一组映射关系到\(y\)，即通过样本\((x,y)\)学习得到,使得其能够准确的预测\(y\)。这类算法叫做判别性学习算法。</p>
<p>本节我们将介绍生成学习型算法。举个例子，对于一个大象，我们可以为大象构造一个关于其特征的模型。对于一条狗，同样可以为狗构造一个关于关于其特征的模型。对于一个新动物，我们可以根据前述构建的模型，判断这个动物是像大象多一点还是像狗多一点，来确定这动物是大象还是狗。</p>
<p>我们可以根据样本得到大象的特征模型\(p(x|y&#x3D;0)\)和狗的特征模型\(p(x|y&#x3D;1)\)，同时也可以得到\(p(y)\)。根据贝叶斯公式，有如下：</p>
$$\max (p(y|x)) = \max (\frac{p(x|y)p(y)}{p(x)}) = \max (\frac{p(x|y)p(y)}{p(x|y = 1)p(y = 1) + p(x|y = 0)p(y = 0)})$$


<p>对于我们做预测的时候，可以不考虑分母。</p>
$$\max (p(y|x)) = \max (p(x|y)p(y))$$


<blockquote>
<p>为什么不考虑分母?我们可以将\(p(x|y)p(y)\)简化为\(f(y)\),因此分母就是\(f(1)+f(0)\)。如果我们通过样本对完成了建模，因此分母就是一个常数了，因此没有计算的必要了。</p>
</blockquote>
<h2 id="2-高斯判别分析"><a href="#2-高斯判别分析" class="headerlink" title="2 高斯判别分析"></a>2 高斯判别分析</h2><h3 id="2-1-多维高斯分布介绍"><a href="#2-1-多维高斯分布介绍" class="headerlink" title="2.1 多维高斯分布介绍"></a>2.1 多维高斯分布介绍</h3><p>略，详见cs229-note2.pdf</p>
<h3 id="2-2-高斯判别分析模型"><a href="#2-2-高斯判别分析模型" class="headerlink" title="2.2 高斯判别分析模型"></a>2.2 高斯判别分析模型</h3><p>对于一个分类问题,\(y \in (0,1)\)。假设\(x\)是连续的随机变量，服从高斯分布。具体描述如下：</p>
$$y \sim Bernoulli(\phi)$$$$x|y = 0 \sim N({\mu _0},\Sigma)$$$$x|y = 1 \sim N({\mu _1},\Sigma)$$
$$p(x|y = 0) = \frac{1}{{{(2\pi )}^{\frac{n}{2}}}|\Sigma {|^{\frac{n}{2}}}}\exp ( - \frac{1}{2}{(x - {\mu _0})^T}{\Sigma ^{ - 1}}(x - {\mu _0}))$$
$$p(x|y = 1) = \frac{1}{{{(2\pi )}^{\frac{n}{2}}}|\Sigma {|^{\frac{n}{2}}}}\exp ( - \frac{1}{2}{(x - {\mu _1})^T}{\Sigma ^{ - 1}}(x - {\mu _1}))$$


<h3 id="2-3-公式推导"><a href="#2-3-公式推导" class="headerlink" title="2.3 公式推导"></a>2.3 公式推导</h3><p>我们设置\(\phi  &#x3D; p(y &#x3D; 0)\)。根据前面的分析，在给定样本的情况下，我们需要保证下式最大。</p>
$$\prod\limits _{i = 1}^m {p({y^i}|{x^i})}  = \prod\limits _{i = 1}^m {p({x^i}|{y^i})p({y^i})}$$


<p>这里为了简写，我们设置\({z _0} &#x3D; x - {\mu _0}\) ，\({z _1} &#x3D; x - {\mu _1}\)。取自然对数后，得到最大释然函数:</p>

$$\ell ({\mu _1},{\mu _1},\Sigma ,\phi ) = \sum\limits _{i = 1}^m {\ln \{ {{[\frac{1}{{{{(2\pi )}^{\frac{n}{2}}}|\Sigma {|^{\frac{n}{2}}}}}\exp ( - \frac{1}{2}z _0^T{\Sigma ^{ - 1}}{z _0})]}^{1\{ {y^i} = 0\} }}{{[\frac{1}{{{{(2\pi )}^{\frac{n}{2}}}|\Sigma {|^{\frac{n}{2}}}}}\exp ( - \frac{1}{2}z _1^T{\Sigma ^{ - 1}}{z _1})]}^{1\{ {y^i} = 1\} }}\} } $$
$$\ell ({\mu _0},{\mu _1},\Sigma ,\phi ) = \sum\limits _{i = 1}^m {\{ 1\{ {y^i} = 0\} [ - \frac{1}{2}z _0^T{\Sigma ^{ - 1}}{z _0} - \frac{n}{2}\ln 2\pi  - \frac{n}{2}\ln |\Sigma | + \ln \phi ] + 1\{ {y^i} = 1\} } [ - \frac{1}{2}z _1^T{\Sigma ^{ - 1}}{z _1} - \frac{n}{2}\ln 2\pi  - \frac{n}{2}\ln |\Sigma | + \ln (1 - \phi )]\}$$


<p>然后我们分别对各个参数求偏导数:</p>

$$\frac{{\partial \ell ({\mu _0},{\mu _1},\sum ,\phi )}}{{\partial \phi }} = \sum\limits _{i = 1}^m {(\frac{{1\{ {y^i} = 1\} }}{\phi } - \frac{{1\{ {y^i} = 0\} }}{{1 - \phi }})}  = \sum\limits _{i = 1}^n {(\frac{{1\{ {y^i} = 1\} }}{\phi } - \frac{{1 - 1\{ {y^i} = 1\} }}{{1 - \phi }})}  = 0$$


<p>所以有:</p>

$$\phi  = \frac{1}{m}\sum\limits _{i = 1}^n {1\{ {y^i} = 1\} }$$


<p>然后对\(\mu _0\)求导：</p>

$${\nabla _{{\mu _0}}}\ell ({\mu _0},{\mu _1},\Sigma ,\phi ) = \sum\limits _{i = 1}^m {{\nabla _{{\mu _0}}}( - \frac{1}{2}{{({x^i} - {\mu _0})}^T}{\Sigma ^{ - 1}}({x^i} - {\mu _0}))1\{ {y^i} = 0\} }$$$${\nabla _{{\mu _0}}}\ell ({\mu _0},{\mu _1},\Sigma ,\phi ) =  - \frac{1}{2}\sum\limits _{i = 1}^m {{\nabla _{{\mu _0}}}( - \mu _0^T{\Sigma ^{ - 1}}{x^i} - {{({x^i})}^T}{\Sigma ^{ - 1}}{\mu _0} + \mu _0^T{\Sigma ^{ - 1}}{\mu _0})1\{ {y^i} = 0\} }$$

<p>我们对上面的式子分头计算。</p>
<blockquote>
<p>\(\Sigma\)为对角阵。由于求偏微分的数为一个常数，因此该值与其迹的相同。另外，这里使用了一些关于迹的公式。具体详见附录。</p>
</blockquote>

$${\nabla _{{\mu _0}}}\mu _0^T{\Sigma ^{ - 1}}x = {\nabla _{{\mu _0}}}tr(\mu _0^T{\Sigma ^{ - 1}}{x^i}) = {[{\nabla _{\mu _0^T}}tr(\mu _0^T{\Sigma ^{ - 1}}{x^i})]^T} = {[{({\Sigma ^{ - 1}}{x^i})^T}]^T} = {\Sigma ^{ - 1}}{x^i}$$
$${\nabla _{{\mu _0}}}{({x^i})^T}{\Sigma ^{ - 1}}{\mu _0} = {\nabla _{{\mu _0}}}tr({({x^i})^T}{\Sigma ^{ - 1}}{\mu _0}) = {\nabla _{{\mu _0}}}{\mu _0}{({x^i})^T}{\Sigma ^{ - 1}} = {({({x^i})^T}{\Sigma ^{ - 1}})^T} = {\Sigma ^{ - 1}}{x^i}$$
$${\nabla _{{\mu _0}}}\mu _0^T{\Sigma ^{ - 1}}{\mu _0} = {[{\nabla _{\mu _0^T}}\mu _0^T{\Sigma ^{ - 1}}{\mu _0}E]^T} = [E\mu _0^T{\Sigma ^{ - 1}} + {E^T}\mu _0^T{({\Sigma ^{ - 1}})^T}] = 2{\Sigma ^{ - 1}}{\mu _0}$$


<p>所以有:</p>

$${\nabla _{{\mu _0}}}\ell ({\mu _0},{\mu _1},\Sigma ,\phi ) =  - \sum\limits _{i = 1}^m {{\nabla _{{\mu _0}}}({\Sigma ^{ - 1}}{\mu _0} - {\Sigma ^{ - 1}}{x^i})1\{ {y^i} = 0\} }  = 0$$
$${\mu _0} = \frac{{\sum\limits _{i = 1}^m {1\{ {y^i} = 0\} {x^i}} }}{{\sum\limits _{i = 1}^m {1\{ {y^i} = 0\} } }}$$


<p>同理有:</p>

$${\mu _1} = \frac{{\sum\limits _{i = 1}^m {1\{ {y^i} = 1\} {x^i}} }}{{\sum\limits _{i = 1}^m {1\{ {y^i} = 1\} } }}$$


<p>然后对\(\Sigma^{-1}\)求偏导数。</p>
$${\nabla _{{\Sigma ^{ - 1}}}}\ell ({\mu _0},{\mu _1},\Sigma ,\phi ) = \sum\limits _{i = 1}^m {{\nabla _{{\Sigma ^{ - 1}}}}[( - \frac{1}{2}z _0^T{\Sigma ^{ - 1}}{z _0} + \frac{1}{2}\ln \frac{1}{{|\Sigma |}})1\{ {y^i} = 0\}  + ( - \frac{1}{2}z _1^T{\Sigma ^{ - 1}}{z _1} + \frac{1}{2}\ln \frac{1}{{|\Sigma |}})1\{ {y^i} = 1\} ]}$$


<p>这里分开计算:</p>

$${\nabla _{\Sigma ^{ - 1}}}(z _0^T{\Sigma ^{ - 1}}{z _0}) = {\nabla _{\sum ^{ - 1}}}tr(z _0^T{\Sigma ^{ - 1}}{z _0}) = {\nabla _{\sum ^{ - 1}}}tr({\Sigma ^{ - 1}}{z _0}z _0^T) = {({z _0}z _0^T)^T} = {z _0}z _0^T$$
$${\nabla _{\Sigma ^{ - 1}}}\ln \frac{1}{|\Sigma |} = {\nabla _{\Sigma ^{ - 1}}}\ln |{\Sigma ^{ - 1}}| = \frac{1}{|{\Sigma ^{ - 1}}|}{\nabla _{\Sigma ^{ - 1}}}{\Sigma ^{ - 1}} = {({({\Sigma ^{ - 1}})^{ - 1}})^T}$$


<p>所以有:</p>

$${\nabla _{{\sum ^{ - 1}}}}\ell ({\mu _0},{\mu _1},\Sigma ,\phi ) = \sum\limits _{i = 1}^m {[( - \frac{1}{2}{z _0}z _0^T + \frac{1}{2}\Sigma )1\{ {y^i} = 0\}  + ( - \frac{1}{2}{z _1}z _1^T + \frac{1}{2}\Sigma )1\{ {y^i} = 1\} ]}  = 0$$
$$\Sigma  = \frac{1}{m}\sum\limits _{i = 1}^m {[({x^i} - {\mu _0}){{({x^i} - {\mu _0})}^T}1\{ {y^i} = 0\}  + ({x^i} - {\mu _1}){{({x^i} - {\mu _1})}^T}1\{ {y^i} = 1\} ]}$$
$$\Sigma  = \frac{1}{m}\sum\limits _{i = 1}^m {[({x^i} - {\mu _{{y^i}}}){{({x^i} - {\mu _{{y^i}}})}^T}]}$$


<h2 id="2-3-高斯型算法实例"><a href="#2-3-高斯型算法实例" class="headerlink" title="2.3 高斯型算法实例"></a>2.3 高斯型算法实例</h2><p>我们分别以 \((1,1)\)和\((2,2)\)为均值生成一组高斯分布。下面图是生成的样本。</p>
<img src="/images/机器学习/监督学习-生成型算法GDA样本.png" width=50% height=50% text-align=center/>

<p>前提假设是我们知道两组分类是符合高斯分布的，切假定协方差相同。但我们不知道两组数据的均值和协方差。根据前面的公式有如下代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> multivariate_normal</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> matplotlib.lines <span class="keyword">import</span> Line2D</span><br><span class="line"></span><br><span class="line"><span class="comment">## this is a program about GDA</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">global</span> MAKE_DATA</span><br><span class="line"><span class="keyword">global</span> SHOW_PIC</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_data</span>(<span class="params">x</span>):</span><br><span class="line">  <span class="keyword">if</span> MAKE_DATA == <span class="literal">True</span>:</span><br><span class="line">    mean_1 = [<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">    mean_2 = [<span class="number">2</span>,<span class="number">2</span>]</span><br><span class="line">    cov = [[<span class="number">0.1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.1</span>]]</span><br><span class="line">    arr1 = np.random.multivariate_normal(mean_1, cov, <span class="number">100</span>)</span><br><span class="line">    arr2 = np.random.multivariate_normal(mean_2, cov, <span class="number">50</span>)</span><br><span class="line">    <span class="built_in">print</span>(arr1)</span><br><span class="line">    <span class="built_in">print</span>(arr2)</span><br><span class="line">    x.append(arr1)</span><br><span class="line">    x.append(arr2)</span><br><span class="line">  <span class="keyword">if</span> MAKE_DATA == <span class="literal">False</span>:</span><br><span class="line">    arr1 = np.array([[ <span class="number">0.88235916</span> , <span class="number">1.01511634</span>],[ <span class="number">0.75243817</span> , <span class="number">0.76520033</span>],[ <span class="number">0.95710848</span> , <span class="number">1.41894337</span>],[ <span class="number">1.48682891</span> , <span class="number">0.78885043</span>],[ <span class="number">1.24047011</span> , <span class="number">0.71984948</span>],[ <span class="number">0.67611276</span> , <span class="number">1.07909452</span>],[ <span class="number">1.03243669</span> , <span class="number">1.08929695</span>],[ <span class="number">1.0296548</span>  , <span class="number">1.25023769</span>],[ <span class="number">1.54134008</span> , <span class="number">0.39564824</span>],[ <span class="number">0.34645057</span> , <span class="number">1.61499636</span>],[ <span class="number">0.77206174</span> , <span class="number">1.23613698</span>],[ <span class="number">0.91446988</span> , <span class="number">1.38537765</span>],[ <span class="number">0.99982962</span> , <span class="number">1.34448471</span>],[ <span class="number">0.78745962</span> , <span class="number">0.9046565</span> ],[ <span class="number">0.74946602</span> , <span class="number">1.07424473</span>],[ <span class="number">1.09294839</span> , <span class="number">1.14711993</span>],[ <span class="number">0.39266844</span> , <span class="number">0.78788004</span>],[ <span class="number">0.83112357</span> , <span class="number">1.2762774</span> ],[ <span class="number">1.05056188</span> , <span class="number">1.13351562</span>],[ <span class="number">1.62101523</span> , <span class="number">1.15035562</span>],[ <span class="number">0.70377517</span> , <span class="number">1.1136416</span> ],[ <span class="number">1.03715472</span> , <span class="number">0.47905693</span>],[ <span class="number">0.94598381</span> , <span class="number">0.8874837</span> ],[ <span class="number">0.94447128</span> , <span class="number">2.02796925</span>],[ <span class="number">0.72442242</span> , <span class="number">1.09835206</span>],[ <span class="number">0.69046731</span> , <span class="number">1.46232182</span>],[ <span class="number">1.20744606</span> , <span class="number">1.10280041</span>],[ <span class="number">0.70665746</span> , <span class="number">0.82139503</span>],[ <span class="number">1.08803887</span> , <span class="number">1.4450361</span> ],[ <span class="number">0.88530961</span> , <span class="number">0.75727475</span>],[ <span class="number">0.98418545</span> , <span class="number">0.80248161</span>],[ <span class="number">0.74970386</span> , <span class="number">1.13205709</span>],[ <span class="number">0.72586454</span> , <span class="number">1.06058385</span>],[ <span class="number">0.9071812</span>  , <span class="number">1.09975063</span>],[ <span class="number">0.75182835</span> , <span class="number">0.93570147</span>],[ <span class="number">0.80052289</span> , <span class="number">1.08168507</span>],[ <span class="number">0.40180652</span> , <span class="number">0.9526211</span> ],[ <span class="number">0.62312617</span> , <span class="number">0.84385058</span>],[ <span class="number">0.68212516</span> , <span class="number">1.25912717</span>],[ <span class="number">1.19773245</span> , <span class="number">0.16399654</span>],[ <span class="number">0.96093132</span> , <span class="number">0.43932091</span>],[ <span class="number">1.25471657</span> , <span class="number">0.92371829</span>],[ <span class="number">1.12330272</span> , <span class="number">1.26968747</span>],[ <span class="number">1.30361985</span> , <span class="number">0.99862123</span>],[ <span class="number">1.23477665</span> , <span class="number">1.1742804</span> ],[ <span class="number">0.28471876</span> , <span class="number">0.5806044</span> ],[ <span class="number">1.89355099</span> , <span class="number">1.19928671</span>],[ <span class="number">1.09081369</span> , <span class="number">1.28467312</span>],[ <span class="number">1.40488635</span> , <span class="number">0.90034427</span>],[ <span class="number">1.11672364</span> , <span class="number">1.49070515</span>],[ <span class="number">1.35385212</span> , <span class="number">1.35767891</span>],[ <span class="number">0.92746374</span> , <span class="number">1.79096697</span>],[ <span class="number">1.89142562</span> , <span class="number">0.98228303</span>],[ <span class="number">1.0555218</span>  , <span class="number">0.86070833</span>],[ <span class="number">0.69001255</span> , <span class="number">1.12874741</span>],[ <span class="number">0.98137315</span> , <span class="number">1.3398852</span> ],[ <span class="number">1.02525371</span> , <span class="number">0.77572865</span>],[ <span class="number">1.1354295</span>  , <span class="number">1.07098552</span>],[ <span class="number">1.50829164</span> , <span class="number">1.43065998</span>],[ <span class="number">1.09928764</span> , <span class="number">1.55540292</span>],[ <span class="number">0.64695084</span> , <span class="number">0.79920395</span>],[ <span class="number">0.82059034</span> , <span class="number">0.97533491</span>],[ <span class="number">0.56345455</span> , <span class="number">1.08168272</span>],[ <span class="number">1.06673215</span> , <span class="number">1.19448556</span>],[ <span class="number">0.96512548</span> , <span class="number">1.5268577</span> ],[ <span class="number">0.96914451</span> , <span class="number">1.00902985</span>],[ <span class="number">0.72879413</span> , <span class="number">0.92476415</span>],[ <span class="number">1.0931483</span>  , <span class="number">1.13572242</span>],[ <span class="number">1.34765121</span> , <span class="number">0.83841006</span>],[ <span class="number">1.57813788</span> , <span class="number">0.65915892</span>],[ <span class="number">0.59032608</span> , <span class="number">0.82747946</span>],[ <span class="number">0.83838504</span> , <span class="number">0.67588473</span>],[ <span class="number">1.35101322</span> , <span class="number">1.21027851</span>],[ <span class="number">0.71762153</span> , <span class="number">0.41839038</span>],[ <span class="number">0.61295604</span> , <span class="number">0.66555018</span>],[ <span class="number">0.64379346</span> , <span class="number">0.92925228</span>],[ <span class="number">1.1194968</span>  , <span class="number">0.65876736</span>],[ <span class="number">0.39495437</span> , <span class="number">0.67246734</span>],[ <span class="number">1.05223282</span> , <span class="number">0.17889116</span>],[ <span class="number">0.97810984</span> , <span class="number">1.12794664</span>],[ <span class="number">0.98392719</span> , <span class="number">0.73590255</span>],[ <span class="number">1.25587405</span> , <span class="number">1.21853038</span>],[ <span class="number">1.01150226</span> , <span class="number">1.01835571</span>],[ <span class="number">1.02251614</span> , <span class="number">0.72704228</span>],[ <span class="number">1.00261519</span> , <span class="number">0.95347185</span>],[ <span class="number">0.96362523</span> , <span class="number">0.8607009</span> ],[ <span class="number">0.88034659</span> , <span class="number">1.2307104</span> ],[ <span class="number">0.75907236</span> , <span class="number">0.92799796</span>],[ <span class="number">0.54898709</span> , <span class="number">1.69882285</span>],[ <span class="number">0.55032649</span> , <span class="number">0.98831566</span>],[ <span class="number">1.33360789</span> , <span class="number">1.19793298</span>],[ <span class="number">0.83231239</span> , <span class="number">0.8946538</span> ],[ <span class="number">1.05173094</span> , <span class="number">1.26324289</span>],[ <span class="number">0.81482231</span> , <span class="number">0.56198584</span>],[ <span class="number">1.03854797</span> , <span class="number">1.0553811</span> ],[ <span class="number">1.32669227</span> , <span class="number">1.61115811</span>],[ <span class="number">1.13322152</span> , <span class="number">1.68151695</span>],[ <span class="number">0.39754618</span> , <span class="number">1.19392967</span>],[ <span class="number">0.61344185</span> , <span class="number">1.05281434</span>],[ <span class="number">1.18415366</span> , <span class="number">0.864884</span>  ]])</span><br><span class="line">    arr2 = np.array([[ <span class="number">2.15366548</span> , <span class="number">1.88035458</span>],[ <span class="number">2.36978774</span> , <span class="number">1.76550283</span>],[ <span class="number">2.46261387</span> , <span class="number">2.10568262</span>],[ <span class="number">1.90475526</span> , <span class="number">1.95242885</span>],[ <span class="number">1.77712677</span> , <span class="number">1.96004856</span>],[ <span class="number">1.5995514</span>  , <span class="number">2.1323943</span> ],[ <span class="number">1.52727223</span> , <span class="number">1.50295551</span>],[ <span class="number">1.80330407</span> , <span class="number">1.57942301</span>],[ <span class="number">1.86487049</span> , <span class="number">1.87234414</span>],[ <span class="number">1.9586354</span>  , <span class="number">1.96279729</span>],[ <span class="number">2.59668134</span> , <span class="number">2.414423</span>  ],[ <span class="number">2.818419</span>   , <span class="number">1.76280366</span>],[ <span class="number">2.01511628</span> , <span class="number">2.10858546</span>],[ <span class="number">2.15907962</span> , <span class="number">1.81593012</span>],[ <span class="number">1.63966834</span> , <span class="number">2.2209023</span> ],[ <span class="number">2.47220599</span> , <span class="number">1.70482956</span>],[ <span class="number">2.08760748</span> , <span class="number">2.51601971</span>],[ <span class="number">1.50547722</span> , <span class="number">1.8487145</span> ],[ <span class="number">1.68125583</span> , <span class="number">2.64968501</span>],[ <span class="number">2.01924282</span> , <span class="number">2.0953572</span> ],[ <span class="number">2.22563534</span> , <span class="number">2.18266325</span>],[ <span class="number">2.2684291</span>  , <span class="number">2.23581599</span>],[ <span class="number">2.13787557</span> , <span class="number">1.9999382</span> ],[ <span class="number">1.02638695</span> , <span class="number">1.68134967</span>],[ <span class="number">2.35614619</span> , <span class="number">1.32072125</span>],[ <span class="number">2.20054871</span> , <span class="number">1.47401445</span>],[ <span class="number">1.99454827</span> , <span class="number">1.71658741</span>],[ <span class="number">1.83269065</span> , <span class="number">2.47662909</span>],[ <span class="number">2.40097251</span> , <span class="number">2.21823862</span>],[ <span class="number">2.54404652</span> , <span class="number">1.85742018</span>],[ <span class="number">1.84150027</span> , <span class="number">2.06350351</span>],[ <span class="number">1.69490855</span> , <span class="number">1.70169334</span>],[ <span class="number">1.44745704</span> , <span class="number">1.88295233</span>],[ <span class="number">2.24376639</span> , <span class="number">1.67530495</span>],[ <span class="number">1.42911921</span> , <span class="number">1.81854548</span>],[ <span class="number">1.33789289</span> , <span class="number">2.27686128</span>],[ <span class="number">2.43509821</span> , <span class="number">1.95032131</span>],[ <span class="number">1.9512447</span>  , <span class="number">1.4595415</span> ],[ <span class="number">2.13041192</span> , <span class="number">1.79372755</span>],[ <span class="number">2.2753866</span>  , <span class="number">2.23781951</span>],[ <span class="number">2.26753401</span> , <span class="number">1.78149305</span>],[ <span class="number">2.06505449</span> , <span class="number">2.01939606</span>],[ <span class="number">2.44426826</span> , <span class="number">2.1437101</span> ],[ <span class="number">2.16607141</span> , <span class="number">2.31077167</span>],[ <span class="number">1.96097237</span> , <span class="number">2.49100193</span>],[ <span class="number">1.37255424</span> , <span class="number">1.60735016</span>],[ <span class="number">1.63947758</span> , <span class="number">2.17852314</span>],[ <span class="number">2.13722666</span> , <span class="number">2.00559707</span>],[ <span class="number">1.222696</span>   , <span class="number">1.67075059</span>],[ <span class="number">2.56982685</span> , <span class="number">2.51218813</span>]])</span><br><span class="line">    x.append(arr1)</span><br><span class="line">    x.append(arr2)</span><br><span class="line">  <span class="keyword">if</span> SHOW_PIC == <span class="number">1</span>:</span><br><span class="line">    figure, ax = plt.subplots()</span><br><span class="line">    ax.set_xlim(left=-<span class="number">1</span>, right=<span class="number">4</span>)</span><br><span class="line">    ax.set_ylim(bottom=-<span class="number">1</span>, top=<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(arr1)):</span><br><span class="line">      plt.plot(arr1[i][<span class="number">0</span>], arr1[i][<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;+&#x27;</span>, color=<span class="string">&#x27;g&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(arr2)):</span><br><span class="line">      plt.plot(arr2[i][<span class="number">0</span>], arr2[i][<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, color=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;x1&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;x2&quot;</span>)</span><br><span class="line">    plt.plot()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcPhi</span>(<span class="params">x</span>):</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">len</span>(x[<span class="number">1</span>])/(<span class="built_in">len</span>(x[<span class="number">0</span>])+<span class="built_in">len</span>(x[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcMu</span>(<span class="params">x,i</span>):</span><br><span class="line">  <span class="keyword">return</span> x[i].mean(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcSigma</span>(<span class="params">x,mu_0,mu_1</span>):</span><br><span class="line">  <span class="built_in">sum</span>=np.array([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>]])</span><br><span class="line">  x0=x[<span class="number">0</span>]</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x0)):</span><br><span class="line">    z0=np.array([x0[i]-mu_0])</span><br><span class="line">    z0T = np.array([x0[i]-mu_0]).transpose()</span><br><span class="line">    <span class="built_in">sum</span> = <span class="built_in">sum</span> + np.dot(z0T, z0)</span><br><span class="line">    <span class="built_in">print</span>(np.dot(z0T, z0))</span><br><span class="line">  x1=x[<span class="number">1</span>]</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x1)):</span><br><span class="line">    z1=np.array([x1[i]-mu_1])</span><br><span class="line">    z1T = np.array([x1[i]-mu_1]).transpose()</span><br><span class="line">    <span class="built_in">sum</span> = <span class="built_in">sum</span> + np.dot(z1T, z1)</span><br><span class="line">    <span class="built_in">print</span>(np.dot(z1T, z1))</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">sum</span>/(<span class="built_in">len</span>(x0)+<span class="built_in">len</span>(x1))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">classify</span>(<span class="params">x,mu_0,mu_1,sigma,phi</span>):</span><br><span class="line">  var0 = multivariate_normal(mean=mu_0.tolist(), cov=sigma.tolist())</span><br><span class="line">  var1 = multivariate_normal(mean=mu_1.tolist(), cov=sigma.tolist())</span><br><span class="line">  <span class="comment"># p(y=1|x) = p(x|y=1)/p(y=1) / p(x) = p(x|y=1)p(y=1) / (p(x|y=1)p(y=1)+p(x|y=0)p(y=0))</span></span><br><span class="line">  <span class="keyword">return</span> var1.pdf(x)*phi/(var1.pdf(x)*phi+var0.pdf(x)*(<span class="number">1</span>-phi)) &gt; <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">testClassify</span>(<span class="params">mu_0,mu_1,sigma,phi</span>):</span><br><span class="line">  <span class="keyword">if</span> SHOW_PIC != <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">  figure, ax = plt.subplots()</span><br><span class="line">  ax.set_xlim(left=-<span class="number">1</span>, right=<span class="number">4</span>)</span><br><span class="line">  ax.set_ylim(bottom=-<span class="number">1</span>, top=<span class="number">4</span>)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    x1 = random.randint(<span class="number">0</span>,<span class="number">3000</span>)/<span class="number">1000</span></span><br><span class="line">    x2 = random.randint(<span class="number">0</span>,<span class="number">3000</span>)/<span class="number">1000</span></span><br><span class="line">    <span class="keyword">if</span>( classify([x1,x2],mu_0,mu_1,sigma,phi)) == <span class="literal">True</span>:</span><br><span class="line">      plt.plot(x1, x2, <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;+&#x27;</span>, color=<span class="string">&#x27;g&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      plt.plot(x1, x2, <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, color=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">  <span class="comment"># draw y = -x+3</span></span><br><span class="line">  (line1_xs, line1_ys) = [(<span class="number">0</span>, <span class="number">3</span>), (<span class="number">3</span>, <span class="number">0</span>)]</span><br><span class="line">  ax.add_line(Line2D(line1_xs, line1_ys, linewidth=<span class="number">1</span>, color=<span class="string">&#x27;blue&#x27;</span>))</span><br><span class="line">  plt.xlabel(<span class="string">&quot;x1&quot;</span>)</span><br><span class="line">  plt.ylabel(<span class="string">&quot;x2&quot;</span>)</span><br><span class="line">  plt.plot()</span><br><span class="line">  plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">  <span class="comment"># 0 debug option</span></span><br><span class="line">  MAKE_DATA = <span class="literal">False</span></span><br><span class="line">  SHOW_PIC = <span class="number">1</span>        <span class="comment"># 0 - do not show   1 - show sample data    2 - show test data</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 1 make the sample</span></span><br><span class="line">  <span class="comment"># the format of x is [ndarray0,ndarray1] , ndarray1 is the set of the first class, we set y=0.</span></span><br><span class="line">  <span class="comment"># ndarray1 is the set of the second class, we set y=1</span></span><br><span class="line">  x = []</span><br><span class="line">  make_data(x)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> MAKE_DATA == <span class="literal">True</span>:</span><br><span class="line">    exit(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 2 learn from the sample</span></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;hello&quot;</span>)</span><br><span class="line">  phi = calcPhi(x)                <span class="comment"># phi  = p(y=1)  means close (2,2)</span></span><br><span class="line">  mu_0 = calcMu(x,<span class="number">0</span>)</span><br><span class="line">  mu_1 = calcMu(x,<span class="number">1</span>)</span><br><span class="line">  sigma = calcSigma(x,mu_0,mu_1)</span><br><span class="line">  <span class="built_in">print</span>(phi,<span class="string">&quot;, &quot;</span>,mu_0,<span class="string">&quot;, &quot;</span>,mu_1)</span><br><span class="line">  <span class="built_in">print</span>(sigma.tolist())</span><br><span class="line"></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;---------&quot;</span>)</span><br><span class="line">  <span class="comment"># 3 test classify</span></span><br><span class="line">  testClassify(mu_0,mu_1,sigma,phi)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>对于两个协方差相同的样本，我们知道两组数据有相同的概率分布曲线，具体都应该是一个个同心圆。因此，我们可以确定一条垂直于两点连线的曲线是对该样本的理论上正确的分割。因此，我们得到如下结果，并与理论分割的曲线，即\(y&#x3D;-x+3\)比较，发现对我们随机产生的样本分类效果非常好。</p>
<img src="/images/机器学习/监督学习-生成型算法GDA结果.png" width=50% height=50% text-align=center/>


<h2 id="附录-A-相关公式"><a href="#附录-A-相关公式" class="headerlink" title="附录 A 相关公式"></a>附录 A 相关公式</h2><p>Hessian矩阵定义:</p>
$${\nabla _A}f(A) = \left( {\begin{array}{*{20}{c}}  {\frac{{\partial f}}{{\partial {A _{11}}}}}& \ldots &{\frac{{\partial f}}{{\partial {A _{1n}}}}} \\\    \vdots & \ddots & \vdots  \\\   {\frac{{\partial f}}{{\partial {A _{n1}}}}}& \cdots &{\frac{{\partial f}}{{\partial {A _{nn}}}}} \end{array}} \right)$$



<p>矩阵的迹的定义:</p>

$$trA = \sum\limits _{i = 1}^n {{A _{ii}}}$$


<p>关于矩阵迹的公式:</p>

$$tr(a) = a$$
$$tr(aA) = atr(A)$$
$$tr(aA) = atr(A)$$
$$tr(ABC) = tr(CAB) + tr(BCA)$$
$$trA = tr{A^T}$$
$$tr(A + B) = trA + trB$$
$${\nabla _A}tr(AB) = {B^T}$$
$${\nabla _{A^T}}f(A) = {({\nabla _A}f(A))^T}$$
$${\nabla _{A^T}}trAB{A^T}C = CAB + {C^T}A{B^T}$$
$${\nabla _{A^T}}trAB{A^T}C = CAB + {C^T}A{B^T}$$


<blockquote>
<p>对矩阵做展开能证明上述大部分公式，这里暂时略。</p>
</blockquote>
<h2 id="附录B"><a href="#附录B" class="headerlink" title="附录B"></a>附录B</h2><p>手写公式和word版博客地址:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/zhengchenyu/MyBlog/tree/master/doc/mlearning/生成型学习算法</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2017/08/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-5-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%94%9F%E6%88%90%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" data-id="cm6uj1jcl00077rrm4tvc5qho" data-title="机器学习-2.5-监督学习之生成型学习算法" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/08/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-6-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          机器学习-2.6-监督学习之朴素贝叶斯算法
        
      </div>
    </a>
  
  
    <a href="/2017/07/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-4-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8Bsoftmax/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">机器学习-2.4-监督学习之softmax</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/02/">February 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/02/07/2024-04-22/">(no title)</a>
          </li>
        
          <li>
            <a href="/2025/02/07/ErasuceCode%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/12/25/RSS-%E8%BF%9C%E7%A8%8BMerge%E7%9A%84%E8%AE%BE%E8%AE%A1/">RSS-远程Merge的设计</a>
          </li>
        
          <li>
            <a href="/2023/12/25/RSS-%E8%BF%9C%E7%A8%8BMerge%E7%9A%84%E8%AE%BE%E8%AE%A1EN/">RSS-Remote Merge Design</a>
          </li>
        
          <li>
            <a href="/2017/10/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-3-2-%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/">机器学习-3.2-非监督学习之主成分分析.md</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 zhengchenyu<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>