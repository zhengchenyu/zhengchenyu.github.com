<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="zhengchenyu">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-机器学习-2-4-监督学习之softmax" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2017/07/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-4-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8Bsoftmax/" class="article-date">
  <time class="dt-published" datetime="2017-07-24T10:35:50.000Z" itemprop="datePublished">2017-07-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2017/07/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-4-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8Bsoftmax/">机器学习-2.4-监督学习之softmax</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="Softmax-Regression"><a href="#Softmax-Regression" class="headerlink" title="Softmax Regression"></a>Softmax Regression</h1><p>下面介绍一下指数族分布的另外一个例子。之前的逻辑回归中，可以用来解决二分类的问题。对于有k中可能结果的时候，问题就转化为多分类问题了，也就是接下来要说明的softmax regression问题。<br>s</p>
<h2 id="1-函数定义"><a href="#1-函数定义" class="headerlink" title="1 函数定义"></a>1 函数定义</h2><p>在进行下一步推导前，我们先定义部分辅助函数。我们这里事先定义一个k-1维向量\(T(y)\)，这里具体对应于指数族分布的\(T(y)\),具体定义如下:</p>

$$T(1) = \left[ \begin{gathered}  1 \hfill \\\  0 \hfill \\\  0 \hfill \\\  ... \hfill \\\  0 \hfill \\ \end{gathered}  \right],T(2) = \left[ \begin{gathered}  0 \hfill \\\  1 \hfill \\\  0 \hfill \\\  ... \hfill \\\  0 \hfill \\ \end{gathered}  \right],T(3) = \left[ \begin{gathered}  0 \hfill \\\  0 \hfill \\\  1 \hfill \\\  ... \hfill \\\  0 \hfill \\\ \end{gathered}  \right],...,T(k - 1) = \left[ \begin{gathered}  0 \hfill \\\  0 \hfill \\\  0 \hfill \\\  ... \hfill \\\  1 \hfill \\ \end{gathered}  \right],T(k) = \left[ \begin{gathered}  0 \hfill \\\  0 \hfill \\\  0 \hfill \\\  ... \hfill \\\  0 \hfill \\ \end{gathered}  \right]$$


<p>我们定义\(T(y) _1\)表示的\(T(y)\)第一个元素，其他依次类推。</p>
<p>然后再引入一个函数，具体定义如下:</p>
$$1\{ true\} = 1$$
$$1\{ false\} = 0$$

<blockquote>
<p>可以通过带入值的方式验证上式。</p>
</blockquote>
<h2 id="2模型推导"><a href="#2模型推导" class="headerlink" title="2	模型推导"></a>2	模型推导</h2><p>在softmax中,我们知道\(y \in \{ 1,2,…,k\}\)。我们设置\(y&#x3D;1\)的概率为\(\phi _1\),<br>类似的有\(y&#x3D;k\)的概率为\(\phi _k\)，并且我们轻易得到如下公式：</p>
$$\sum\limits _{i = 1}^k {\phi _i}  = 1$$$${\phi _k} = \sum\limits _{i = 1}^{k - 1}{\phi _i}$$


<p>由于我们已经知道\(p(y &#x3D; i) &#x3D; {\phi _i}\)，综上可以得到:</p>

$$p(y) = \phi _1^{1\{ y = 1\} }\phi _2^{1\{ y = 2\} }...\phi _k^{1\{ y = k\} } = \phi _1^{1\{ y = 1\} }\phi _2^{1\{ y = 2\} }...\phi _k^{1 - \sum\limits _{i = 1}^{k - 1} {1\{ y = i\} } } = \phi _1^{T{(y)} _1}\phi _2^{T{{(y)} _2}}...\phi _k^{1 - \sum\limits _{i = 1}^{k - 1} {T{(y)} _i} }$$$$p(y) = \exp (T{(y) _1}\ln {\phi _1} + T{(y) _2}\ln {\phi _2} + ... + (1 - \sum\limits _{i = 1}^{k - 1} {T{{(y)} _i}} )\ln {\phi _k})$$$$p(y) = \exp (T{(y) _1}\ln \frac{\phi _1}{\phi _k} + T{(y) _2}\ln \frac{\phi _2}{\phi _k} + ... + T{(y) _{k - 1}}\ln \frac{\phi _{k - 1}}{\phi _k} + \ln {\phi _k})$$


<p>我们对照指数族分布，其中\(T(y)\)我们已经定义，可以得到其他参数如下:</p>

$$b(y) = 1$$
$$a(\eta ) =  - \ln {\phi _k}$$
$$\eta  = \left[ \begin{gathered}  \ln \frac{\phi _1}{\phi _k} \hfill \\\  \ln \frac{\phi _2}{\phi _k} \hfill \\\  ... \hfill \\\  \ln \frac{\phi _{k - 1}}{\phi _k} \hfill \\\ \end{gathered}  \right]$$


<p>我们用\(\eta\)来表示\(\phi\)，目标是将我们的问题归整为一个变量的问题，从而更容易计算。对上面向量拆开计算得到:</p>

$${\eta _i} = \ln \frac{\phi _i}{\phi _k}$$


<p>继而可以转化为:</p>

$${\eta _i} = \ln \frac{\phi _i}{\phi _k}$$


<p>又由于我们已知\(\sum\limits _{j &#x3D; 1}^k {\phi _j}  &#x3D; 1\)，所以有:</p>

$$\sum\limits _{i = 1}^k {e^{\eta _i}{\phi _k}}  = 1$$


<p>继而有:</p>

$${\phi _k} = \frac{1}{\sum\limits _{j = 1}^k {e^{\eta _j}}}$$


<p>最后我们可以得到:</p>

$${\phi _i} = \frac{e^{\eta _i}}{\sum\limits _{j = 1}^k {e^{\eta _j}}}$$


<p>由于指数组分布假设是关于输入的线性函数，所以得到在已知\(x\)和\(\theta\)的情况下\(y&#x3D;i\)的概率的公式，如下：</p>
$$p(y = i|x,\theta ) = {\phi _i} = \frac{e^{\eta _i}}{\sum\limits _{j = 1}^k {e^{\eta _j}}} = \frac{e^{\theta _i^Tx}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}}$$


<p>这里我们可以得到\({h_\theta }(x)\),如下:</p>
$${h_\theta }(x) = E[T(y)|x,\theta ] = E[\left( \begin{gathered}  1\{ y = 1\}  \hfill \\\  1\{ y = 2\}  \hfill \\\  ... \hfill \\\  1\{ y = k - 1\}  \hfill \\\ \end{gathered}  \right)|x,\theta ]$$

$${h_ \theta }(x) = E[\left( \begin{gathered}  {\phi _1} \hfill \\\  {\phi _2} \hfill \\\  ... \hfill \\\  {\phi _{k - 1}} \hfill \\\ \end{gathered}  \right)|x,\theta ] = \left( \begin{gathered}  \frac{e^{\theta _1^Tx}}{\sum\limits _{j = 1}^k {e^{\theta  _j^Tx}}} \hfill \\\  \frac{e^{\theta _2^Tx}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}} \hfill \\\  ... \hfill \\\  \frac{e^{\theta _{k - 1}^Tx}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}} \hfill \\\\end{gathered}  \right)$$


<blockquote>
<p>其中,\(k\)为取值的可能集合的大小，\(\theta\)为一个\(k*n\)的矩阵。</p>
</blockquote>
<p>到这里我们可以定义我们的损失函数了，如下:</p>
$$J(x) = \frac{1}{2}\sum\limits _{i = 1}^m {{(T(y) - {h _\theta }(x))}^2}$$


<p>上式是一个向量，我们可以对这个向量继续求平方和，来衡量准确度，如下：</p>
$$J(x) = \frac{1}{2}\sum\limits _{i = 1}^m {|{{(T(y) - {h _\theta }(x))}^2}|}$$


<p>损失函数已经定义完成，我们就有了算法的截止条件了。接下来就是找到算法的最优迭代方向，也就是计算其偏导数了。</p>
<p>我们使用最大释然性来计算迭代方向。建模的原则是：对于一个已知\(y&#x3D;i\)的样本\((x,y)\)，我们需要找到一个方向去迭代\(\theta\)，使得\(\phi _i\)尽可能大，使得其他\(\phi\)尽可能小。当然由于所有\(\phi\)之和为1,所以我们只需要保证\(\phi _i\)尽可能大即可。因此,在已经的\(m\)个样本的情况下，我们只需要保证下面的式子最大:</p>

$$\sum\limits _{i = 1}^m {p(y = {y^i}|x,\theta )}  = \sum\limits _{i = 1}^m {\prod\limits _{l = 1}^k {p{(y = l|x,\theta )}^{1\{ {y^i} = l\}}} }$$


<p>取自然对数后，我们定义最大似然函数:</p>

$$l(\theta ) = \sum\limits _{i = 1}^m {\ln \prod\limits _{l = 1}^k {(\frac{e^{\theta _l^Tx}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}})^{1\{{y^i} = l\}}}}$$

<p>当且仅当\({y^i} &#x3D; l\)的时候, \(^{1{y^i&#x3D;l\} &#x3D; 1})。其他值为0，我们可以将连乘转化，如下：</p>
$$l(\theta ) = \sum\limits _{i = 1}^m {\ln (\frac{e^{\theta _{y^i}^Tx}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}})}  = \sum\limits _{i = 1}^m {\ln [\theta _{y^i}^Tx - \ln \sum\limits _{j = 1}^k {e^{\theta _j^Tx}}]} $$


<p>然后对其求偏导数,对\(y _i&#x3D;f\)的时候后有：</p>
$$\frac{\partial l(\theta )}{\partial {\theta _f}} = \sum\limits _{i = 1}^m {\ln [{x _f} - \frac{e^{\theta _f^Tx}{x _f}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}}]}$$


<p>对\({y_i} \ne f\)的时候，有：</p>
$$\frac{\partial l(\theta )}{\partial {\theta _f}} = \sum\limits _{i = 1}^m {\ln [0 - \frac{e^{\theta _f^Tx}{x _f}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}}]}$$


<p>综上，可以得到:</p>
$$\frac{\partial l(\theta )}{\partial {\theta _f}} = \sum\limits _{i = 1}^m {\ln [1\{ {y^i} = f\}  - \frac{e^{\theta _f^Tx}}{\sum\limits _{j = 1}^k {e^{\theta _j^Tx}}}]{x _f}}  = \sum\limits _{i = 1}^m {\ln [1\{{y^i} = f\}  - {\phi _f}]{x _f}}$$


<blockquote>
<p>\(x _f\)是一个向量，因为公式一次迭代更新一个维度下的一组\(\theta\)。事实上，这里是一个向量求微分。如果我们对的某个元素\(\theta\)进行微分，我们依然能够得到这个公式。</p>
</blockquote>
<h2 id="3-实际问题的解决"><a href="#3-实际问题的解决" class="headerlink" title="3 实际问题的解决"></a>3 实际问题的解决</h2><p>我们随机制造一组数据，在\(2*2\)的空间内使用直线\({x _2} &#x3D; 0.5*{x _1}\)和直线\({x _2} &#x3D; -0.5*{x _1}+2\)具体的分类如下:</p>
<img src="/images/机器学习/监督学习-softmax样本.png" width=50% height=50% text-align=center/>

<p>根据之前的推导，编写如下代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.lines <span class="keyword">import</span> Line2D</span><br><span class="line"></span><br><span class="line">g_x_arr=[[<span class="number">1</span>, <span class="number">0.035</span>, <span class="number">1.344</span>], [<span class="number">1</span>, <span class="number">0.662</span>, <span class="number">0.598</span>], [<span class="number">1</span>, <span class="number">1.791</span>, <span class="number">1.889</span>], [<span class="number">1</span>, <span class="number">0.158</span>, <span class="number">0.12</span>], [<span class="number">1</span>, <span class="number">1.55</span>, <span class="number">1.835</span>], [<span class="number">1</span>, <span class="number">2.0</span>, <span class="number">0.613</span>], [<span class="number">1</span>, <span class="number">1.176</span>, <span class="number">0.368</span>], [<span class="number">1</span>, <span class="number">0.564</span>, <span class="number">0.043</span>], [<span class="number">1</span>, <span class="number">1.559</span>, <span class="number">1.507</span>], [<span class="number">1</span>, <span class="number">1.998</span>, <span class="number">0.988</span>], [<span class="number">1</span>, <span class="number">0.082</span>, <span class="number">0.941</span>], [<span class="number">1</span>, <span class="number">0.542</span>, <span class="number">1.371</span>], [<span class="number">1</span>, <span class="number">0.542</span>, <span class="number">0.671</span>], [<span class="number">1</span>, <span class="number">1.34</span>, <span class="number">1.856</span>], [<span class="number">1</span>, <span class="number">0.049</span>, <span class="number">0.089</span>], [<span class="number">1</span>, <span class="number">1.933</span>, <span class="number">0.871</span>], [<span class="number">1</span>, <span class="number">1.753</span>, <span class="number">1.024</span>], [<span class="number">1</span>, <span class="number">0.315</span>, <span class="number">1.341</span>], [<span class="number">1</span>, <span class="number">0.829</span>, <span class="number">1.26</span>], [<span class="number">1</span>, <span class="number">0.686</span>, <span class="number">1.721</span>], [<span class="number">1</span>, <span class="number">1.222</span>, <span class="number">1.129</span>], [<span class="number">1</span>, <span class="number">0.55</span>, <span class="number">0.075</span>], [<span class="number">1</span>, <span class="number">0.767</span>, <span class="number">0.346</span>], [<span class="number">1</span>, <span class="number">1.516</span>, <span class="number">1.752</span>], [<span class="number">1</span>, <span class="number">1.347</span>, <span class="number">0.905</span>], [<span class="number">1</span>, <span class="number">0.127</span>, <span class="number">0.782</span>], [<span class="number">1</span>, <span class="number">1.169</span>, <span class="number">1.272</span>], [<span class="number">1</span>, <span class="number">1.301</span>, <span class="number">0.273</span>], [<span class="number">1</span>, <span class="number">0.081</span>, <span class="number">0.739</span>], [<span class="number">1</span>, <span class="number">0.203</span>, <span class="number">0.658</span>], [<span class="number">1</span>, <span class="number">0.347</span>, <span class="number">1.064</span>], [<span class="number">1</span>, <span class="number">0.793</span>, <span class="number">1.193</span>], [<span class="number">1</span>, <span class="number">1.428</span>, <span class="number">0.326</span>], [<span class="number">1</span>, <span class="number">0.509</span>, <span class="number">0.983</span>], [<span class="number">1</span>, <span class="number">0.12</span>, <span class="number">0.884</span>], [<span class="number">1</span>, <span class="number">0.251</span>, <span class="number">0.282</span>], [<span class="number">1</span>, <span class="number">0.73</span>, <span class="number">0.445</span>], [<span class="number">1</span>, <span class="number">1.889</span>, <span class="number">1.323</span>], [<span class="number">1</span>, <span class="number">1.314</span>, <span class="number">1.795</span>], [<span class="number">1</span>, <span class="number">1.297</span>, <span class="number">1.467</span>], [<span class="number">1</span>, <span class="number">1.669</span>, <span class="number">0.613</span>], [<span class="number">1</span>, <span class="number">0.753</span>, <span class="number">0.114</span>], [<span class="number">1</span>, <span class="number">0.94</span>, <span class="number">1.972</span>], [<span class="number">1</span>, <span class="number">0.738</span>, <span class="number">1.603</span>], [<span class="number">1</span>, <span class="number">1.508</span>, <span class="number">1.237</span>], [<span class="number">1</span>, <span class="number">0.979</span>, <span class="number">0.572</span>], [<span class="number">1</span>, <span class="number">0.128</span>, <span class="number">1.254</span>], [<span class="number">1</span>, <span class="number">0.569</span>, <span class="number">0.155</span>], [<span class="number">1</span>, <span class="number">0.88</span>, <span class="number">0.211</span>], [<span class="number">1</span>, <span class="number">0.405</span>, <span class="number">0.603</span>], [<span class="number">1</span>, <span class="number">1.02</span>, <span class="number">1.9</span>], [<span class="number">1</span>, <span class="number">0.438</span>, <span class="number">1.535</span>], [<span class="number">1</span>, <span class="number">1.506</span>, <span class="number">1.638</span>], [<span class="number">1</span>, <span class="number">1.712</span>, <span class="number">0.394</span>], [<span class="number">1</span>, <span class="number">0.556</span>, <span class="number">0.124</span>], [<span class="number">1</span>, <span class="number">0.444</span>, <span class="number">0.115</span>], [<span class="number">1</span>, <span class="number">0.595</span>, <span class="number">1.009</span>], [<span class="number">1</span>, <span class="number">0.165</span>, <span class="number">0.089</span>], [<span class="number">1</span>, <span class="number">1.57</span>, <span class="number">0.634</span>], [<span class="number">1</span>, <span class="number">1.429</span>, <span class="number">1.181</span>], [<span class="number">1</span>, <span class="number">0.8</span>, <span class="number">0.671</span>], [<span class="number">1</span>, <span class="number">1.914</span>, <span class="number">1.091</span>], [<span class="number">1</span>, <span class="number">0.594</span>, <span class="number">0.569</span>], [<span class="number">1</span>, <span class="number">0.935</span>, <span class="number">0.277</span>], [<span class="number">1</span>, <span class="number">0.47</span>, <span class="number">0.522</span>], [<span class="number">1</span>, <span class="number">0.94</span>, <span class="number">1.924</span>], [<span class="number">1</span>, <span class="number">0.194</span>, <span class="number">1.933</span>], [<span class="number">1</span>, <span class="number">0.612</span>, <span class="number">0.613</span>], [<span class="number">1</span>, <span class="number">0.236</span>, <span class="number">0.894</span>], [<span class="number">1</span>, <span class="number">1.888</span>, <span class="number">0.251</span>], [<span class="number">1</span>, <span class="number">1.548</span>, <span class="number">0.191</span>], [<span class="number">1</span>, <span class="number">1.543</span>, <span class="number">0.603</span>], [<span class="number">1</span>, <span class="number">1.521</span>, <span class="number">0.02</span>], [<span class="number">1</span>, <span class="number">0.923</span>, <span class="number">0.856</span>], [<span class="number">1</span>, <span class="number">0.649</span>, <span class="number">1.31</span>], [<span class="number">1</span>, <span class="number">0.379</span>, <span class="number">1.746</span>], [<span class="number">1</span>, <span class="number">1.345</span>, <span class="number">0.902</span>], [<span class="number">1</span>, <span class="number">0.937</span>, <span class="number">0.524</span>], [<span class="number">1</span>, <span class="number">1.018</span>, <span class="number">0.68</span>], [<span class="number">1</span>, <span class="number">1.738</span>, <span class="number">1.623</span>], [<span class="number">1</span>, <span class="number">1.534</span>, <span class="number">1.9</span>], [<span class="number">1</span>, <span class="number">0.139</span>, <span class="number">1.911</span>], [<span class="number">1</span>, <span class="number">1.508</span>, <span class="number">1.173</span>], [<span class="number">1</span>, <span class="number">0.798</span>, <span class="number">0.865</span>], [<span class="number">1</span>, <span class="number">0.451</span>, <span class="number">1.186</span>], [<span class="number">1</span>, <span class="number">1.63</span>, <span class="number">1.123</span>], [<span class="number">1</span>, <span class="number">0.82</span>, <span class="number">0.848</span>], [<span class="number">1</span>, <span class="number">1.213</span>, <span class="number">1.48</span>], [<span class="number">1</span>, <span class="number">0.894</span>, <span class="number">0.664</span>], [<span class="number">1</span>, <span class="number">1.456</span>, <span class="number">0.934</span>], [<span class="number">1</span>, <span class="number">0.59</span>, <span class="number">1.525</span>], [<span class="number">1</span>, <span class="number">0.522</span>, <span class="number">1.329</span>], [<span class="number">1</span>, <span class="number">1.179</span>, <span class="number">1.396</span>], [<span class="number">1</span>, <span class="number">0.527</span>, <span class="number">0.273</span>], [<span class="number">1</span>, <span class="number">1.399</span>, <span class="number">1.215</span>], [<span class="number">1</span>, <span class="number">0.966</span>, <span class="number">1.514</span>], [<span class="number">1</span>, <span class="number">1.341</span>, <span class="number">0.028</span>], [<span class="number">1</span>, <span class="number">0.479</span>, <span class="number">0.191</span>], [<span class="number">1</span>, <span class="number">1.193</span>, <span class="number">0.724</span>], [<span class="number">1</span>, <span class="number">0.714</span>, <span class="number">1.285</span>]]</span><br><span class="line">g_Ty_arr=[[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>]]</span><br><span class="line">g_y_arr=[<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">phi</span>(<span class="params">i,theta,x</span>):</span><br><span class="line">	theta_n = <span class="built_in">len</span>(theta)</span><br><span class="line">	numerator = math.exp(np.dot(theta[i], x))</span><br><span class="line">	denominator = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(theta_n):                      <span class="comment"># maybe need to optimize</span></span><br><span class="line">		denominator = denominator + math.exp(np.dot(theta[j],x))</span><br><span class="line">	<span class="keyword">return</span> numerator/denominator</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">h_theta</span>(<span class="params">theta,x</span>):</span><br><span class="line">	theta_n = <span class="built_in">len</span>(theta)</span><br><span class="line">	ret = []</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(theta_n):</span><br><span class="line">		ret.append(phi(i,theta,x))</span><br><span class="line">	<span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">one</span>(<span class="params">y,i</span>):</span><br><span class="line">	<span class="keyword">if</span> y == i:</span><br><span class="line">		<span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		<span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">derl</span>(<span class="params">f,x_arr,y_arr,theta</span>):</span><br><span class="line">	<span class="comment"># 0.</span></span><br><span class="line">	<span class="comment"># f is the dimension to calc the partial derivative</span></span><br><span class="line">	<span class="comment"># 1. calc the size</span></span><br><span class="line">	<span class="comment"># theta_n is the size of theta, equals to the length of Ty&#x27; result set -1 .Here, equals lenght of &#123;0,1,2&#125; = 3-1 =2</span></span><br><span class="line">	<span class="comment"># m is the sum of samples</span></span><br><span class="line">	<span class="comment"># x_n is the dimension of the input variable &#x27;x&#x27; + 1 (for x_0 =1). Here is 2 + 1 = 3</span></span><br><span class="line">	theta_n = <span class="built_in">len</span>(theta)</span><br><span class="line">	m = <span class="built_in">len</span>(x_arr)</span><br><span class="line">	x_n = <span class="built_in">len</span>(x_arr[<span class="number">0</span>])</span><br><span class="line">	<span class="comment"># initial the output variable sum.Here is a vector, and the length of this vector is x_n</span></span><br><span class="line">	<span class="built_in">sum</span> = []</span><br><span class="line">	<span class="keyword">for</span> x_dim_index <span class="keyword">in</span> <span class="built_in">range</span>(x_n):</span><br><span class="line">			<span class="built_in">sum</span>.append(<span class="number">0</span>)</span><br><span class="line">	<span class="comment"># 2. calc the partial derivative</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(x_n):</span><br><span class="line">		<span class="built_in">sum</span>[i] = <span class="number">0</span></span><br><span class="line">		<span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">			<span class="built_in">sum</span>[i]=<span class="built_in">sum</span>[i]+(one(y_arr[j],f)-phi(f,theta,x_arr[j]))*x_arr[j][i]</span><br><span class="line">		<span class="built_in">sum</span>[i]=<span class="built_in">sum</span>[i]/m</span><br><span class="line">	<span class="comment">#sum=plus_vector(sum,theta[f],0.1)</span></span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">sum</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plus_vector</span>(<span class="params">arr1,arr2,a</span>):</span><br><span class="line">	<span class="keyword">return</span> [x + a * y <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(arr1, arr2)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">theta,a,x_arr,y_arr</span>):</span><br><span class="line">	theta_n = <span class="built_in">len</span>(theta)</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(theta_n):</span><br><span class="line">		theta[i]=plus_vector(theta[i],derl(i, x_arr,y_arr,theta),a)</span><br><span class="line">	<span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">judge1</span>(<span class="params">theta,x_arr,y_arr_vector,limit,debug</span>):</span><br><span class="line">	j_theta = J(theta,x_arr,y_arr_vector)</span><br><span class="line">	<span class="keyword">if</span> j_theta &lt; limit:</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">	<span class="keyword">if</span> debug:</span><br><span class="line">		<span class="built_in">print</span>(<span class="string">&quot;|J_theta(x)| = &quot;</span>, j_theta,<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">J</span>(<span class="params">theta,x_arr,y_arr_vector</span>):</span><br><span class="line">	<span class="built_in">sum</span>=<span class="number">0</span></span><br><span class="line">	m = <span class="built_in">len</span>(x_arr)</span><br><span class="line">	y_len = <span class="built_in">len</span>(y_arr_vector[<span class="number">0</span>])</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">		<span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(y_len):</span><br><span class="line">			<span class="built_in">sum</span> = <span class="built_in">sum</span> + (phi(j,theta,x_arr[i]) - y_arr_vector[i][j])**<span class="number">2</span></span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">sum</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calTheVaule</span>():</span><br><span class="line">	<span class="comment">#theta=[[theta_1_1,theta_1_2],[theta_2_1,theta_2_2],[theta_3_1,theta_3_2]]</span></span><br><span class="line">	theta = [[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]]</span><br><span class="line">	a = <span class="number">1</span></span><br><span class="line">	count = <span class="number">0</span></span><br><span class="line">	<span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">		<span class="keyword">if</span> judge1(theta,g_x_arr,g_Ty_arr,<span class="number">1</span>,<span class="literal">True</span>):</span><br><span class="line">			<span class="keyword">break</span></span><br><span class="line">		theta=update(theta,a,g_x_arr,g_y_arr)</span><br><span class="line">		count = count + <span class="number">1</span></span><br><span class="line">		<span class="built_in">print</span>(<span class="string">&quot;count=&quot;</span>,count,<span class="string">&quot;theta=&quot;</span>,theta)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcIndex</span>(<span class="params">theta,x</span>):</span><br><span class="line">  phi0 = phi(<span class="number">0</span>,theta,x)</span><br><span class="line">  phi1 = phi(<span class="number">1</span>,theta,x)</span><br><span class="line">  phi2 = phi(<span class="number">2</span>,theta,x)</span><br><span class="line">  <span class="keyword">if</span> phi0 &gt;= phi1 <span class="keyword">and</span> phi0 &gt;=phi1:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  <span class="keyword">elif</span> phi1&gt;= phi0 <span class="keyword">and</span> phi1 &gt;= phi2:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">testValue</span>():</span><br><span class="line">  <span class="comment"># limit =1， count = 33844</span></span><br><span class="line">  theta = [[<span class="number">17.418683742474045</span>, <span class="number">13.989348587756815</span>, -<span class="number">37.731307869980014</span>],[-<span class="number">33.56035031194091</span>, <span class="number">1.3797244978249763</span>, <span class="number">34.095843086377855</span>],[<span class="number">19.97548491920147</span>, -<span class="number">11.65484983882828</span>, <span class="number">7.628008093823735</span>]]</span><br><span class="line">  samples = <span class="number">100</span></span><br><span class="line">  figure, ax = plt.subplots()</span><br><span class="line">  <span class="comment"># 设置x，y值域</span></span><br><span class="line">  ax.set_xlim(left=<span class="number">0</span>, right=<span class="number">2</span>)</span><br><span class="line">  ax.set_ylim(bottom=<span class="number">0</span>, top=<span class="number">2</span>)</span><br><span class="line">  <span class="comment"># 两条line的数据</span></span><br><span class="line">  (line1_xs, line1_ys) = [(<span class="number">0</span>, <span class="number">2</span>), (<span class="number">0</span>, <span class="number">1</span>)]</span><br><span class="line">  (line2_xs, line2_ys) = [(<span class="number">0</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">1</span>)]</span><br><span class="line">  <span class="comment"># 创建两条线，并添加</span></span><br><span class="line">  ax.add_line(Line2D(line1_xs, line1_ys, linewidth=<span class="number">1</span>, color=<span class="string">&#x27;blue&#x27;</span>))</span><br><span class="line">  ax.add_line(Line2D(line2_xs, line2_ys, linewidth=<span class="number">1</span>, color=<span class="string">&#x27;blue&#x27;</span>))</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(samples):</span><br><span class="line">    x_0 = random.randint(<span class="number">0</span>,<span class="number">2000</span>)/<span class="number">1000</span></span><br><span class="line">    x_1 = random.randint(<span class="number">0</span>,<span class="number">2000</span>)/<span class="number">1000</span></span><br><span class="line">    index = calcIndex(theta,[<span class="number">1</span>,x_0,x_1])</span><br><span class="line">    <span class="keyword">if</span> index == <span class="number">0</span>:</span><br><span class="line">      plt.plot(x_0, x_1, <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;x&#x27;</span>, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> index == <span class="number">1</span>:</span><br><span class="line">      plt.plot(x_0, x_1, <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;+&#x27;</span>, color=<span class="string">&#x27;g&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      plt.plot(x_0, x_1, <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, color=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">  plt.xlabel(<span class="string">&quot;x1&quot;</span>)</span><br><span class="line">  plt.ylabel(<span class="string">&quot;x2&quot;</span>)</span><br><span class="line">  plt.plot()</span><br><span class="line">  plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__== <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">  <span class="comment">#calTheVaule()</span></span><br><span class="line">  testValue()</span><br></pre></td></tr></table></figure>

<p>经过33844次迭代之后，我们得到\(\theta\) &#x3D; [[17.418683742474045, 13.989348587756815, -37.731307869980014],[-33.56035031194091, 1.3797244978249763, 34.095843086377855],[19.97548491920147, -11.65484983882828, 7.628008093823735]]</p>
<blockquote>
<p>迭代的次数越多数据会越准确，这里迭代了上三万次，实际上可以通过牛顿法来减少迭代的次数，这里就不再重新代码了，牛顿法可以参见前面的文章。</p>
</blockquote>
<p>然后，我们随机制造一组值，看看分类效果，具体如下：</p>
<img src="/images/机器学习/监督学习-softmax计算结果.png" width=50% height=50% text-align=center/>

<h2 id="附录-公式推导手写版"><a href="#附录-公式推导手写版" class="headerlink" title="附录 公式推导手写版"></a>附录 公式推导手写版</h2><img src="/images/机器学习/监督学习-softmax公式推导手写版1.png" width=50% height=50% text-align=center/>

<img src="/images/机器学习/监督学习-softmax公式推导手写版2.png" width=50% height=50% text-align=center/>

<img src="/images/机器学习/监督学习-softmax公式推导手写版3.png" width=50% height=50% text-align=center/>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2017/07/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-4-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8Bsoftmax/" data-id="cm6uj1jcl00067rrmbtdt6rl5" data-title="机器学习-2.4-监督学习之softmax" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-机器学习-2.3-监督学习之通用线性模型" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2017/07/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2.3-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%80%9A%E7%94%A8%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time class="dt-published" datetime="2017-07-16T06:33:58.000Z" itemprop="datePublished">2017-07-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2017/07/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2.3-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%80%9A%E7%94%A8%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/">机器学习-2.3-监督学习之通用线性模型</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="Generalized-Linear-Models"><a href="#Generalized-Linear-Models" class="headerlink" title="Generalized Linear Models"></a>Generalized Linear Models</h1><p>回归分析中我们使用了高斯分布,分类问题服从伯努利分布。这两种方法都是第一个更通用的分布的特例，我们称其Generalized Linear Models (GLMs)</p>
<h2 id="1-指数族分布"><a href="#1-指数族分布" class="headerlink" title="1 指数族分布"></a>1 指数族分布</h2><p>我们定义如下形式为指数族分布：</p>
$$p(y;\eta ) = b(y)exp({\eta ^T}T(y) - a(\eta ))$$


<p>其中,\(\eta \)是自然参数(natural parameter )，\(T(y) \)叫做充分统计量(sufficient statistic),一般\(T(y)&#x3D;y\)。\(a(\eta) \)是log partition function 。\({e^{ - a\left( \eta  \right)}}\)在归整化很重要，这个参数确保\(p(y;\eta )\)归整为1。</p>
<h2 id="2-指数族分布的特例"><a href="#2-指数族分布的特例" class="headerlink" title="2 指数族分布的特例"></a>2 指数族分布的特例</h2><p>其中伯努利分布和高斯分布都是指数族分布的一个特例。</p>
<h3 id="2-1-伯努利分布的指数族表达"><a href="#2-1-伯努利分布的指数族表达" class="headerlink" title="2.1 伯努利分布的指数族表达"></a>2.1 伯努利分布的指数族表达</h3><p>伯努利分布的指数表示:</p>
$$p(y,\varphi ) = {\varphi ^y}{(1 - \varphi )^{1 - y}} = \exp (y\log \frac{\varphi }{1 - \varphi } + \log (1 - \varphi ))$$


<p>对照指数函数族的表达式，有\(b(y) &#x3D; 1\),\(T(y) &#x3D; y\),\(\eta  &#x3D; \log \frac{\varphi }{1 - \varphi}\)等价于\(\varphi&#x3D;\frac{e^\eta}{e^\eta + 1}\),\(a(y) &#x3D; \log \frac{1}{1 - \varphi } &#x3D; \log ({e^\eta } + 1)\)</p>
<h3 id="2-2-高斯分布的指数族表达"><a href="#2-2-高斯分布的指数族表达" class="headerlink" title="2.2 高斯分布的指数族表达"></a>2.2 高斯分布的指数族表达</h3><p>高斯分布的指数表示:</p>
$$p(y;\mu ) = \frac{1}{{\sqrt {2\pi } }}\exp ( - \frac{1}{2}{(y - \mu )^2}) = \frac{1}{{\sqrt {2\pi } }}\exp ( - \frac{1}{2}{y^2})\exp (\mu y - \frac{1}{2}{\mu ^2})$$


<p>对照指数函数族的表达式，有\(b(y) &#x3D; \frac{1}{\sqrt {2\pi }}\exp ( - \frac{1}{2}{y^2})\),\( \eta &#x3D; \mu \),\(T(y) &#x3D; y\),\(a(\eta ) &#x3D; \frac{1}{2}{u^2} &#x3D; \frac{1}{2}{\eta ^2}\)。</p>
<h2 id="3-指数函数组的三个假设"><a href="#3-指数函数组的三个假设" class="headerlink" title="3 指数函数组的三个假设"></a>3 指数函数组的三个假设</h2><p>三个假设分别为:</p>
<ul>
<li>\(y|x;\theta\)服从指数分布</li>
<li>给定\(x\)的情况下，我们的目标是预测\(T(y)\)。在大多数的例子中，\(T(y)&#x3D;y\),因此就意味着我们通过学习得到函数h, 使得\(h(x)&#x3D;E(y|x)\), 意思就是给定x，得到函数h使得其与训练样本的数学期望相等。</li>
<li>自然参数\(\theta\)与输入\(x\)是线性关系。</li>
</ul>
<p>对照上面的过程，我们可知伯努利分布和高斯分布都符合这三个假设</p>
<h2 id="4-GLMs公式推导"><a href="#4-GLMs公式推导" class="headerlink" title="4 GLMs公式推导"></a>4 GLMs公式推导</h2><p>还是之前的问题，已知有\(m\)个样本，通过学习模拟正确的\(h(x)\),使得其能够有效的推测\(y\)。(这里我们假定了\(T(y)&#x3D;y\))</p>
<p>得到最大释然函数:</p>
$$l(\eta ) = \sum\limits_{i = 1}^m {b({y^i}){\text{ }}exp({\eta ^T}T({y^i}) - a(\eta ))} $$


<p>然后为了求其最大值，我们对其取自然对数后求倒数。</p>
$$\frac{{\partial \ln (l(\eta ))}}{{\partial \eta }} = \sum\limits _{i = 1}^m {\frac{{\partial (\ln b({y^i}) + {\eta ^T}T({y^i}) - a(\eta ))}}{{\partial \eta }}}  = \sum\limits _{i = 1}^m {(T({y^i})}  - \frac{{\partial a(\eta )}}{{\partial \eta }})$$


<p>在我们之前指数族分布的定义3中，\(\eta\)是各个维度\(x\)的线性表达。有\(\eta  &#x3D; {\theta ^T}x\)。所以，我们对每一个维度求偏导数，有：</p>
$$\frac{{\partial \ln (l(\eta ))}}{{\partial {\theta _j}}} = \frac{{\partial \ln (l(\eta ))}}{{\partial \eta }}\frac{{\partial \eta }}{{\partial {\theta _j}}} = \sum\limits _{i = 1}^m {(T({y^i})}  - \frac{{\partial a(\eta )}}{{\partial \eta }})x _j^i$$

<p>如果指数分布为伯努利分布，我们将式子带入其中，能够轻易的关于\(\theta _j\)的偏导数，且与之前推导的结果一致，具体过程不详细说了。高斯分布同理。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2017/07/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2.3-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%80%9A%E7%94%A8%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" data-id="cm6uj1jcm000d7rrm7rwoc7p2" data-title="机器学习-2.3-监督学习之通用线性模型" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-机器学习-2.2-监督学习之分类" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2017/06/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2.2-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%88%86%E7%B1%BB/" class="article-date">
  <time class="dt-published" datetime="2017-06-13T13:20:17.000Z" itemprop="datePublished">2017-06-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2017/06/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2.2-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%88%86%E7%B1%BB/">机器学习 2.2 监督学习之逻辑回归</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h1><p>本章继续研究监督学习的问题。</p>
<h2 id="1-回归问题"><a href="#1-回归问题" class="headerlink" title="1 回归问题"></a>1 回归问题</h2><p>我们随机制造一个样本空间\({x _2} \geqslant 2{x _1} + 1\)，对于对空间进行分类。具体例子如下：</p>
<img src="/images/机器学习/监督学习-分类问题样本.png" width=50% height=50% text-align=center/>

<p>对于这些样本，假定可以通过二维线性曲线进行分类。然后模拟出某条曲线，使样本得到一个很好的分类。</p>
<p>这个分类问题的样本的y值只有0或1(0-1分布或伯努利分布)，即表示在直线上面或直线下面。如果用之前的线性回归结果将非常糟糕。因为对于分类问题，线性回归得到的线性曲线，得到的\(y &gt; 1\)或\(y &lt; 0\)这些是没有意义的，肯定是错误的。因此引入了logistic函数，转换为Logistic regression，logistic函数如下：</p>

$$h(z) = \frac{1}{{1 + {e^{ - z}}}}$$


<p>函数曲线如下：</p>
<img src="/images/机器学习/监督学习-分类问题辅助函数.png" width=50% height=50% text-align=center/>

<p>如果我们设置\(z &#x3D; {\theta _1}{x _1} + {\theta _0} - {x _2}\)。因此，对于分类问题就可以转化为：对于\({x _2} \geqslant {\theta _1}{x _1} + {\theta _0}\)就可以转化为h(z)趋向于0的程度，对于\({x _2} \leqslant {\theta _1}{x _1} + {\theta _0}\)就可以转化为h(z)趋向于1的程度。我们简化为如下公式:</p>

$$z = {\theta _1}{x _1} + {\theta _0}{x _0} + {\theta _2}{x _2} = {\theta ^T}x$$


<p>其中,\({x _0} &#x3D; 1\)，\({\theta _2} &#x3D;  - 1\)。</p>
<h2 id="2-回归问题推导"><a href="#2-回归问题推导" class="headerlink" title="2 回归问题推导"></a>2 回归问题推导</h2><p>我们求辅助函数的导数，以备后来使用：</p>

$$h'(z) = (\frac{1}{{1 + {e^{ - z}}}})' =  - \frac{{ - {e^{ - z}}}}{{{{(1 + {e^{ - z}})}^2}}} = \frac{1}{{1 + {e^{ - z}}}}(1 - 1 + {e^{ - z}}) = h(z)(1 - h(z))$$


<p>接下来我们从公式角度对问题进行建模分析与推导。</p>
<p>这里我们将我们的函数\({h_\theta }(x)\)转化为如下形式:</p>

$${h_\theta }(x) = \frac{1}{{1 + {e^{ - {\theta ^T}x}}}}$$


<p>对于0-1分布，由于\({h_\theta }(x)\)只能为0或1。所以，在已知\(x\)和\({\theta}\)的情况下，我们可以认为有\({h_\theta }(x)\)的概率得到\(y&#x3D;1\)。因此有如下公式：</p>

$$p(y = 1|x,\theta ) = {h_\theta }(x)$$
$$p(y = 0|x,\theta ) = 1 - {h_\theta }(x)$$


<p>综合两个公式有如下公式：</p>

$$p(y|x,\theta ) = {h_\theta }{(x)^y}{(1 - {h _\theta }(x))^{(1 - y)}}$$

<p>即在已经样本的情况下得到准确的\(y\)，即要使用极大释然方法，得到样本概率乘积的最大值,即得到下式的最大值：</p>

$$L(\theta ) = \prod\limits _{i = 1}^m {{h _\theta }{{(x{}^i)}^{{y^i}}}{{(1 - {h _\theta }({x^i}))}^{(1 - {y^i})}}} $$


<p>取自然对数，对其中一个维度\({\theta _j}\)求偏导数，如下：</p>

$$\ell ({\theta _j}) = \frac{{\partial \ln (L(\theta ))}}{{\partial {\theta _j}}} = \frac{\partial } {{\partial {\theta _j}}}(\sum\limits _{i = 1}^m {({y^i}\ln ({h _\theta }(x{}^i)) + (1 - {y^i})} \ln (1 - {h _\theta }(x{}^i))))$$ 


<p>我们使用一个辅助函数\(z &#x3D; {\theta ^T}x\), 并且\(\ln x\)的倒数是\(\frac{1}{x}\)。<br>又由于复合函数有如下求导法则：</p>

$$\frac{{df(g(x))}}{{dx}} = \frac{{df(z)}}{{dz}}g(x)'$$


<p>其中\(z &#x3D; g(x)\)。。(这里再次强调下标识维度，上表是样本编号)。所以有：</p>

$$\ell ({\theta _j}) = \frac{{\partial L(\theta )}}{{\partial {\theta _j}}} = \frac{\partial }{{\partial {\theta _j}}}(\sum\limits _{i = 1}^m {({y^i}\ln ({h _\theta }(x{}^i)) + (1 - {y^i})} \ln (1 - {h _\theta }(x{}^i))))$$


<p>这里设\(z &#x3D; {\theta ^T}x\)，有：</p>

$$\ell ({\theta _j}) = \sum\limits _{i = 1}^m {({y^i}\frac{1}{{h(z)}}h'(z) + (1 - {y^i})} \frac{1}{{1 - h(z)}}( - h'(z)))$$


<p>将之前的倒数公式带入其中,有:</p>

$$\ell ({\theta _j}) = \sum\limits _{i = 1}^m {(x _j^i(y - {h _\theta }({x^i})))} $$


<p>因此这样就可以使用如下的式子学习样本值：</p>

$${\theta _j} = {\theta _j} + \alpha \sum\limits _{i = 1}^m {(x _j^i(y - {h _\theta }({x^i})))} $$


<h2 id="3-代码"><a href="#3-代码" class="headerlink" title="3 代码"></a>3 代码</h2><p>从这一章开始讲代码从matlab转化更为通用的python。</p>
<p>我们选取合适的步长，当\(J({\theta})\)小于合适的阈值(这里配置为4)，就认为迭代完成。分类代码如下:</p>
<pre><code>import random
import math
import numpy as np
from matplotlib import pyplot as plt
k = 2
b = 1
x1_res = [-0.711, -0.2798, 0.4258, 0.4108, 0.0492, -0.842, -0.6036, -0.8394, 0.395, -0.8612, 0.806, 0.1476, 0.5464,
          0.3992,
          0.8326, 0.76, -0.7288, 0.0718, -0.4436, 0.8448, -0.017, -0.108, -0.4032, -0.9678, -0.4322, -0.9024, -0.0146,
          -0.5346, -0.3808, 0.0498, 0.6376, -0.3946, 0.5896, -0.9504, 0.2046, -0.0994, 0.468, -0.9236, 0.0996, 0.4366,
          -0.167, 0.9618, 0.5796, -0.383, 0.0254, -0.367, 0.0734, -0.8434, -0.9948, -0.0188]
x2_res = [-0.5108, -0.1254, 0.6166, 0.5296, 0.5758, -0.5972, 0.3994, 0.9808, 0.7134, -0.9414, -0.098, -0.2778, 0.667,
          0.1916, -0.0298, -0.5124, -0.509, -0.9754, 0.261, 0.4256, -0.7104, 0.8398, 0.7094, -0.5558, -0.799, 0.666,
          0.392,
          0.6688, 0.7832, 0.6372, -0.788, -0.1138, 0.0922, -0.6254, -0.1456, 0.3608, 0.4694, -0.8946, 0.9708, 0.8736,
          -0.5254, 0.0252, 0.6554, -0.2486, 0.9064, -0.35, -0.7724, 0.131, 0.4446, 0.2468]
          
def make_samples(x0_arr,x1_arr,x2_arr,y_arr):
  # classified by x_2 = 2*x_1 + 1 . (x_1,x_2) is the point of 2-D plane
  # y is 0 or 1, means &#39;x_2 &gt;= 2*x_1 + 1 &#39;  or &#39;x_2 &lt; 2*x_1 + 1&#39;
  for i in range(len(x1_res)):
    x0_arr.append(1)
    x1_arr.append(x1_res[i])
    x2_arr.append(x2_res[i])
    y_arr.append(0 if (x2_arr[i] &gt;= k * x1_arr[i] + b ) else 1)

def h(z):
  return 1/(1 + math.exp(-1*z))

def h_xita(x0,x1,x2,xita):
  return h(transvection([x0,x1,x2], xita))

def transvection(a,b):
  return np.dot(a, b)

def J(x0_arr,x1_arr,x2_arr,y_arr,xita):
  sum = 0;
  length = len(x1_arr)
  for i in range(length):
    sum = sum + ( h_xita(x0_arr[i],x1_arr[i],x2_arr[i],xita) - y_arr[0] )**2
  return sum/2

def updateXita(x0_arr,x1_arr,x2_arr,y_arr,xita,a,update_x):
  sum = 0;
  length = len(x0_arr)
  for i in range(length):
    sum = sum + ( y_arr[i] - h_xita(x0_arr[i],x1_arr[i],x2_arr[i],xita) ) * update_x[i]
  return sum*a

def showPic(k,b,point_x1_arr,point_x2_arr):
  ## show line
  X = np.arange(-1,1,0.001)
  Y=[]
  for i in range(X.size):
    Y.append(k*X[i]+b)
  plt.plot(X, Y, &#39;b--&#39;, label=&quot;logistic&quot;)
  ## show points
  for i in range(len(point_x1_arr)):
    if(point_x2_arr[i]&gt;2*point_x1_arr[i]+1):
      plt.plot(point_x1_arr[i],point_x2_arr[i],&#39;b--&#39;,marker = &#39;x&#39;, color = &#39;g&#39;)
    else:
      plt.plot(point_x1_arr[i],point_x2_arr[i],&#39;r--&#39;,marker = &#39;+&#39;, color = &#39;r&#39;)
  plt.xlabel(&quot;x1&quot;)
  plt.ylabel(&quot;x2&quot;)
  plt.figure(figsize=(8, 4))
  plt.show()

if __name__== &quot;__main__&quot;:
  x0_arr = []
  x1_arr = []
  x2_arr = []
  y_arr = []
  make_samples(x0_arr,x1_arr,x2_arr,y_arr)
  xita = [1,20,-1]
  a = 0.01
  limit = 4
  count = 0
  while 1:
    j = J(x0_arr, x1_arr, x2_arr, y_arr, xita)
    print(&quot;j=&quot;,j)
    if j &lt; limit:
      break;
    xita[0] = xita[0] + updateXita(x0_arr, x1_arr, x2_arr, y_arr, xita, a, x0_arr)
    xita[1] = xita[1] + updateXita(x0_arr, x1_arr, x2_arr, y_arr, xita, a, x1_arr)
    count = count + 1
    print(&quot;cout=&quot;,count)
    print(&quot;xita0=&quot;,xita[0],&quot; xita1=&quot;,xita[1])
  showPic(xita[1],xita[0],x1_arr,x2_arr)
  print(xita)
</code></pre>
<p>经过数论迭代分析，得到\( \theta _0 &#x3D; 2.7841420917875297 \)，\( \theta _1 &#x3D; 4.850051144306185 \)。</p>
<p>得到如下结果:</p>
<img src="/images/机器学习/监督学习-分类问题计算结果1.png" width=50% height=50% text-align=center/>

<h2 id="4-代码的改进"><a href="#4-代码的改进" class="headerlink" title="4 代码的改进"></a>4 代码的改进</h2><p>经过试验发现上面的算法的问题是如果选取合适的阈值。这里将算法修改为两次的\(J({\theta})\)的差值小于0.00001，这样就任务函数已经几近收敛,然后结束迭代。另外，如果\(J({\theta})\)的值某一次迭代增大了,说明算法异常或者迭代参数设置过大。修改后的代码如下:</p>
<pre><code>  while 1:                                                                             
    j = J(x0_arr, x1_arr, x2_arr, y_arr, xita)                                         
    print(&quot;j=&quot;,j)                                                                      
    if j &gt; j_last:                                                                     
      print(&quot;算法异常或选取迭代系数过大&quot;)                                                           
      break;                                                                           
    if j_last - j &lt; 0.00001:                                                           
      print(&quot;out &quot;,j_last - j)                                                         
      break;                                                                           
    j_last = j                                                                         
    xita[0] = xita[0] + updateXita(x0_arr, x1_arr, x2_arr, y_arr, xita, a, x0_arr)     
    xita[1] = xita[1] + updateXita(x0_arr, x1_arr, x2_arr, y_arr, xita, a, x1_arr)    
</code></pre>
<p>得到如下结果:</p>
<img src="/images/机器学习/监督学习-分类问题计算结果2.png" width=50% height=50% text-align=center/>

<h2 id="5-牛顿法改进"><a href="#5-牛顿法改进" class="headerlink" title="5 牛顿法改进"></a>5 牛顿法改进</h2><p>这里说明另外一种快速求值的方法，牛顿法。<br><img src="/images/机器学习/牛顿法示意图.png" width=50% height=50% text-align=center/></p>
<p>如上所示为牛顿法的简易过程。我们初始值为\(x _0\)，然后在\((x _0,f(x _0))\)做切线得到\(x _0\)，依次类推会接近于得到\(x _n\),使得\(f(x _n)\)接近于0。具体算法的证明请见文献2。</p>
<p>根据上面的算法，我们可以轻易的推导出</p>

$${x _{i + 1}} = {x _i} - \frac{{f({x _i})}}{{f'({x _i})}}$$


<p>联系之前的问题，我们可以转化快速得到\({\theta _j}\)使得\(\ell ({\theta _j}) &#x3D; 0\)问题。（注: 之前的算法使找到最优下降方向使得\(J({\theta})\)最小，而牛顿法是直接得到最优的\({\theta _j}\)）</p>
<p>本问题的公式如下:</p>

$$\ell ({\theta _j}) = \sum\limits _{i = 1}^m {(x _j^i(y - {h _\theta }({x^i})))} $$



$$\ell '({\theta _j}) =  - \sum\limits _{i = 1}^m {{{(x _j^i)}^2}} $$



$${\theta _j}: = {\theta _j} - \frac{{\ell ({\theta _j})}}{{\ell '({\theta _j})}} = {\theta _j} - \frac{{\sum\limits _{i = 1}^m {(x _j^i(y - {h _\theta }({x^i})))} }}{{ - \sum\limits _{i = 1}^m {{{(x _j^i)}^2}} }}$$


<p>代码如下:</p>
<pre><code>import sys
import math
import numpy as np
from matplotlib import pyplot as plt
k = 2
b = 1
x1_res = [-0.711, -0.2798, 0.4258, 0.4108, 0.0492, -0.842, -0.6036, -0.8394, 0.395, -0.8612, 0.806, 0.1476, 0.5464,
          0.3992,
          0.8326, 0.76, -0.7288, 0.0718, -0.4436, 0.8448, -0.017, -0.108, -0.4032, -0.9678, -0.4322, -0.9024, -0.0146,
          -0.5346, -0.3808, 0.0498, 0.6376, -0.3946, 0.5896, -0.9504, 0.2046, -0.0994, 0.468, -0.9236, 0.0996, 0.4366,
          -0.167, 0.9618, 0.5796, -0.383, 0.0254, -0.367, 0.0734, -0.8434, -0.9948, -0.0188]
x2_res = [-0.5108, -0.1254, 0.6166, 0.5296, 0.5758, -0.5972, 0.3994, 0.9808, 0.7134, -0.9414, -0.098, -0.2778, 0.667,
          0.1916, -0.0298, -0.5124, -0.509, -0.9754, 0.261, 0.4256, -0.7104, 0.8398, 0.7094, -0.5558, -0.799, 0.666,
          0.392,
          0.6688, 0.7832, 0.6372, -0.788, -0.1138, 0.0922, -0.6254, -0.1456, 0.3608, 0.4694, -0.8946, 0.9708, 0.8736,
          -0.5254, 0.0252, 0.6554, -0.2486, 0.9064, -0.35, -0.7724, 0.131, 0.4446, 0.2468]

def make_samples(x0_arr,x1_arr,x2_arr,y_arr):
  # classified by x_2 = 2*x_1 + 1 . (x_1,x_2) is the point of 2-D plane
  # y is 0 or 1, means &#39;x_2 &gt;= 2*x_1 + 1 &#39;  or &#39;x_2 &lt; 2*x_1 + 1&#39;
  for i in range(len(x1_res)):
    x0_arr.append(1)
    x1_arr.append(x1_res[i])
    x2_arr.append(x2_res[i])
    y_arr.append(0 if (x2_arr[i] &gt;= k * x1_arr[i] + b ) else 1)

def h(z):
  return 1/(1 + math.exp(-1*z))

def h_xita(x0,x1,x2,xita):
  return h(transvection([x0,x1,x2], xita))

def transvection(a,b):
  return np.dot(a, b)

def l(x0_arr,x1_arr,x2_arr,y_arr,xita,a,update_x):
  sum = 0;
  for i in range(len(x0_arr)):
    sum = sum + ( y_arr[i] - h_xita(x0_arr[i],x1_arr[i],x2_arr[i],xita) ) * update_x[i]
  return sum*a

def derl(update_x):
  sum = 0;
  for i in range(len(update_x)):
    sum = sum + update_x[i]*update_x[i];
  return -1 * sum

def showPic(k,b,point_x1_arr,point_x2_arr):
  ## show line
  X = np.arange(-1,1,0.001)
  Y=[]
  for i in range(X.size):
    Y.append(k*X[i]+b)
  plt.plot(X, Y, &#39;b--&#39;, label=&quot;logistic&quot;)
  ## show points
  for i in range(len(point_x1_arr)):
    if(point_x2_arr[i]&gt;2*point_x1_arr[i]+1):
      plt.plot(point_x1_arr[i],point_x2_arr[i],&#39;b--&#39;,marker = &#39;x&#39;, color = &#39;g&#39;)
    else:
      plt.plot(point_x1_arr[i],point_x2_arr[i],&#39;r--&#39;,marker = &#39;+&#39;, color = &#39;r&#39;)
  plt.xlabel(&quot;x1&quot;)
  plt.ylabel(&quot;x2&quot;)
  plt.figure(figsize=(8, 4))
  plt.show()

if __name__== &quot;__main__&quot;:
  x0_arr = []
  x1_arr = []
  x2_arr = []
  y_arr = []
  make_samples(x0_arr,x1_arr,x2_arr,y_arr)
  xita = [1,20,-1]
  xita_bak = [10000,10000]
  count = 0
  while 1:
    xita[0] = xita[0] - l(x0_arr, x1_arr, x2_arr, y_arr, xita, 1,x0_arr)/derl(x0_arr)
    xita[1] = xita[1] - l(x0_arr, x1_arr, x2_arr, y_arr, xita, 1, x1_arr) / derl(x1_arr)
    if abs(l(x0_arr, x1_arr, x2_arr, y_arr, xita, 1,x0_arr)) &lt;= 0.001 :
      print(&quot;xita = &quot;,xita)
      break;
    xita_bak[1]=xita[1]
    xita_bak[0]=xita[0]
    count = count + 1
    print(&quot;count=&quot;, count, &quot; xita = &quot;, xita)

  showPic(xita[1],xita[0],x1_arr,x2_arr)
  print(xita)
</code></pre>
<p>得到\({\theta _0}\)和\({\theta _1}\)分别为<br>2.7762287194293407, 4.835356147838368。得到的曲线如下：</p>
<img src="/images/机器学习/监督学习-分类问题计算结果3.png" width=50% height=50% text-align=center/>

<h2 id="6-参考文献"><a href="#6-参考文献" class="headerlink" title="6 参考文献"></a>6 参考文献</h2><ul>
<li>1 &lt;&lt; cs229-notes1 &gt;&gt;</li>
<li>2 &lt;&lt; 计算机科学计算 &gt;&gt; 施吉林,张宏伟，金光日编</li>
</ul>
<blockquote>
<p>未来代码将会维护在<a target="_blank" rel="noopener" href="https://github.com/zhengchenyu/mlearning">https://github.com/zhengchenyu/mlearning</a></p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2017/06/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2.2-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%88%86%E7%B1%BB/" data-id="cm6uj1jcm000c7rrmcimk1e0d" data-title="机器学习 2.2 监督学习之逻辑回归" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-机器学习-2.1-监督学习之线性回归" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2017/06/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2.1-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" class="article-date">
  <time class="dt-published" datetime="2017-06-10T13:52:17.000Z" itemprop="datePublished">2017-06-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2017/06/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2.1-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">机器学习 2.1 监督学习之线性回归</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h1><p><img src="http://ord3iii9m.bkt.clouddn.com/image/github/mlearning%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E8%AF%B4%E6%98%8E.png" alt="监督学习说明"> </p>
<p>如上图所示，监督学习实质就是: 在给定训练集合，使用某种学习算法得到学习函数h，能够较为准确的预测y。</p>
<h2 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1 线性回归"></a>1 线性回归</h2><p>下面有一组房价关于房子面积以及卧室数目的样本数据。试着从这些样本数据中构造线性模型，以预测房价。</p>
<table>
<thead>
<tr>
<th>房间数目</th>
<th>房间大小</th>
<th>房价</th>
</tr>
</thead>
<tbody><tr>
<td>2</td>
<td>88.0</td>
<td>1760288</td>
</tr>
<tr>
<td>2</td>
<td>88.0</td>
<td>1762136</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
</tbody></table>
<h3 id="1-1-梯度下降法"><a href="#1-1-梯度下降法" class="headerlink" title="1.1 梯度下降法"></a>1.1 梯度下降法</h3><h4 id="1-1-1-梯度下降法说明"><a href="#1-1-1-梯度下降法说明" class="headerlink" title="1.1.1 梯度下降法说明"></a>1.1.1 梯度下降法说明</h4><p>假设房价与房间数目、房屋带下呈线性关系，有如下关系:</p>

$${h_\theta }(x) = {\theta _0} + {\theta _1}{x _1} + {\theta _2}{x _2}$$


<p>假设\({x_0} &#x3D; 1\),则有如下公式:</p>

$${h_\theta }(x) = {\theta _0}{x _0} + {\theta _1}{x _1} + {\theta _2}{x _2} = \sum\limits _{i = 0}^n{\theta _i}{x _i}$$


<p>假设y为样本实际的值，因此有如下损失函数可以定义：</p>

$$J(\theta ) = \frac{1}{2}\sum\limits _{i = 0}^m {{{({h _\theta }({x^i}) - {y^i})}^2}} $$


<p>我们只要保证上面的公式趋于0就是合适的，因此这个问题就转化为得到合适的使得\(J(\theta )\)最小。实际也就转化为利用最小二乘法进行回归分析。</p>
<p>做这样一个想想，对于二维的情况。\(J(\theta )\)是一个关于的二维函数的话，他一定类似于锅状的凸函数。根据梯度下降算法(参考&lt;&lt;最优化方法&gt;&gt;)，沿着某一个方向的负梯度方向就是在该方向上下降最大的方向。因此，我们可以选取各个维度的负梯度作为下降的方向，为了方便各个梯度方向选取同样的步长。因此，我们对任意维度采用如下公式进行递归(为步长)：</p>

$${\theta _j}: = {\theta _j} - \alpha \frac{{\partial J(\theta )}}{{\partial {\theta _j}}} = {\theta _j} - \alpha \sum\limits _{i = 0}^m{({h _\theta }({x^i}) - {y^i}) {x _j}}$$


<p>我们简化，我们仅仅对房间大小与房价的一维关系进行回归分析。</p>
<blockquote>
<p>注：对于每一次迭代运算都需要对所有全量信息进行计算。这样带来了大量的计算。工程上可以采用采样的方式计算新的，这里数据较少，就不用该方法了。</p>
</blockquote>
<h4 id="1-1-2-梯度下降法案例"><a href="#1-1-2-梯度下降法案例" class="headerlink" title="1.1.2 梯度下降法案例"></a>1.1.2 梯度下降法案例</h4><p>我们有一组数据，为房间大小与房价的关系。我们的目标是通过使用梯度下降算法，模拟得到满足学习函数，以预测房价。可以看如下点图:</p>
<p><img src="http://ord3iii9m.bkt.clouddn.com/image/github/mlearning%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%88%BF%E4%BB%B7%E7%82%B9%E5%9B%BE.png" alt="房价点图"></p>
<p>根据上一节的公式编写如下代码进行回归分析:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">% 以下为主算法流程</span><br><span class="line">% y = xita0 +xita1*x</span><br><span class="line">mfilename=&#x27;/Users/zcy/Desktop/mlearning/prices&#x27;</span><br><span class="line">[v,x,y]=textread(mfilename,&#x27;%n%n%n&#x27;);</span><br><span class="line"> </span><br><span class="line">limit = 1000;</span><br><span class="line">y=y./10000;</span><br><span class="line">% step</span><br><span class="line">a = 0.0000001;</span><br><span class="line"> </span><br><span class="line">xita0 = 0.01;</span><br><span class="line">xita1 = 1.6;</span><br><span class="line">err = JFun(x,y,xita0,xita1);</span><br><span class="line">disp([&#x27;err = &#x27;,num2str(err)])</span><br><span class="line">count = 0; </span><br><span class="line">while err&gt;limit </span><br><span class="line">    xita0 = xita0 - a*gradient(x,y,xita0,xita1,true);</span><br><span class="line">    xita1 = xita1 - a*gradient(x,y,xita0,xita1,false)    </span><br><span class="line">    err = JFun(x,y,xita0,xita1);</span><br><span class="line">    count = count + 1;</span><br><span class="line">    disp([&#x27;err = &#x27;,num2str(err),&#x27;, count = &#x27;,num2str(count)])</span><br><span class="line">end</span><br><span class="line"> </span><br><span class="line">scatter(x,y,&#x27;k&#x27;)</span><br><span class="line">hold on</span><br><span class="line">x1 = 70:0.01:100;</span><br><span class="line">y1 = xita0+x1.*xita1;</span><br><span class="line">plot(x1,y1)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>JFun.m文件流程:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">function [s]  = JFun(x,y,xita0,xita1)</span><br><span class="line">s = 0;</span><br><span class="line">for i = 1:size(x)</span><br><span class="line">   s = s + 0.5*((xita0*1+xita1*x(i)-y(i))^2);</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<p>gradient.m文件: </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">function [s]  = gradient(x,y,xita0,xita1,b)</span><br><span class="line">s = 0;</span><br><span class="line">if b </span><br><span class="line">    for i = 1:size(x)</span><br><span class="line">        s = s + (xita0*1+xita1*x(i)-y(i))*1;</span><br><span class="line">    end</span><br><span class="line">else </span><br><span class="line">    for i = 1:size(x)</span><br><span class="line">        s = s + (xita0*1+xita1*x(i)-y(i))*x(i);</span><br><span class="line">    end</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<blockquote>
<p>这里为了示意，代码仅考虑两维，实际不建议代码这样写。另外，选取合适的limit值和步长需要经过打印调试。选取不当，容易造成函数无法收敛。</p>
</blockquote>
<p>回归得到如下曲线：</p>
<p><img src="http://ord3iii9m.bkt.clouddn.com/image/github/mlearning%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%88%BF%E4%BB%B7%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BB%93%E6%9E%9C1.png" alt="回归曲线1"></p>
<p>修改limit为500之后，回归更加准确。</p>
<p><img src="http://ord3iii9m.bkt.clouddn.com/image/github/mlearning%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%88%BF%E4%BB%B7%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BB%93%E6%9E%9C2.png" alt="回归曲线2"></p>
<h3 id="1-2-线性回归的解析解"><a href="#1-2-线性回归的解析解" class="headerlink" title="1.2 线性回归的解析解"></a>1.2 线性回归的解析解</h3><p>略，详见斯坦福大学机器学习讲义。</p>
<h3 id="1-3-线性回归的概率解释"><a href="#1-3-线性回归的概率解释" class="headerlink" title="1.3 线性回归的概率解释"></a>1.3 线性回归的概率解释</h3><p>本节从概率的角度进行线性回归分析。对我们的任意一组变量有如下公式：</p>

$${y^i} = {\theta ^T}{x^i} + {\varepsilon ^i}$$


<p>其中\(\varepsilon \)为误差，假设我们的误差服从正太分布<br>\({\varepsilon ^i} \sim {\rm N}(0,{\sigma ^2})\)。即有如下公式：</p>

$$p({\varepsilon ^i}) = \frac{1}{{\sqrt {2\pi } \sigma }}{e^{( - \frac{{{{({\varepsilon ^i})}^2}}}{{2{\sigma ^2}}})}}$$


<p>代入上面的公式有：</p>

$$p({y^i}|{x^i},\theta ) = \frac{1}{{\sqrt {2\pi } \sigma }}{e^{( - \frac{{{{({y^i} - {\theta ^T}{x^i})}^2}}}{{2{\sigma ^2}}})}}$$


<p>该公式的含义为在已知\(x ^i \)和\(\theta \)的情况下，得到准确的\(y ^i \)概率。我们可以利用最大似然估计，最大释然估计实质就是对所有采样值调整参数得到最准确y值的方法。(详细请看概率论)。有如下公式:</p>

$$L(\theta ) = \prod\limits _{i = 1}^m {p({y^i}|{x^i},\theta )}  = \prod\limits _{i = 1}^m {\frac{1}{{\sqrt {2\pi } \sigma }}{e^{( - \frac{{{{({y^i} - {\theta ^T}{x^i})}^2}}}{{2{\sigma ^2}}})}}} $$


<p>上面的公式是对各个数据采样得到准确y的概率的乘积，问题也就转化为调整使的上式最大，进一步转化为求的上式的倒数为0的情况。对上式去log，如下:</p>

$$[\log (L(\theta )) = \log (\prod\limits _{i = 1}^m {\frac{1}{{\sqrt {2\pi } \sigma }}{e^{( - \frac{{{{({y^i} - {\theta ^T}{x^i})}^2}}}{{2{\sigma ^2}}})}}} )$$



$$\log (L(\theta )) = \sum\limits _{i = 1}^m {\log \frac{1}{{\sqrt {2\pi } \sigma }}{e^{( - \frac{{{{({y^i} - {\theta ^T}{x^i})}^2}}}{{2{\sigma ^2}}})}}}  = m\log \frac{1}{{\sqrt {2\pi } \sigma }} - \frac{1}{{{\sigma ^2}}} \cdot \frac{1}{2}\sum\limits _{i = 1}^m {{{({y^i} - {\theta ^T}{x^i})}^2}} $$


<p>看上式最后一项，前面部分为常数，因此问题有转化为求最小值的问题了。</p>
<p><strong>因此从概率角度上考虑误差的情况下，我们前面基于最小二乘的算法也是正确的。</strong></p>
<h3 id="1-4-局部加权回归-Loess"><a href="#1-4-局部加权回归-Loess" class="headerlink" title="1.4 局部加权回归(Loess)"></a>1.4 局部加权回归(Loess)</h3><h4 id="1-4-1-局部加权回归说明"><a href="#1-4-1-局部加权回归说明" class="headerlink" title="1.4.1 局部加权回归说明"></a>1.4.1 局部加权回归说明</h4><p>有一组非线性去先，譬如\(y &#x3D; {x^2}\)。如果使用之前的方法拟合(即假设其为y&#x3D;kx+b型曲线)，必然不会得到理想的结果。对于这样的问题，我们想求出\(f(20)\),加入我们只对\(x\)在20附近的样本进行一维拟合，我们就可以得到一个精确值。因此，我们引入如下公式：</p>

$$J(\theta ) = \frac{1}{2}\sum\limits_ {i = 0}^m {{\omega ^i}{{({h_ \theta }({x^i}) - {y^i})}^2}} $$



$${\omega ^i} = {e^{ - \frac{{{{({x^i} - x)}^2}}}{{2{\tau ^2}}}}}$$


<p>其中，在原来的价值函数中引入权重\(\omega\)。\(\tau\)是波长，该值越小，越趋于向20附近的值进行回归计算。对于求\(f(20)\)的情况，在的公式中\(x\)恒为20,可以知道该值在\(x ^i\)趋近于20的时候趋近于1，趋近于\( \pm \infty \)的时候趋近于0。也就是相当于仅仅是在20附近进行拟合。由于新引入的权重函数与\(\theta\)无关。所以得到如下公式:</p>

$${\theta _j}: = {\theta _j} - \alpha \frac{{\partial J(\theta )}}{{\partial {\theta _j}}} = {\theta _j} - \alpha \sum\limits _{i = 0}^m {{\omega ^i}({h _\theta }({x^i}) - {y^i})} {x _j}$$


<p>我们这里是一个一维的问题，可以得到如下公式:</p>

$$J(\theta ) = \frac{1}{2}\sum\limits _{i = 0}^m {{\omega ^i}{{({\theta _1}{x^i} + {\theta _0} - {y^i})}^2}} $$



$${\omega ^i} = {e^{ - \frac{{{{({x^i} - 20)}^2}}}{{2{\tau ^2}}}}}$$


<h4 id="1-4-2-局部加权回归案例"><a href="#1-4-2-局部加权回归案例" class="headerlink" title="1.4.2 局部加权回归案例"></a>1.4.2 局部加权回归案例</h4><p>主流程代码:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">% y = xita0 +xita1*x</span><br><span class="line">x = -100:1:100;</span><br><span class="line">y = x.*x;</span><br><span class="line">% x= 20, y = 40x - 400</span><br><span class="line">x0 = 20;       </span><br><span class="line">tao = 5;</span><br><span class="line">a = 0.0003;</span><br><span class="line"> </span><br><span class="line">xita0 = -1;</span><br><span class="line">xita1 = 1;</span><br><span class="line">limit = 7850;</span><br><span class="line">err = JFun_Loess(x,y,xita0,xita1,x0,tao);</span><br><span class="line">count = 0; </span><br><span class="line">while err&gt;limit </span><br><span class="line">    xita0 = xita0 - a*gradient_Loess(x,y,xita0,xita1,true,x0,tao);</span><br><span class="line">    xita1 = xita1 - a*gradient_Loess(x,y,xita0,xita1,false,x0,tao);    </span><br><span class="line">    err = JFun_Loess(x,y,xita0,xita1,x0,tao);</span><br><span class="line">    count = count + 1;</span><br><span class="line">end</span><br><span class="line"> </span><br><span class="line">scatter(x,y,&#x27;k&#x27;)</span><br><span class="line">hold on</span><br><span class="line">x1 = -100:1:100;</span><br><span class="line">y1 = xita0+x1.*xita1;</span><br><span class="line">disp([&#x27;xita0 = &#x27;,num2str(xita0),&#x27;, xita1 = &#x27;,num2str(xita1)])</span><br><span class="line">plot(x1,y1)</span><br></pre></td></tr></table></figure>

<p>JFun_loess:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">function [s]  = JFun_Loess(x,y,xita0,xita1,x0,tao)</span><br><span class="line">s = 0;</span><br><span class="line">for i = 1:size(x&#x27;)</span><br><span class="line">    s = s + 0.5*((xita0*1+xita1*x(i)-y(i))^2)*exp(-0.5*(x(i)-x0)*(x(i)-x0)/tao/tao);</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<p>gradient_Loess:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">function [s]  = gradient_Loess(x,y,xita0,xita1,b,x0,tao)</span><br><span class="line">s = 0;</span><br><span class="line">if b </span><br><span class="line">    for i = 1:size(x&#x27;)</span><br><span class="line">        s = s + (xita0*1+xita1*x(i)-y(i))*1*exp(-0.5*(x(i)-x0)*(x(i)-x0)/tao/tao);</span><br><span class="line">    end</span><br><span class="line">else</span><br><span class="line">    for i = 1:size(x&#x27;)</span><br><span class="line">        s = s + (xita0*1+xita1*x(i)-y(i))*x(i)*exp(-0.5*(x(i)-x0)*(x(i)-x0)/tao/tao);</span><br><span class="line">    end</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

<p>在x&#x3D;20处分析，理论值为y &#x3D; 40x – 400。经过模拟得到的值为y&#x3D;39.6825x-368.253。<br>下图为得到的结果，实际上已经非常接近这个二次曲线在20这个位置的切线了。</p>
<p><img src="http://ord3iii9m.bkt.clouddn.com/image/github/mlearning%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%B1%80%E9%83%A8%E5%8A%A0%E6%9D%83%E5%88%86%E6%9E%90%E7%BB%93%E6%9E%9C%E5%9B%BE.png" alt="局部加权分析结果图"></p>
<h2 id="2-回归分析相关概念"><a href="#2-回归分析相关概念" class="headerlink" title="2 回归分析相关概念"></a>2 回归分析相关概念</h2><p>另外引申一个概念:</p>
<p>之前我们把房价与房屋大小认为是一个一维曲线。如上图，可以看出很多点被反映到曲线中。我们可以称其为欠拟合过程。与之对应的，如果使用高阶函数进行拟合，即假如样本有5个点，我们可以通过一个4阶函数的曲线进行完整拟合，但是这样的曲线往往并不是一个良好的房价与房屋大小的反映，这被称为过拟合。(该部分内容可以想见矩阵与数值分析)</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2017/06/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2.1-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" data-id="cm6uj1jcl00097rrma8ls627t" data-title="机器学习 2.1 监督学习之线性回归" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-机器学习-1-绪论" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2017/06/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1-%E7%BB%AA%E8%AE%BA/" class="article-date">
  <time class="dt-published" datetime="2017-06-10T13:43:10.000Z" itemprop="datePublished">2017-06-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2017/06/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1-%E7%BB%AA%E8%AE%BA/">机器学习 1 绪论</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="机器学习的几种方法"><a href="#机器学习的几种方法" class="headerlink" title="机器学习的几种方法"></a>机器学习的几种方法</h2><p> </p>
<ul>
<li><p>监督学习</p>
<p>根据训练资料中建立一个模式，并依此模式推测新的实例。一般来讲监督学习的任务一般是分类和回归分析，往往需要人工预先准备好范例(训练资料)。</p>
</li>
<li><p>   非监督学习</p>
<p> 不同于监督学习，非监督学习仅对网络提供输入范例，会从这些范例中找到潜在类别规则，主要目的是对原始资料进行分类。</p>
</li>
<li><p>强化学习</p>
<p>强化学习强调如何基于环境而行动，以取得最大的预期利益。其灵感来源于心理学中的行为主义理论，即有机体如何在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。</p>
</li>
</ul>
<p> </p>
<h2 id="一些规范"><a href="#一些规范" class="headerlink" title="一些规范"></a>一些规范</h2><p> <br>  n表示样本的维度，下标。m 表示样本数目，上标。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2017/06/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1-%E7%BB%AA%E8%AE%BA/" data-id="cm6uj1jcl00057rrmhr29esqj" data-title="机器学习 1 绪论" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-MyFirstBlog" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2017/05/06/MyFirstBlog/" class="article-date">
  <time class="dt-published" datetime="2017-05-06T03:04:07.000Z" itemprop="datePublished">2017-05-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2017/05/06/MyFirstBlog/">MyFirstBlog</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2017/05/06/MyFirstBlog/" data-id="cm6uj1jck00027rrmajny1q56" data-title="MyFirstBlog" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/02/">February 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/02/07/2024-04-22/">(no title)</a>
          </li>
        
          <li>
            <a href="/2025/02/07/ErasuceCode%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/12/25/RSS-%E8%BF%9C%E7%A8%8BMerge%E7%9A%84%E8%AE%BE%E8%AE%A1/">RSS-远程Merge的设计</a>
          </li>
        
          <li>
            <a href="/2023/12/25/RSS-%E8%BF%9C%E7%A8%8BMerge%E7%9A%84%E8%AE%BE%E8%AE%A1EN/">RSS-Remote Merge Design</a>
          </li>
        
          <li>
            <a href="/2017/10/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-3-2-%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/">机器学习-3.2-非监督学习之主成分分析.md</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 zhengchenyu<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>