<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>机器学习-2.7-监督学习之支持向量机 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="支持向量机(SVM)1 Margins(间隔)对于一个逻辑回归问题，\(p(y &#x3D; 1|x, \theta )\)可以通过下面的式子表示：  $${h _ \theta }(x) &#x3D; g({\theta ^T}x)$$   \(\theta^Tx\)越大， \(g({\theta ^T}x)\)就越大， 我们就有可高的信心认为\(y&#x3D;1\)。 同理假如\({ h _\the">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习-2.7-监督学习之支持向量机">
<meta property="og:url" content="http://example.com/2017/09/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-7-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="支持向量机(SVM)1 Margins(间隔)对于一个逻辑回归问题，\(p(y &#x3D; 1|x, \theta )\)可以通过下面的式子表示：  $${h _ \theta }(x) &#x3D; g({\theta ^T}x)$$   \(\theta^Tx\)越大， \(g({\theta ^T}x)\)就越大， 我们就有可高的信心认为\(y&#x3D;1\)。 同理假如\({ h _\the">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%9B%BE1.png">
<meta property="og:image" content="http://example.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%9B%BE2.png">
<meta property="og:image" content="http://example.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%9B%BE3.png">
<meta property="og:image" content="http://example.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%9B%BE4.png">
<meta property="og:image" content="http://example.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E8%AE%A1%E7%AE%97%E7%BB%93%E6%9E%9C1.png">
<meta property="og:image" content="http://example.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E8%AE%A1%E7%AE%97%E7%BB%93%E6%9E%9C2.png">
<meta property="article:published_time" content="2017-09-08T08:18:25.000Z">
<meta property="article:modified_time" content="2025-02-07T08:06:18.199Z">
<meta property="article:author" content="zhengchenyu">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%9B%BE1.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<meta name="generator" content="Hexo 8.1.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-机器学习-2-7-监督学习之支持向量机" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2017/09/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-7-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" class="article-date">
  <time class="dt-published" datetime="2017-09-08T08:18:25.000Z" itemprop="datePublished">2017-09-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      机器学习-2.7-监督学习之支持向量机
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="支持向量机-SVM"><a href="#支持向量机-SVM" class="headerlink" title="支持向量机(SVM)"></a>支持向量机(SVM)</h1><h2 id="1-Margins-间隔"><a href="#1-Margins-间隔" class="headerlink" title="1 Margins(间隔)"></a>1 Margins(间隔)</h2><p>对于一个逻辑回归问题，\(p(y &#x3D; 1|x, \theta )\)可以通过下面的式子表示：</p>

$${h _ \theta }(x) = g({\theta ^T}x)$$


<p>\(\theta^Tx\)越大， \(g({\theta ^T}x)\)就越大， 我们就有可高的信心认为\(y&#x3D;1\)。 同理假如\({ h _\theta }(x) &lt;&lt; 0\), 我们就有很强的信息认为\(y&#x3D;0\)。 (注: &gt;&gt; 表示远大于，&lt;&lt;表示远小于)</p>
<img src="/images/机器学习/监督学习-支持向量机图1.png" width=50% height=50% text-align=center/>

<p>上面这张图，x为正样本，o为负样本。中间的直线是分割超平面。上面图中，我们可以知道对于样本A，我们有很强的信心认为\(y&#x3D;1\)。然后对于C却接近边界，如果稍微改变分割超平面，就可能导致\(y&#x3D;0\)。因此，越远离分割超平面的样本，我们越有信息获得他的分类。</p>
<h2 id="2-函数化与几何间隔"><a href="#2-函数化与几何间隔" class="headerlink" title="2 函数化与几何间隔"></a>2 函数化与几何间隔</h2><p>区别与之前的逻辑回归。引入支持向量机，我们需要重新定义符号。对于SVM问题, \(y \in \{  - 1,1\} \),定义函数为:</p>
$${h_{w,b}}(x) = g({w^T}x + b)$$

<p>对于\(z &#x3D; {w^T}x + b &gt; 0\), 有\({h _ \theta }(x) &#x3D; 1\)。 否则\({h _ \theta }(x) &#x3D; -1\)。</p>
<p>函数化几何间隔，有如下公式:</p>

$${\hat \gamma ^{(i)}} = {y^{(i)}}({w^T}x + b)$$
	

<p>上式很容易理解，对于正样本\(y&#x3D;1\),对于负样本\(y&#x3D;-1\)。因此可以得到\({\hat \gamma ^{(i)}}\)为一个正值。我们可以就可以定义其为几何间隔，可以理解为距离。根据上一节的分析，一个超平面对样本分割的好坏，取决于距离超平面最近的样本。因为我们定义最小间隔如下：</p>

$$\hat \gamma  = {\min _{i = 1,...,m}}{\hat \gamma ^{(i)}}$$

<img src="/images/机器学习/监督学习-支持向量机图2.png" width=50% height=50% text-align=center/>

<p>根据上图，我们知道向量\(w\)与超平面\({w^T}x + b &#x3D; 0\)垂直。</p>
<blockquote>
<p>在超平面找任意两点做直线，易证其与法向量內积为零。因此可知法向量与平面垂直。</p>
</blockquote>
<p>我们可以得知B点的坐标为\({x^{(i)}} - {r^{(i)}}\frac{w}{||w||}\)，我们将坐标代入超平面方程，有:</p>

$${w^T}({x^{(i)}} - {r^{(i)}}\frac{w}{||w||}) + b = 0$$

<p>由于知道\(||w|| &#x3D; {w^T}w\)，有：</p>

$${r^{(i)}} = {(\frac{w}{||w||})^T}{x^{(i)}} + \frac{b}{||w||}$$

<p>上面公式对应于正样本，对于负样本则需要加一个符号。因此我们可以综合得到如下公式:</p>

$${r^{(i)}} = {y^{(i)}}({(\frac{w}{||w||})^T}{x^{(i)}} + \frac{b}{||w||})$$

<p>对于公式\({\hat \gamma ^{(i)}} &#x3D; {y^{(i)}}({w^T}x + b)\)中，\(||w|| &#x3D; 1\)的时候为几何间隔。上面的式子实际上就是定义了几何间隔。我们通过定义了向量\(w\)的尺度，这样不会因为选择w比例的不同而产生不同间隔的问题。</p>
<h2 id="3-最优间隔分类器"><a href="#3-最优间隔分类器" class="headerlink" title="3 最优间隔分类器"></a>3 最优间隔分类器</h2><blockquote>
<p>以下假定我们样本数据是严格线性可分的，否则通过间隔最大化来计算就毫无意义了。</p>
</blockquote>
<p>根据前面的分析，我们只需要找到超平面，使得间隔\(\hat \gamma \)最大即可。因此我们可以得到如下最优化问题:</p>
$$\begin{gathered}   & {\max _{w,b}}\gamma  \hfill \\\  s.t.: & {y^{(i)}}({w^T}{x^{(i)}} + b) \geqslant \gamma , & i = 1,...,m \hfill \\\   & ||w|| = 1 \hfill \\\ \end{gathered} $$

<p>由于\(||w|| - 1 &#x3D; 0\)不是凸函数，所以需要转化问题。<br>注&gt; 为什么凸函数如此重要？因为对于一个凸优化问题，认为局部极小值点必为全局极小值点。对\(f(\lambda x + (1 - \lambda )y) \leqslant \lambda f(x) + (1 - \lambda )f(y)\),其中\(0 \leqslant \lambda  \leqslant 1\),则\(f(x)\)为凸函数。可以理解为类似于\(y &#x3D; {x^2}\)的图形。然而对于\(||w|| - 1 &#x3D; 0\)，具体为\(\sqrt {w _1^2 + w _2^2 + … + w _n^2}  &#x3D; 1\)，二维的情景可以理解为一个圆，三维的话则为一个球。几何图形中，可以发现对于球或圆的上半部分正好与凸函数相反，因此不是凸函数。可以代入公式证明。<br>进一步转化问题，如下：</p>

$$\begin{gathered}   & {\max _{w,b}}\frac{\gamma }{||w||} \hfill \\\  s.t.: & {y^{(i)}}({w^T}{x^{(i)}} + b) \geqslant \gamma , & i = 1,...,m \hfill \\\\end{gathered} $$

<p>我们知道\(\gamma \)大小，取决于\(w\)和\(b\)的尺度，但是\(w\)和\(b\)的尺度的改变不会影响分配效果。因此我们固定\(\gamma \)为1。将问题转化为:</p>

$$\begin{gathered}   & {\min _{w,b}}\frac{1}{2}||w|{|^2} \hfill \\  s.t.: & {y^{(i)}}({w^T}{x^{(i)}} + b) \geqslant 1, & i = 1,...,m \hfill \\ \end{gathered}$$

<p>事实上对于这个问题，我们可以换一种几何意义的解释。如下图所示，我们需要找到\({w^T}x + b &#x3D; 1\)和\({w^T}x + b &#x3D; -1\)能够有效分割样本，并且保证是两个超平面之间间隔最大，即使\(\frac{2}{||w||}\)最大，也就意味着使\(\frac{1}{2}||w|{|^2}\)最小。同样我们可以得到上面的优化问题。</p>
<img src="/images/机器学习/监督学习-支持向量机图3.png" width=50% height=50% text-align=center/>

<p>另外，我们距离超平面最近的点，即\(\gamma  &#x3D; 1\)的点，我们称之为支持向量。</p>
<h2 id="4-拉格朗日对偶问题"><a href="#4-拉格朗日对偶问题" class="headerlink" title="4 拉格朗日对偶问题"></a>4 拉格朗日对偶问题</h2><p>在对计算上一级得到的优化问题之前，我们介绍一下拉格朗日对偶问题与KKT条件，以便于更容易解决问题。考虑一个通用的优化问题，如下：</p>
$$\begin{gathered}   & {\min _x}f(x) \hfill \\\  s.t.: & {g _i}(x) \leqslant 0, & i = 1,...,k \hfill \\\   & {h _j}(x) = 0, & j = 1,...,l \hfill \\\ \end{gathered} $$


<p>然后得到拉格朗日函数，如下：</p>
$$L(x,\alpha ,\beta ) = f(x) + \sum\limits _{i = 1}^k {{\alpha _i}{g _i}(x)}  + \sum\limits _{j = 1}^l {{\beta _i}{h _i}(x)} $$$${\alpha _i} \geqslant 0$$

<p>对于我们的原始问题如何用拉格朗日函数表达呢？我们知道上面的拉格朗日后面两项的最大值为零。因此我们就可以将原始问题转化为以\({\alpha _i}\)和\({\beta _i}\)为参数情况下，求拉格朗日函数的最大值。具体转化为如下形式:</p>

$${\theta _P}(x) = {\max _{\alpha ,\beta ;{\alpha _i} \geqslant 0}}L(x,\alpha ,\beta )$$

<p>然后我们上面的式子，关于x取极小值，就与目标问题一致了。原始为的最优解最终可以通过如下方式表述:</p>

$${p^*} = {\min _x}{\theta _P}(x) = {\min _x}{\max _{\alpha ,\beta ;{\alpha _i} \geqslant 0}}L(x,\alpha ,\beta )$$

<p>我们这里可以写出其对偶问题：</p>
$${\theta _D}(\alpha ,\beta ) = {\min _x}L(x,\alpha ,\beta )$$


<p>对偶问题的最优解如下：</p>
$${d^*} = {\max _{\alpha ,\beta ;{\alpha _i} \geqslant 0}}{\theta _D}(\alpha ,\beta ) = {\max _{\alpha ,\beta ;{\alpha _i} \geqslant 0}}{\min _x}L(x,\alpha ,\beta )$$

<p>我们知道maxmin&lt;minmax,所以我们得到如下式子:</p>
$${d^*} = {\max _{\alpha ,\beta ;{\alpha _i} \geqslant 0}}{\min _x}L(x,\alpha ,\beta ) \leqslant {\min _x}{\max _{\alpha ,\beta ;{\alpha _i} \geqslant 0}}L(x,\alpha ,\beta ) = {p^\*}$$

<p>对于上面的式子只要\(f _i\)和\(g _i\)为凸函数，\(h _i\)为仿射函数，即可使等式两端相等。</p>
<blockquote>
<p>仿射函数为线性函数加截距。<br>另外对于该问题的最优值必满足KKT条件。另外，上述拉格朗日函数对相应参数需要取得极值。最终得到如下条件:</p>
</blockquote>

$$\frac{\partial }{{\partial {x _i}}}L({x^*},{\alpha^*},{\beta^*}) = 0$$
$$\frac{\partial }{{\partial \beta }}L({x^*},{\alpha ^*},{\beta ^*}) = 0$$$${\alpha ^*}{g _i}(x) = 0$$
$${g _i}(x) \leqslant 0$$$${\alpha ^*} \geqslant 0$$

<blockquote>
<p>KKT条件的原理暂时不深入，目前处于应用阶段，有时间再考虑具体原理。</p>
</blockquote>
<h1 id="5-最优边界分类器"><a href="#5-最优边界分类器" class="headerlink" title="5 最优边界分类器"></a>5 最优边界分类器</h1><p>将我们的优化问题转化为如下标准型：</p>
$$\begin{gathered}   & {\min _{w,b}}\frac{1}{2}||w|{|^2} \hfill \\\  s.t.: &  - {y^{(i)}}({w^T}{x^{(i)}} + b) + 1 \leqslant 0, & i = 1,...,n \hfill \\\\end{gathered} $$

<p>根据前面的说明，我们可以通过解对偶问题最优解来获得该问题的最优解。<br>首先写出拉格朗日对偶函数：</p>
$$L(w,b,\alpha ) = \frac{1}{2}||w|{|^2} - \sum\limits _{i = 1}^m {{\alpha _i}({y^{(i)}}({w^T}{x^{(i)}} + b) - 1)} $$

<p>参数上一节的公式，这里\(w\)和\(b\)对应于\(x\)。我们需要关于\(w\)，\(b\)求极小值，然后求得的极值点代入拉格朗日函数，然后求转化后的拉格朗日函数的极大值即可。</p>
$$\frac{\partial }{{\partial w}}L(w,b,\alpha ) = w - \sum\limits _{i = 1}^m {{\alpha _i}{y^{(i)}}{x^{(i)}}}  = 0$$$$\frac{\partial }{{\partial b}}L(w,b,\alpha ) = \sum\limits _{i = 1}^m {{\alpha _i}{y^{(i)}}}  = 0$$

<p>得到\(w &#x3D; \sum\limits _{i &#x3D; 1}^m {\alpha _i}{y^{(i)}{x^{(i)}}} \)，\(\sum\limits _{i &#x3D; 1}^m {\alpha _i}{y^{(i)}}  &#x3D; 0\)。然后将其代入拉格朗日函数：</p>

$$L(\alpha ) = \frac{1}{2}{w^T}w - \sum\limits _{i = 1}^m {{\alpha _i}({y^{(i)}}({w^T}{x^{(i)}} + b) - 1)} $$
$$L(\alpha ) =  - \frac{1}{2}{(\sum\limits _{i = 1}^m {{\alpha _i}{y^{(i)}}{x^{(i)}}} )^T}(\sum\limits _{i = 1}^m {{\alpha _i}{y^{(i)}}{x^{(i)}}} ) + \sum\limits _{i = 1}^m {{\alpha _i}} $$


<p>由于\({\alpha _i}\)和\({y^{(i)}}\)为变量，实际上面的式子就是一个\((Ax + By + Cz)(Ax + By + Cz)\)的问题。因此可以归纳为如下公式：</p>
$$L(\alpha ) =  - \frac{1}{2}\sum\limits _{i = 1}^m {\sum\limits _{j = 1}^m {{\alpha _i}{\alpha _j}{y^{(i)}}{y^{(j)}} < {x^{(i)}},{x^{(j)}} > } }  + \sum\limits _{i = 1}^m {{\alpha _i}} $$

<p>问题就转化为：</p>

$$\begin{gathered}   & {\max _\alpha }L(\alpha ) =  - \frac{1}{2}\sum\limits _{i = 1}^m {\sum\limits _{j = 1}^m {{\alpha _i}{\alpha _j}{y^{(i)}}{y^{(j)}} < {x^{(i)}},{x^{(j)}} > } }  + \sum\limits _{i = 1}^m {{\alpha _i}}  \hfill \\\  s.t.: & {\alpha _i} \geqslant 0 \hfill \\\   & \sum\limits _{i = 1}^n {{\alpha _i}{y^{(i)}}}  = 0 \hfill \\\\end{gathered} $$

<p>由于我们知道支持向量的间隔必须为1，因此我们可以根据其计算\(b\)。设支持向量的集合为S,对属于结合S的样本有\({y^{(i)}}({w^T}{x^{(i)}} + b) &#x3D; 1\)。由于\(w &#x3D; \sum\limits _{i &#x3D; 1}^m {\alpha _i}{y^{(i)}}{x^{(i)}} \)，又由于对所有的非支持向量，有\({\alpha _i} &#x3D; 0\)。因此我们可以综合均值得到：</p>
$$b = \frac{1}{{|S|}}\sum\limits _{i = 1}^S {({y^{(i)}} - \sum\limits _{j = 1}^S {{\alpha _j}{y^{(j)}} < {x^{(i)}},{x^{(j)}} > } )} $$


<h2 id="6-正则化"><a href="#6-正则化" class="headerlink" title="6 正则化"></a>6 正则化</h2><p>关于之前的问题我们假定样本严格可分。但是实际上需要容忍一些误差。因此我们将公式修正为如下形式(C为常数)：</p>

$$\begin{gathered}   & {\min _{w,b,\xi }}\frac{1}{2}||w|{|^2} + C\sum\limits _{i = 1}^m {{\xi _i}}  \hfill \\\  s.t.: & {y^{(i)}}({w^T}{x^{(i)}} + b) \geqslant 1 - {\xi _i} &  & i = 1,...,m \hfill \\\   & {\xi _i} \geqslant 0 &  &  &  & i = 1,...,m \hfill \\\\end{gathered} $$

<p>我们允许\({y^{(i)}}({w^T}{x^{(i)}} + b)\)小于1，但是不希望小太多。所以，我们需要保证\({\xi _i}\)的总和尽可能小，因此得上面的式子。然后我们可以得到拉格朗日公式如下：</p>
$$L(w,b,\xi ,\alpha ,r) = \frac{1}{2}||w|{|^2} + C\sum\limits _{i = 1}^m {{\xi _i}}  - \sum\limits _{i = 1}^m {{\alpha _i}({y^{(i)}}({w^T}{x^{(i)}} + b) - 1 + {\xi _i})}  - \sum\limits _{i = 1}^m {{r _i}{\xi _i}} $$


<p>对\(w\)，\(b\)，\(\xi \), 求导得:</p>
$$\frac{\partial }{{\partial w}}L(w,b,\xi ,\alpha ,r) = w - \sum\limits _{i = 1}^m {{\alpha _i}{y^{(i)}}{x^{(i)}}}  = 0$$
$$\frac{\partial }{{\partial b}}L(w,b,\xi ,\alpha ,r) =  - \sum\limits _{i = 1}^m {{\alpha _i}{y^{(i)}}}  = 0$$
$$\frac{\partial }{{\partial {\xi _i}}}L(w,b,\xi ,\alpha ,r) = C - {r _i} - {\alpha _i} = 0$$

<p>可以得到\(w &#x3D; \sum\limits _{i &#x3D; 1}^m {\alpha _i}{y^{(i)}}{x^{(i)}}\),\(\sum\limits _{i &#x3D; 1}^m {\alpha _i}{y^{(i)}} &#x3D; 0\),\(C &#x3D; {r _i} + {\alpha _i}\)。其中我们知道\({r _i} \geqslant 0\)，所以也可得到\(0 \leqslant {\alpha _i} \leqslant C\)。带入拉格朗日函数得:</p>
$$L(\alpha ) =  - \frac{1}{2}\sum\limits _{i = 1}^m {\sum\limits _{j = 1}^m {{\alpha _i}{\alpha _j}{y^{(i)}}{y^{(j)}} < {x^{(i)}},{x^{(j)}} > } }  + \sum\limits _{i = 1}^m {({r _i} + {\alpha _i}){\xi _i}}  - \sum\limits _{i = 1}^m {{\alpha _i}({\xi _i}}  - 1) - \sum\limits _{i = 1}^m {{r _i}{\xi _i}} $$

<p>我们对上面的式子乘以-1，转化为求最小值的问题，可以得到最终的优化问题：</p>
$$\begin{gathered}   & \min L(\alpha ) = \min \frac{1}{2}\sum\limits _{i = 1}^m {\sum\limits _{j = 1}^m {{\alpha _i}{\alpha _j}{y^{(i)}}{y^{(j)}} < {x^{(i)}},{x^{(j)}} > } }  - \sum\limits _{i = 1}^m {{\alpha _i}}  \hfill \\\  s.t.: & \sum\limits _{i = 1}^m {{\alpha _i}{y^{(i)}}}  = 0 \hfill \\\   & 0 \leqslant {\alpha _i} \leqslant C \hfill \\\\end{gathered} $$


<p>可以得到如下的KKT条件：</p>

$${\alpha _i},{r _i} \geqslant 0$$$${y^{(i)}}({w^T}{x^{(i)}} + b) - 1 + {\xi _i} \geqslant 0$$$${\alpha _i}({y^{(i)}}({w^T}{x^{(i)}} + b) - 1 + {\xi _i}) = 0$$$${\xi _i} \geqslant 0,{r _i}{\xi _i} = 0$$

<p>对于上述KKT条件我们可以转换为如下形式：</p>
$${y^{(i)}}({w^T}{x^{(i)}} + b) \geqslant 1 , {\alpha _i} = 0$$
$${y^{(i)}}({w^T}{x^{(i)}} + b) \leqslant 1 , {\alpha _i} = C$$
$${y^{(i)}}({w^T}{x^{(i)}} + b) = 1 , 0 \leqslant {\alpha _i} \leqslant C$$

<p>很容易转化上面的式子，三个条件分别表示在比支持向量距离分割超平面远的样本，比支持向量距离分割超平面近的可容忍的误差样本，支持向量对应的样本。</p>
<h2 id="7-SMO算法理论"><a href="#7-SMO算法理论" class="headerlink" title="7 SMO算法理论"></a>7 SMO算法理论</h2><p>这一节使用SMO算法解决上一节归纳出来的优化问题。<br>SMO算法的思想来自于坐标上升算法，坐标上升算法的主要思想是一次遍历一个变量，然后把其他变量当做是常亮，进在一个维度上优化。<br>然后对于我们之前的问题，有\(\sum\limits _{i &#x3D; 1}^m {\alpha _i}{y^{(i)}}  &#x3D; 0\)。设置我们设\(\zeta  &#x3D; {\alpha _1}{y^{(1)}} + {\alpha _2}{y^{(2)}} &#x3D; \sum\limits _{i &#x3D; 3}^m {\alpha _i}{y^{(i)}} \)，将\({\alpha _1}\)用\({\alpha _2}\)表达，然后得到关于\({\alpha _2}\)的二次函数，这样很容易取得极值。当所有样本满足KKT条件，且无法继续增加，我们就可以认为此刻取得最优值。</p>
<p>由于我们知道\(0 \leqslant {\alpha _i} \leqslant C\)，所以我们可以求的其取值范围，我们可以将二维变量表述为一个方格内，具体如下：</p>
<img src="/images/机器学习/监督学习-支持向量机图4.png" width=50% height=50% text-align=center/>

<p>最多四种情况代入，经过求截距等一系列操作，可以将的取值范围归纳为下面的公式，其中L表示上界，H表示上界。<br>同号时有：</p>
$$L = \max (0,\alpha _1^{old} + \alpha _2^{old} - C)$$
$$H = \min (C,\alpha _1^{old} + \alpha _2^{old})$$


<p>异号的时候有：</p>
$$L = \max (0,\alpha _2^{old} - \alpha _2^{old})$$
$$H = \min (C,C + \alpha _2^{old} - \alpha _1^{old})$$

<p>进一步求解二次规划问题：</p>

$$\psi ({\alpha _1},{\alpha _2}) = \frac{1}{2}\alpha _1^2k(1,1) + \frac{1}{2}\alpha _2^2k(2,2) + {y^{(1)}}{y^{(2)}}{\alpha _1}{\alpha _2}k(1,2) - {\alpha _1} - {\alpha _2} + {y^{(1)}}{\alpha _1}{v _1} + {y^{(2)}}{\alpha _2}{v _2} + M$$

<p>上式中\(k(i,j) &#x3D;  &lt; {x _i},{x _j} &gt; \)具体是核函数的简写，下节会介绍。\(M\)为与\(\alpha _1\),\(\alpha _2\)无关的参数。另外，\({v _1} &#x3D; \sum\limits _{i &#x3D; 3}^m {\alpha _i}{y^{(i)}}k(1,i) \),\({v _2} &#x3D; \sum\limits _{i &#x3D; 3}^m {\alpha _i}{y^{(i)}}k(2,i) \)。</p>
<p>我们设\(\zeta  &#x3D; {\alpha _1}{y^{(1)}} + {\alpha _2}{y^{(2)}}\)，所以有\({\alpha _1} &#x3D; {y^{(1)}}(\zeta  - {\alpha _2}{y^{(2)}})\),代入上式:</p>

$$\psi ({\alpha _2}) = \frac{1}{2}{(\zeta  - {\alpha _2}{y^{(2)}})^2}k(1,1) + \frac{1}{2}\alpha _2^2k(2,2) + {y^{(2)}}(\zeta  - {\alpha _2}{y^{(2)}}){\alpha _2}k(1,2) - (\zeta  - {\alpha _2}{y^{(2)}}){y^{(1)}} - {\alpha _2} + (\zeta  - {\alpha _2}{y^{(2)}}){v _1} + {y^{(2)}}{\alpha _2}{v _2} + M$$

<p>然后求导数:</p>
$$\frac{\partial }{{\partial {\alpha _2}}}\psi ({\alpha _2}) = ({\alpha _2}{y^{(2)}} - \zeta ){y^{(2)}}k(1,1) + {\alpha _2}k(2,2) + {y^{(2)}}\zeta k(1,2) - 2{\alpha _2}k(1,2) + {y^{(1)}}{y^{(2)}} - {y^{(1)}}{y^{(2)}} - {y^{(2)}}{v _1} + {y^{(2)}}{v _2} = 0$$$$\frac{\partial }{{\partial {\alpha _2}}}\psi ({\alpha _2}) = {\alpha _2}(k(1,1) + k(1,2) - 2k(1,2)) - \zeta {y^{(2)}}k(1,1) + \zeta {y^{(2)}}k(1,2) + {y^{(1)}}{y^{(2)}} - {y^{(1)}}{y^{(2)}} - {y^{(1)}}{v _1} + {y^{(2)}}{v _2} = 0$$

<p>我们设\(\eta  &#x3D; k(1,1) + k(1,2) - 2k(1,2)\)，对\(\eta\)大于0的情况，导数为0的极值点就是极小值。对于\(\eta\)小于等于0的情况，最小值点肯定取自于边界，我们需要比较函数在\(L\)和\(H\)的大小。</p>
<p>让我们继续简化上面的式子，对于\(\eta\)大于0的情况下，取得极值。由于我们轻易得到下面的关系。</p>

$${v_1} = \sum\limits _{i = 1}^m {{\alpha _i}{y^{(2)}}k(1,i)}  + b - b - \sum\limits _{i = 1}^2 {{\alpha _i}{y^{(2)}}k(1,i)}  = f({x^{(1)}}) - b - \sum\limits _{i = 1}^2 {{\alpha _i}{y^{(2)}}k(1,i)} $$

$${v_2} = \sum\limits _{i = 1}^m {{\alpha _i}{y^{(2)}}k(2,i)}  + b - b - \sum\limits _{i = 1}^2 {{\alpha _i}{y^{(2)}}k(2,i)}  = f({x^{(2)}}) - b - \sum\limits _{i = 1}^2 {{\alpha _i}{y^{(2)}}k(2,i)} $$

<p>将上面的关系代入的如下极值：</p>
$$\alpha _2^{new} = \frac{{\zeta {y^{(2)}}k(1,1) - \zeta {y^{(2)}}k(1,2) + {y^{(2)}}({y^{(1)}} - f({x^{(1)}}) - ({y^{(2)}} - f({x^{(2)}})) - {y^{(2)}}\sum\limits _{i = 1}^2 {\alpha _i^{old}{y^{(2)}}k(1,i)}  + {y^{(2)}}\sum\limits _{i = 1}^2 {\alpha _i^{old}{y^{(2)}}k(2,i)} }}{\eta }$$


<p>我们将\(\zeta  &#x3D; {\alpha _1}{y^{(1)}} + {\alpha _2}{y^{(2)}}\)代入上式，得:</p>
$$\alpha _2^{new} = \frac{{{\alpha _1}{y^{(1)}}{y^{(2)}}k(1,1) + {\alpha _2}k(1,1) - {\alpha _1}{y^{(1)}}{y^{(2)}}k(1,2) - {\alpha _2}k(1,2) - {\alpha _1}{y^{(1)}}{y^{(2)}}k(1,1) - {\alpha _2}k(1,2) + {\alpha _2}{y^{(1)}}{y^{(2)}}k(1,2) + {\alpha _2}k(2,2) + {y^{(2)}}({y^{(1)}} - f({x^{(1)}}) - ({y^{(2)}} - f({x^{(2)}}))}}{\eta }$$

<p>约掉部分选项，有:</p>
$$\alpha _2^{new} = \alpha _2^{old} + \frac{{{y^{(2)}}({y^{(1)}} - f({x^{(1)}}) - ({y^{(2)}} - f({x^{(2)}})))}}{\eta }$$
$$\alpha _2^{new} = \alpha _2^{old} + \frac{{{y^{(2)}}({e _1} - {e _2})}}{\eta }$$

<p>接下来就只剩下求\(b\)的问题了，根据上一节最后转化的KKT条件。对\(0 \leqslant {\alpha _1} \leqslant C\)的情况下，有\({y^{(1)}}({w^T}{x^{(1)}} + b) &#x3D; 1\)。所以有：</p>

$$b = {y^{(1)}} - \sum\limits _{i = 1}^m {{\alpha _i}} {y^{(i)}}k(1,i)$$


<p>进一步展开有：</p>

$$b = {y^{(1)}} - \sum\limits _{i = 3}^m {\alpha _i^{new}{y^{(i)}}k(1,i)}  - \alpha _1^{new}{y^{(1)}}k(1,1) - \alpha _2^{new}{y^{(2)}}k(1,2)$$


<p>我们知道上面的式子中\({\alpha _3}\)到\({\alpha _m}\)并没有发生变化，因此有：</p>

$$\sum\limits _{i = 3}^m {\alpha _i^{new}{y^{(i)}}k(1,i)}  = \sum\limits _{i = 3}^m {\alpha _i^{old}{y^{(i)}}k(1,i)}  = \sum\limits _{i = 1}^m {\alpha _i^{old}{y^{(i)}}k(1,i)}  - \alpha _1^{old}{y^{(1)}}k(1,1) - \alpha _2^{old}{y^{(2)}}k(1,2) = f({x^{(1)}}) - b - \alpha _1^{old}{y^{(1)}}k(1,1) - \alpha _2^{old}{y^{(2)}}k(1,2)$$

<p>代入如上式得:</p>
$${b^{new}} =  - {e_1} + (\alpha _1^{old} - \alpha _1^{new}){y^{(1)}}k(1,1) + (\alpha _1^{old} - \alpha _1^{new}){y^{(2)}}k(1,2) + {b^{old}}$$


<p>对于\({\alpha _2}\)有同样的道理。</p>
<p>综上，假如\(0 \leqslant {\alpha _1} \leqslant C\)，有:</p>
$${b^{new}} =  - {e_1} + (\alpha _1^{old} - \alpha _1^{new}){y^{(1)}}k(1,1) + (\alpha _1^{old} - \alpha _1^{new}){y^{(2)}}k(1,2) + {b^{old}}$$


<p>假如\(0 \leqslant {\alpha _2} \leqslant C\)，有：</p>
$${b^{new}} =  - {e_2} + (\alpha _2^{old} - \alpha _2^{new}){y^{(1)}}k(2,1) + (\alpha _2^{old} - \alpha _2^{new}){y^{(2)}}k(2,2) + {b^{old}}$$


<p>假如\(0 \leqslant {\alpha _1},{\alpha _2} \leqslant C\),事实上上面两个公司的出来的结果是一样的，因此不用特殊计算。<br>如果不满足在\(0\)和\(C\)的范围，则去两个公式的中间值。(笔者认为没有必要更新)</p>
<h2 id="8-SMO算法实践"><a href="#8-SMO算法实践" class="headerlink" title="8 SMO算法实践"></a>8 SMO算法实践</h2><p>在SMO论文中有具体的伪代码，算法的主要逻辑就是要保证每个样本都满足KKT条件，且直到所有\(\alpha \)达到极值，即不需要更新为止。</p>
<p>然后是关于\(\alpha \)的选择，第一个\(\alpha \)我们可以随机选择一个违反KKT条件的，第二个我们选择能够最大程度更新\(\alpha \)的值，看上一节的公式，实际会选择\(|e _1-e _2|\)最大的样本点作为第二个\(\alpha \)。具体逻辑可以参考SMO的论文，或者下面代码的注释。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.lines <span class="keyword">import</span> Line2D</span><br><span class="line"></span><br><span class="line"><span class="keyword">global</span> g_y_vec    <span class="comment"># g_y_vec为样本输出, 大小g_m</span></span><br><span class="line"><span class="keyword">global</span> g_x_mat    <span class="comment"># g_x_mat为样本的输入, 大小2*g_m</span></span><br><span class="line"><span class="keyword">global</span> g_m        <span class="comment"># g_m 为样本数目</span></span><br><span class="line"><span class="keyword">global</span> g_alpha    <span class="comment"># g_alpha 大小为g_m</span></span><br><span class="line"><span class="keyword">global</span> g_C</span><br><span class="line"><span class="keyword">global</span> g_w        <span class="comment"># 实际没有使用，而是通过alpha表示的</span></span><br><span class="line"><span class="keyword">global</span> g_b        <span class="comment"># y = wx+b</span></span><br><span class="line"><span class="keyword">global</span> g_y_now    <span class="comment"># 表示当前参数计算的y，即svmOutPut对应的g_y_now</span></span><br><span class="line"><span class="keyword">global</span> g_err      <span class="comment"># g_err 表示svmOutput - g_y_vec对应的序列</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># g_arr0 means classify of -1</span></span><br><span class="line">g_arr0 = np.array([[ <span class="number">0.88235916</span> , <span class="number">1.01511634</span>],[ <span class="number">0.75243817</span> , <span class="number">0.76520033</span>],[ <span class="number">0.95710848</span> , <span class="number">1.41894337</span>],[ <span class="number">1.48682891</span> , <span class="number">0.78885043</span>],[ <span class="number">1.24047011</span> , <span class="number">0.71984948</span>],[ <span class="number">0.67611276</span> , <span class="number">1.07909452</span>],[ <span class="number">1.03243669</span> , <span class="number">1.08929695</span>],[ <span class="number">1.0296548</span>  , <span class="number">1.25023769</span>],[ <span class="number">1.54134008</span> , <span class="number">0.39564824</span>],[ <span class="number">0.34645057</span> , <span class="number">1.61499636</span>],[ <span class="number">0.77206174</span> , <span class="number">1.23613698</span>],[ <span class="number">0.91446988</span> , <span class="number">1.38537765</span>],[ <span class="number">0.99982962</span> , <span class="number">1.34448471</span>],[ <span class="number">0.78745962</span> , <span class="number">0.9046565</span> ],[ <span class="number">0.74946602</span> , <span class="number">1.07424473</span>],[ <span class="number">1.09294839</span> , <span class="number">1.14711993</span>],[ <span class="number">0.39266844</span> , <span class="number">0.78788004</span>],[ <span class="number">0.83112357</span> , <span class="number">1.2762774</span> ],[ <span class="number">1.05056188</span> , <span class="number">1.13351562</span>],[ <span class="number">1.62101523</span> , <span class="number">1.15035562</span>],[ <span class="number">0.70377517</span> , <span class="number">1.1136416</span> ],[ <span class="number">1.03715472</span> , <span class="number">0.47905693</span>],[ <span class="number">0.94598381</span> , <span class="number">0.8874837</span> ],[ <span class="number">0.94447128</span> , <span class="number">2.02796925</span>],[ <span class="number">0.72442242</span> , <span class="number">1.09835206</span>],[ <span class="number">0.69046731</span> , <span class="number">1.46232182</span>],[ <span class="number">1.20744606</span> , <span class="number">1.10280041</span>],[ <span class="number">0.70665746</span> , <span class="number">0.82139503</span>],[ <span class="number">1.08803887</span> , <span class="number">1.4450361</span> ],[ <span class="number">0.88530961</span> , <span class="number">0.75727475</span>],[ <span class="number">0.98418545</span> , <span class="number">0.80248161</span>],[ <span class="number">0.74970386</span> , <span class="number">1.13205709</span>],[ <span class="number">0.72586454</span> , <span class="number">1.06058385</span>],[ <span class="number">0.9071812</span>  , <span class="number">1.09975063</span>],[ <span class="number">0.75182835</span> , <span class="number">0.93570147</span>],[ <span class="number">0.80052289</span> , <span class="number">1.08168507</span>],[ <span class="number">0.40180652</span> , <span class="number">0.9526211</span> ],[ <span class="number">0.62312617</span> , <span class="number">0.84385058</span>],[ <span class="number">0.68212516</span> , <span class="number">1.25912717</span>],[ <span class="number">1.19773245</span> , <span class="number">0.16399654</span>],[ <span class="number">0.96093132</span> , <span class="number">0.43932091</span>],[ <span class="number">1.25471657</span> , <span class="number">0.92371829</span>],[ <span class="number">1.12330272</span> , <span class="number">1.26968747</span>],[ <span class="number">1.30361985</span> , <span class="number">0.99862123</span>],[ <span class="number">1.23477665</span> , <span class="number">1.1742804</span> ],[ <span class="number">0.28471876</span> , <span class="number">0.5806044</span> ],[ <span class="number">1.89355099</span> , <span class="number">1.19928671</span>],[ <span class="number">1.09081369</span> , <span class="number">1.28467312</span>],[ <span class="number">1.40488635</span> , <span class="number">0.90034427</span>],[ <span class="number">1.11672364</span> , <span class="number">1.49070515</span>],[ <span class="number">1.35385212</span> , <span class="number">1.35767891</span>],[ <span class="number">0.92746374</span> , <span class="number">1.79096697</span>],[ <span class="number">1.89142562</span> , <span class="number">0.98228303</span>],[ <span class="number">1.0555218</span>  , <span class="number">0.86070833</span>],[ <span class="number">0.69001255</span> , <span class="number">1.12874741</span>],[ <span class="number">0.98137315</span> , <span class="number">1.3398852</span> ],[ <span class="number">1.02525371</span> , <span class="number">0.77572865</span>],[ <span class="number">1.1354295</span>  , <span class="number">1.07098552</span>],[ <span class="number">1.50829164</span> , <span class="number">1.43065998</span>],[ <span class="number">1.09928764</span> , <span class="number">1.55540292</span>],[ <span class="number">0.64695084</span> , <span class="number">0.79920395</span>],[ <span class="number">0.82059034</span> , <span class="number">0.97533491</span>],[ <span class="number">0.56345455</span> , <span class="number">1.08168272</span>],[ <span class="number">1.06673215</span> , <span class="number">1.19448556</span>],[ <span class="number">0.96512548</span> , <span class="number">1.5268577</span> ],[ <span class="number">0.96914451</span> , <span class="number">1.00902985</span>],[ <span class="number">0.72879413</span> , <span class="number">0.92476415</span>],[ <span class="number">1.0931483</span>  , <span class="number">1.13572242</span>],[ <span class="number">1.34765121</span> , <span class="number">0.83841006</span>],[ <span class="number">1.57813788</span> , <span class="number">0.65915892</span>],[ <span class="number">0.59032608</span> , <span class="number">0.82747946</span>],[ <span class="number">0.83838504</span> , <span class="number">0.67588473</span>],[ <span class="number">1.35101322</span> , <span class="number">1.21027851</span>],[ <span class="number">0.71762153</span> , <span class="number">0.41839038</span>],[ <span class="number">0.61295604</span> , <span class="number">0.66555018</span>],[ <span class="number">0.64379346</span> , <span class="number">0.92925228</span>],[ <span class="number">1.1194968</span>  , <span class="number">0.65876736</span>],[ <span class="number">0.39495437</span> , <span class="number">0.67246734</span>],[ <span class="number">1.05223282</span> , <span class="number">0.17889116</span>],[ <span class="number">0.97810984</span> , <span class="number">1.12794664</span>],[ <span class="number">0.98392719</span> , <span class="number">0.73590255</span>],[ <span class="number">1.25587405</span> , <span class="number">1.21853038</span>],[ <span class="number">1.01150226</span> , <span class="number">1.01835571</span>],[ <span class="number">1.02251614</span> , <span class="number">0.72704228</span>],[ <span class="number">1.00261519</span> , <span class="number">0.95347185</span>],[ <span class="number">0.96362523</span> , <span class="number">0.8607009</span> ],[ <span class="number">0.88034659</span> , <span class="number">1.2307104</span> ],[ <span class="number">0.75907236</span> , <span class="number">0.92799796</span>],[ <span class="number">0.54898709</span> , <span class="number">1.69882285</span>],[ <span class="number">0.55032649</span> , <span class="number">0.98831566</span>],[ <span class="number">1.33360789</span> , <span class="number">1.19793298</span>],[ <span class="number">0.83231239</span> , <span class="number">0.8946538</span> ],[ <span class="number">1.05173094</span> , <span class="number">1.26324289</span>],[ <span class="number">0.81482231</span> , <span class="number">0.56198584</span>],[ <span class="number">1.03854797</span> , <span class="number">1.0553811</span> ],[ <span class="number">1.32669227</span> , <span class="number">1.61115811</span>],[ <span class="number">1.13322152</span> , <span class="number">1.68151695</span>],[ <span class="number">0.39754618</span> , <span class="number">1.19392967</span>],[ <span class="number">0.61344185</span> , <span class="number">1.05281434</span>],[ <span class="number">1.18415366</span> , <span class="number">0.864884</span>  ]])</span><br><span class="line"><span class="comment"># g_arr1 means classify of +1</span></span><br><span class="line">g_arr1 = np.array([[ <span class="number">2.15366548</span> , <span class="number">1.88035458</span>],[ <span class="number">2.36978774</span> , <span class="number">1.76550283</span>],[ <span class="number">2.46261387</span> , <span class="number">2.10568262</span>],[ <span class="number">1.90475526</span> , <span class="number">1.95242885</span>],[ <span class="number">1.77712677</span> , <span class="number">1.96004856</span>],[ <span class="number">1.5995514</span>  , <span class="number">2.1323943</span> ],[ <span class="number">1.52727223</span> , <span class="number">1.50295551</span>],[ <span class="number">1.80330407</span> , <span class="number">1.57942301</span>],[ <span class="number">1.86487049</span> , <span class="number">1.87234414</span>],[ <span class="number">1.9586354</span>  , <span class="number">1.96279729</span>],[ <span class="number">2.59668134</span> , <span class="number">2.414423</span>  ],[ <span class="number">2.818419</span>   , <span class="number">1.76280366</span>],[ <span class="number">2.01511628</span> , <span class="number">2.10858546</span>],[ <span class="number">2.15907962</span> , <span class="number">1.81593012</span>],[ <span class="number">1.63966834</span> , <span class="number">2.2209023</span> ],[ <span class="number">2.47220599</span> , <span class="number">1.70482956</span>],[ <span class="number">2.08760748</span> , <span class="number">2.51601971</span>],[ <span class="number">1.50547722</span> , <span class="number">1.8487145</span> ],[ <span class="number">1.68125583</span> , <span class="number">2.64968501</span>],[ <span class="number">2.01924282</span> , <span class="number">2.0953572</span> ],[ <span class="number">2.22563534</span> , <span class="number">2.18266325</span>],[ <span class="number">2.2684291</span>  , <span class="number">2.23581599</span>],[ <span class="number">2.13787557</span> , <span class="number">1.9999382</span> ],[ <span class="number">1.02638695</span> , <span class="number">1.68134967</span>],[ <span class="number">2.35614619</span> , <span class="number">1.32072125</span>],[ <span class="number">2.20054871</span> , <span class="number">1.47401445</span>],[ <span class="number">1.99454827</span> , <span class="number">1.71658741</span>],[ <span class="number">1.83269065</span> , <span class="number">2.47662909</span>],[ <span class="number">2.40097251</span> , <span class="number">2.21823862</span>],[ <span class="number">2.54404652</span> , <span class="number">1.85742018</span>],[ <span class="number">1.84150027</span> , <span class="number">2.06350351</span>],[ <span class="number">1.69490855</span> , <span class="number">1.70169334</span>],[ <span class="number">1.44745704</span> , <span class="number">1.88295233</span>],[ <span class="number">2.24376639</span> , <span class="number">1.67530495</span>],[ <span class="number">1.42911921</span> , <span class="number">1.81854548</span>],[ <span class="number">1.33789289</span> , <span class="number">2.27686128</span>],[ <span class="number">2.43509821</span> , <span class="number">1.95032131</span>],[ <span class="number">1.9512447</span>  , <span class="number">1.4595415</span> ],[ <span class="number">2.13041192</span> , <span class="number">1.79372755</span>],[ <span class="number">2.2753866</span>  , <span class="number">2.23781951</span>],[ <span class="number">2.26753401</span> , <span class="number">1.78149305</span>],[ <span class="number">2.06505449</span> , <span class="number">2.01939606</span>],[ <span class="number">2.44426826</span> , <span class="number">2.1437101</span> ],[ <span class="number">2.16607141</span> , <span class="number">2.31077167</span>],[ <span class="number">1.96097237</span> , <span class="number">2.49100193</span>],[ <span class="number">1.37255424</span> , <span class="number">1.60735016</span>],[ <span class="number">1.63947758</span> , <span class="number">2.17852314</span>],[ <span class="number">2.13722666</span> , <span class="number">2.00559707</span>],[ <span class="number">1.222696</span>   , <span class="number">1.67075059</span>],[ <span class="number">2.56982685</span> , <span class="number">2.51218813</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcLH</span>(<span class="params">id1,id2</span>):</span><br><span class="line">  <span class="keyword">if</span> g_y_vec[id1] == g_y_vec[id2]:</span><br><span class="line">    L = <span class="built_in">max</span>(<span class="number">0</span>,g_alpha[id1]+g_alpha[id2]-g_C)</span><br><span class="line">    H = <span class="built_in">min</span>(g_C,g_alpha[id1]+g_alpha[id2])</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    L = <span class="built_in">max</span>(<span class="number">0</span>,g_alpha[id2]-g_alpha[id1])</span><br><span class="line">    H = <span class="built_in">min</span>(g_C,g_C+g_alpha[id2]-g_alpha[id1])</span><br><span class="line">  <span class="keyword">return</span> (L,H)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">svmOutput</span>(<span class="params">id1</span>):</span><br><span class="line">  <span class="comment"># 这个函数是svm的实际输出，计算当前参数(w,b)下, 计算得到的y</span></span><br><span class="line">  <span class="comment"># 由于w = sum (alpha*y*x), 对于第i个分量x_i所以输出结果应该为w*x_i+b, 也就是 sum (alpha*y*&lt;x_1-m,x_i&gt;)</span></span><br><span class="line">  <span class="comment"># 注: 待会分析下alpha的变化趋势与C的关系</span></span><br><span class="line">  <span class="keyword">global</span> g_b</span><br><span class="line">  <span class="built_in">sum</span> = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">    <span class="built_in">sum</span> += g_alpha[i]*g_y_vec[i]*kernel(i,id1)</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">sum</span>+g_b</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kernel</span>(<span class="params">id1,id2</span>):</span><br><span class="line">  <span class="comment"># 这里核函数为简单的内积</span></span><br><span class="line">  <span class="comment"># id1和id2为int类型，是g_x_mat中的索引</span></span><br><span class="line">  <span class="keyword">return</span> np.dot(g_x_mat[id1],g_x_mat[id2])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compareFun</span>(<span class="params">id1,id2,L,H</span>):</span><br><span class="line">  <span class="comment"># 如果返回1，表示L处去极小值。如果返回-1，H处去极小值。如果是0，表示这次不更新</span></span><br><span class="line">  <span class="comment"># 如果两者相等，这里略过, 说明无法有强力证据证明这个样本属于wx+b&gt;1还是wx+b&lt;1，所以等待下一轮迭代。因此，与L和H相等应该设置一个阀值，判断近似相等。</span></span><br><span class="line">  <span class="keyword">global</span> g_b</span><br><span class="line">  y_1 = g_y_vec[id1]</span><br><span class="line">  y_2 = g_y_vec[id2]</span><br><span class="line">  e_1 = g_err[id1]</span><br><span class="line">  e_2 = g_err[id2]</span><br><span class="line">  alpha_1 = g_alpha[id1]</span><br><span class="line">  alpha_2 = g_alpha[id2]</span><br><span class="line">  k11 = kernel(id1,id1)</span><br><span class="line">  k12 = kernel(id1,id2)</span><br><span class="line">  k22 = kernel(id2,id2)</span><br><span class="line">  s = y_1*y_2</span><br><span class="line">  f_1 = y_1*(e_1+g_b)-alpha_1*k11-s*alpha_2*k12</span><br><span class="line">  f_2 = y_2*(e_2+g_b)-s*alpha_1*k12-alpha_2*k22</span><br><span class="line">  L_1 = alpha_1+s*(alpha_2-L)</span><br><span class="line">  H_1 = alpha_1+s*(alpha_2-H)</span><br><span class="line">  phi_l = L_1*f_1+L*f_2+<span class="number">0.5</span>*L_1*L_1*k11+<span class="number">0.5</span>*L*L*k22+s*L*L_1*k12</span><br><span class="line">  phi_h = H_1*f_1+H*f_2+<span class="number">0.5</span>*H_1*H_1*k11+<span class="number">0.5</span>*H*H*k22+s*H*H_1*k12</span><br><span class="line">  <span class="keyword">if</span> L==H:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  <span class="keyword">if</span> phi_l &lt; phi_h:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">takeStep</span>(<span class="params">id1,id2,err</span>):</span><br><span class="line">  <span class="keyword">if</span> id1==id2:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  alpha_1 = g_alpha[id1]</span><br><span class="line">  alpha_2 = g_alpha[id2]</span><br><span class="line">  y_1 = g_y_vec[id1]</span><br><span class="line">  y_2 = g_y_vec[id2]</span><br><span class="line">  e1 = g_err[id1]</span><br><span class="line">  e2 = g_err[id2]</span><br><span class="line">  s = y_1*y_2</span><br><span class="line">  L,H=calcLH(id1,id2)</span><br><span class="line">  <span class="comment">#print(&quot;id1=&quot;,id1,&quot;, id2=&quot;,id2,&quot; L=&quot;,L,&quot;, H=&quot;,H)</span></span><br><span class="line">  <span class="keyword">if</span> L==H :</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  k11 = kernel(id1,id1)</span><br><span class="line">  k12 = kernel(id1,id2)</span><br><span class="line">  k22 = kernel(id2,id2)</span><br><span class="line">  eta = k11+k22-<span class="number">2</span>*k12</span><br><span class="line">  <span class="comment">#print(&quot;kernel:&quot;,k11,k12,k22,&quot;, eta=&quot;,eta,&quot;e1=&quot;,e1,&quot;e2=&quot;,e2)</span></span><br><span class="line"></span><br><span class="line">  alpha_2_new = alpha_2</span><br><span class="line">  <span class="comment"># 如果eta大于0, 我们可知最小值在边界或极小值点上。事实上，如果极小值不在范围内，必在距离极小值近的那个边界上。</span></span><br><span class="line">  <span class="comment"># 如果eta小于0, 我们可知最小值则必在边界上。我们只需要比较两个边界点函数的大小即可。</span></span><br><span class="line">  <span class="keyword">if</span> eta&gt;<span class="number">0</span> :</span><br><span class="line">    alpha_2_new = alpha_2+y_2*(e1-e2)/eta</span><br><span class="line">    alpha_2_new = <span class="built_in">max</span>(alpha_2_new,L)</span><br><span class="line">    alpha_2_new = <span class="built_in">min</span>(alpha_2_new,H)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># 由于我们知道可以直接这是一个关于alpha的二次函数，并且自变量的取值范围是[L,H], 事实上我们只需要比较alpha_2_new离L和H哪个远即可</span></span><br><span class="line">    <span class="comment"># 但是考虑到eta=0的一次函数特殊情况，我们还是老老实实的计算函数值吧。</span></span><br><span class="line">    ret = compareFun(id1, id2, L, H)</span><br><span class="line">    <span class="keyword">if</span> ret == <span class="number">0</span>:</span><br><span class="line">      alpha_2_new = alpha_2</span><br><span class="line">    <span class="keyword">elif</span> ret == <span class="number">1</span>:</span><br><span class="line">      alpha_2_new = L</span><br><span class="line">    <span class="keyword">elif</span> ret == <span class="number">1</span>:</span><br><span class="line">      alpha_2_new = H</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;----------------eta&lt;=0----------------&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 归整化alpha_2_new</span></span><br><span class="line">  <span class="keyword">if</span> alpha_2_new &lt; err:</span><br><span class="line">    alpha_2_new = <span class="number">0</span></span><br><span class="line">  <span class="keyword">elif</span> alpha_2_new &gt; g_C - err:</span><br><span class="line">    alpha_2_new = g_C</span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">abs</span>(alpha_2_new-alpha_2) &lt; err:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;alpha_2 is no need to update&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># update b</span></span><br><span class="line">  <span class="keyword">global</span> g_b</span><br><span class="line">  alpha_1_new = alpha_1 + s * (alpha_2 - alpha_2_new)      <span class="comment"># 不必担心alpha_1_new不在[0,C]范围内，之前的公式已经保证了</span></span><br><span class="line">  b1_new = -e1-y_1*k11*(alpha_1_new-alpha_1)-y_2*k12*(alpha_2_new-alpha_2) + g_b</span><br><span class="line">  b2_new = -e2-y_1*k12*(alpha_1_new-alpha_1)-y_2*k22*(alpha_2_new-alpha_2) + g_b</span><br><span class="line">  <span class="keyword">if</span> alpha_1_new&gt;<span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> alpha_1_new&lt;g_C:</span><br><span class="line">    g_b = b1_new</span><br><span class="line">  <span class="keyword">elif</span> alpha_2_new&gt;<span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> alpha_2_new&lt;g_C:</span><br><span class="line">    g_b = b2_new</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    g_b = <span class="number">0.5</span>*(b1_new+b2_new)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># update alpha</span></span><br><span class="line">  g_alpha[id1] = alpha_1_new</span><br><span class="line">  g_alpha[id2] = alpha_2_new</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;g_alpha[id1]=&quot;</span>,g_alpha[id1],<span class="string">&quot;g_alpha[id2]=&quot;</span>,g_alpha[id2],<span class="string">&quot;s=&quot;</span>,s,<span class="string">&quot;, alpha_1=&quot;</span>,alpha_1,<span class="string">&quot;, alpha_2=&quot;</span>,alpha_2)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># update err g_y_now</span></span><br><span class="line">  updateYAndErr()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updateYAndErr</span>():</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">    g_y_now[i] = svmOutput(i)</span><br><span class="line">    g_err[i] = svmOutput(i)-g_y_vec[i]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chooseBestAlphaIndex</span>(<span class="params">id2</span>):</span><br><span class="line">  maxIncr = <span class="number">0</span></span><br><span class="line">  maxIndex = -<span class="number">1</span>;</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">    incr = <span class="built_in">abs</span>(g_err[i]-g_alpha[i])</span><br><span class="line">    <span class="keyword">if</span> incr &gt;= maxIncr:</span><br><span class="line">      maxIndex = i</span><br><span class="line">      maxIncr = incr</span><br><span class="line">  <span class="keyword">return</span> maxIndex</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sizeOfNonZerorAndNonC</span>():</span><br><span class="line">  size=<span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">    <span class="keyword">if</span> g_alpha[i]!=<span class="number">0</span> <span class="keyword">and</span> g_alpha[i]!=g_C:</span><br><span class="line">      size= size+<span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> size</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chooseRandomIndex</span>(<span class="params">id2</span>):</span><br><span class="line">  ret = id2;</span><br><span class="line">  <span class="keyword">while</span> ret==id2:</span><br><span class="line">    ret = random.randint(<span class="number">0</span>,g_m-<span class="number">1</span>)</span><br><span class="line">  <span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于examineExample函数,我们一次进选择id2样本对应的alpha与如下规则选择的id1对应的alpha,然后相应跟新其值。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">examineExample</span>(<span class="params">id2</span>):</span><br><span class="line">  y_2 = g_y_vec[id2]</span><br><span class="line">  tol = <span class="number">1e-2</span>        <span class="comment"># 是一个正数</span></span><br><span class="line">  alpha_2 = g_alpha[id2]</span><br><span class="line">  e_2 = svmOutput(id2)-g_y_vec[id2]</span><br><span class="line">  r_2 = e_2 * y_2</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 我们只对当前违反kkt条件的样本对应的alpha进行更新</span></span><br><span class="line">  <span class="comment"># 关于违反kkt条件的说明:</span></span><br><span class="line">  <span class="comment"># (1) r_2 &lt; -tol 表示r_2小于0，即表示输出的预测结果与样本的y符号相反，因此应该属于误差案例的。所以，根据公式，alpha = C。如果alpha &lt; C ,比违反kkt条件</span></span><br><span class="line">  <span class="comment"># (2) r_2 &lt; -tol 表示r_2大于0，即表示输出的预测结果与样本的y符号相同，当误差大于一定的</span></span><br><span class="line">  <span class="keyword">if</span> r_2 &lt; -tol <span class="keyword">and</span> alpha_2 &lt; g_C <span class="keyword">or</span> r_2 &gt; tol <span class="keyword">and</span> alpha_2 &gt; <span class="number">0</span> :</span><br><span class="line">    <span class="comment"># 下面的程序逻辑是这样的:</span></span><br><span class="line">    <span class="comment"># 先遍历alpha非0或非C, 因为我们对于alpha为0和alpha为C的情况, 认为是处于非支持向量和处于误差样本的情况。我们只有根据支持向量下，找到最优的||w||才有意义</span></span><br><span class="line">    <span class="comment"># 首先，我们找到|e1-e2|最大的alpha, 从这里优化。如果优化结果不理想，我们就随机找一个alpha一起计算。如果还不行，就在整个范围alpha范围内计算</span></span><br><span class="line">    <span class="keyword">if</span> sizeOfNonZerorAndNonC()&gt;<span class="number">0</span>:</span><br><span class="line">      id1=chooseBestAlphaIndex(id2)</span><br><span class="line">      <span class="keyword">if</span> takeStep(id1,id2,<span class="number">1e-3</span>):</span><br><span class="line">        <span class="comment">#print(&quot;takeStep1, alpha[&quot;,id1,&quot;]=&quot;,g_alpha[id1],&quot;, alphapp[&quot;,id2,&quot;]=&quot;,g_alpha[id2])</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    r=chooseRandomIndex(id2)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">      id1 = (r+i)%g_m</span><br><span class="line">      <span class="keyword">if</span> id1!=id2 <span class="keyword">and</span> g_alpha[id1]!=<span class="number">0</span> <span class="keyword">and</span> g_alpha[id1]!=g_C:</span><br><span class="line">        <span class="keyword">if</span> takeStep(id1,id2,<span class="number">1e-3</span>):</span><br><span class="line">          <span class="comment">#print(&quot;takeStep2, alpha[&quot;, id1, &quot;]=&quot;, g_alpha[id1], &quot;, alphapp[&quot;, id2, &quot;]=&quot;, g_alpha[id2])</span></span><br><span class="line">          <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    r = chooseRandomIndex(id2)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">      id1 = (r + i) % g_m</span><br><span class="line">      <span class="keyword">if</span> id1 != id2:</span><br><span class="line">        <span class="keyword">if</span> takeStep(id1,id2,<span class="number">1e-3</span>):</span><br><span class="line">          <span class="comment">#print(&quot;takeStep3, alpha[&quot;, id1, &quot;]=&quot;, g_alpha[id1], &quot;, alphapp[&quot;, id2, &quot;]=&quot;, g_alpha[id2])</span></span><br><span class="line">          <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">showPic</span>(<span class="params">w,b</span>):</span><br><span class="line">  <span class="comment"># draw wx+b, x1为横轴，x2为纵轴</span></span><br><span class="line">  k = -w[<span class="number">0</span>]/w[<span class="number">1</span>]</span><br><span class="line">  b = -g_b/w[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">  figure, ax = plt.subplots()</span><br><span class="line">  ax.set_xlim(left=-<span class="number">1</span>, right=<span class="number">4</span>)</span><br><span class="line">  ax.set_ylim(bottom=-<span class="number">1</span>, top=<span class="number">4</span>)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">    x = g_x_mat[i]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">abs</span>(g_alpha[i]-<span class="number">0</span>)&lt;<span class="number">1e-3</span>:</span><br><span class="line">      plt.plot(x[<span class="number">0</span>], x[<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;+&#x27;</span>, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">abs</span>(g_alpha[i]-g_C)&lt;<span class="number">1e-3</span>:</span><br><span class="line">      plt.plot(x[<span class="number">0</span>], x[<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, color=<span class="string">&#x27;g&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      plt.plot(x[<span class="number">0</span>], x[<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, color=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">  <span class="comment"># draw y = -x+3</span></span><br><span class="line">  (line1_xs, line1_ys) = [(<span class="number">0</span>, <span class="number">3</span>), (b, <span class="number">3</span>*k+b)]</span><br><span class="line">  ax.add_line(Line2D(line1_xs, line1_ys, linewidth=<span class="number">1</span>, color=<span class="string">&#x27;blue&#x27;</span>))</span><br><span class="line">  plt.xlabel(<span class="string">&quot;x1&quot;</span>)</span><br><span class="line">  plt.ylabel(<span class="string">&quot;x2&quot;</span>)</span><br><span class="line">  plt.plot()</span><br><span class="line">  plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">  <span class="comment"># 0 make data</span></span><br><span class="line">  g_y_vec = np.array([])</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(g_arr0)):</span><br><span class="line">    g_y_vec=np.append(g_y_vec,-<span class="number">1</span>)</span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(g_arr1)):</span><br><span class="line">    g_y_vec=np.append(g_y_vec,+<span class="number">1</span>)</span><br><span class="line">  g_x_mat = np.vstack((g_arr0,g_arr1))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 1 training</span></span><br><span class="line">  g_m = <span class="built_in">len</span>(g_x_mat)</span><br><span class="line">  g_alpha = np.zeros(g_m)</span><br><span class="line">  g_y_now = np.zeros(g_m)</span><br><span class="line">  g_err = np.zeros(g_m)</span><br><span class="line">  <span class="keyword">global</span> g_b</span><br><span class="line">  g_b = <span class="number">0</span></span><br><span class="line">  numChanged = <span class="number">0</span></span><br><span class="line">  examineAll = <span class="number">1</span></span><br><span class="line">  g_C = <span class="number">10</span></span><br><span class="line">  err = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  updateYAndErr()       <span class="comment"># 实现更新下缓冲，即当前输出与误差值</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> numChanged&gt;<span class="number">0</span> <span class="keyword">or</span> examineAll:</span><br><span class="line">    numChanged = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 循环处理，第一次对所有的样本进行一次处理。</span></span><br><span class="line">    <span class="comment"># 然后对所有非边界的数值进行处理。因为在当前参数下，非边界的样本，我们认为其是支持向量。对于优化||w||的大小，支持向量才有意义。</span></span><br><span class="line">    <span class="keyword">if</span> examineAll:</span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">        numChanged += examineExample(i)</span><br><span class="line">        <span class="comment">#print(&quot;examineAll=1, numChanged=&quot;,numChanged)</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">        <span class="keyword">if</span> g_alpha[i]!=<span class="number">0</span> <span class="keyword">and</span> g_alpha[i]!=g_C:</span><br><span class="line">          numChanged += examineExample(i)</span><br><span class="line">    examineAll = <span class="built_in">abs</span>(examineAll-<span class="number">1</span>)</span><br><span class="line">  <span class="comment">#print(g_alpha)</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 3 show</span></span><br><span class="line">  <span class="comment"># 计算w</span></span><br><span class="line">  w = np.array([<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">    <span class="keyword">if</span> g_alpha[i]!=<span class="number">0</span>:</span><br><span class="line">      w=np.add(w,g_y_vec[i]*g_alpha[i]*g_x_mat[i])</span><br><span class="line"></span><br><span class="line">  showPic(w,g_b)</span><br><span class="line">  <span class="comment">#print(g_alpha)</span></span><br></pre></td></tr></table></figure>

<p>经过数轮迭代得到如下结果，其中直线为得到的分割曲线，蓝色点为支持向量，红色点是那些有良好分类的样本，绿色点为可容忍的误差样本。</p>
<img src="/images/机器学习/监督学习-支持向量机计算结果1.png" width=50% height=50% text-align=center/>


<h2 id="9-核函数"><a href="#9-核函数" class="headerlink" title="9 核函数"></a>9 核函数</h2><p>SMO另个非常强大的地方上，它能够很好的解决非线性问题。我们之前的公式中有\(&lt;{x _i},{x _j}&gt;\)，他是两个向量的內积，代表着两个样本的相关性。我们把这个叫做核函数。核函数不仅仅是可以为简单的內积，还可以对样本进行多维展开，映射到高维地址空间。这样在低维地址空间线性不可分的样本，在高维空间就变得线性可分了。譬如,\(|x| &lt; 1\)不是线性可分的，但是对其进行\(x \to (x,{x^2})\)映射后，也就的到一个二次曲线，我们可以使用\(y &#x3D; 1\)进行线性分割。</p>
<p>下面直接引入高斯核函数，它可以对函数进行无线维空间的映射。具体定义如下：</p>
$$k(x,z) = \exp ( - \frac{{||x - z|{|^2}}}{{2{\sigma ^2}}})$$

<blockquote>
<p>对于核函数的数学理论和几何意义，以及高斯核为啥可以向无限维空间映射，之后有时间需要详细研究。</p>
</blockquote>
<p>我们制造一组\(x _1^2 + x _2^2 &#x3D; 1\)分割的样本，然后尝试对其进行分割，实际上与上一节程序的区别仅仅在于核函数的选取，这里我们使用高斯核函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.patches <span class="keyword">import</span> Circle</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">global</span> g_y_vec    <span class="comment"># g_y_vec为样本输出, 大小g_m</span></span><br><span class="line"><span class="keyword">global</span> g_x_mat    <span class="comment"># g_x_mat为样本的输入, 大小2*g_m</span></span><br><span class="line"><span class="keyword">global</span> g_m        <span class="comment"># g_m 为样本数目</span></span><br><span class="line"><span class="keyword">global</span> g_alpha    <span class="comment"># g_alpha 大小为g_m</span></span><br><span class="line"><span class="keyword">global</span> g_C</span><br><span class="line"><span class="keyword">global</span> g_w        <span class="comment"># 实际没有使用，而是通过alpha表示的</span></span><br><span class="line"><span class="keyword">global</span> g_b        <span class="comment"># y = wx+b</span></span><br><span class="line"><span class="keyword">global</span> g_y_now    <span class="comment"># 表示当前参数计算的y，即svmOutPut对应的g_y_now</span></span><br><span class="line"><span class="keyword">global</span> g_err      <span class="comment"># g_err 表示svmOutput - g_y_vec对应的序列</span></span><br><span class="line"></span><br><span class="line">g_x_mat = np.array([[<span class="number">0.602</span>, <span class="number">0.732</span>], [-<span class="number">0.599</span>, <span class="number">0.945</span>], [-<span class="number">0.337</span>, -<span class="number">0.677</span>], [<span class="number">0.459</span>, -<span class="number">0.486</span>], [<span class="number">1.056</span>, <span class="number">0.137</span>], [<span class="number">0.838</span>, <span class="number">0.623</span>], [<span class="number">1.022</span>, -<span class="number">0.685</span>], [<span class="number">0.504</span>, <span class="number">0.821</span>], [-<span class="number">0.977</span>, -<span class="number">0.724</span>], [<span class="number">1.116</span>, <span class="number">0.56</span>], [<span class="number">0.969</span>, <span class="number">0.151</span>], [-<span class="number">0.693</span>, <span class="number">0.077</span>], [<span class="number">1.042</span>, -<span class="number">0.146</span>], [<span class="number">0.705</span>, -<span class="number">0.215</span>], [<span class="number">1.024</span>, -<span class="number">0.322</span>], [<span class="number">1.025</span>, -<span class="number">0.172</span>], [<span class="number">0.306</span>, -<span class="number">1.12</span>], [-<span class="number">0.131</span>, <span class="number">0.008</span>], [-<span class="number">1.157</span>, -<span class="number">1.081</span>], [<span class="number">0.452</span>, -<span class="number">0.865</span>], [-<span class="number">1.117</span>, -<span class="number">0.533</span>], [-<span class="number">1.083</span>, -<span class="number">0.355</span>], [-<span class="number">0.982</span>, <span class="number">0.572</span>], [-<span class="number">1.053</span>, <span class="number">1.003</span>], [-<span class="number">0.553</span>, -<span class="number">0.434</span>], [-<span class="number">0.115</span>, <span class="number">0.283</span>], [<span class="number">0.785</span>, <span class="number">0.233</span>], [-<span class="number">0.926</span>, -<span class="number">0.299</span>], [-<span class="number">1.039</span>, <span class="number">0.581</span>], [<span class="number">0.869</span>, -<span class="number">1.033</span>], [<span class="number">0.754</span>, -<span class="number">1.091</span>], [-<span class="number">1.096</span>, -<span class="number">0.311</span>], [<span class="number">0.537</span>, <span class="number">0.508</span>], [-<span class="number">0.38</span>, -<span class="number">0.565</span>], [<span class="number">1.165</span>, <span class="number">0.219</span>], [-<span class="number">0.123</span>, <span class="number">0.431</span>], [<span class="number">1.048</span>, -<span class="number">0.896</span>], [-<span class="number">0.409</span>, <span class="number">0.299</span>], [<span class="number">0.537</span>, -<span class="number">0.126</span>], [<span class="number">0.985</span>, -<span class="number">0.577</span>], [-<span class="number">1.135</span>, <span class="number">1.025</span>], [-<span class="number">0.779</span>, <span class="number">0.81</span>], [<span class="number">0.547</span>, <span class="number">0.697</span>], [<span class="number">0.424</span>, -<span class="number">1.015</span>], [<span class="number">0.421</span>, -<span class="number">0.904</span>], [<span class="number">0.151</span>, -<span class="number">0.149</span>], [<span class="number">0.77</span>, -<span class="number">1.011</span>], [-<span class="number">0.401</span>, <span class="number">1.113</span>], [<span class="number">0.817</span>, <span class="number">0.573</span>], [<span class="number">0.87</span>, -<span class="number">0.266</span>], [-<span class="number">0.731</span>, <span class="number">0.418</span>], [-<span class="number">0.651</span>, <span class="number">0.063</span>], [<span class="number">0.731</span>, <span class="number">0.04</span>], [<span class="number">0.649</span>, <span class="number">0.677</span>], [-<span class="number">0.084</span>, -<span class="number">0.568</span>], [<span class="number">0.391</span>, -<span class="number">0.171</span>], [-<span class="number">1.07</span>, <span class="number">0.738</span>], [-<span class="number">0.307</span>, <span class="number">0.702</span>], [<span class="number">0.854</span>, <span class="number">1.125</span>], [<span class="number">0.093</span>, -<span class="number">0.148</span>], [-<span class="number">0.82</span>, <span class="number">0.969</span>], [<span class="number">0.11</span>, -<span class="number">1.011</span>], [<span class="number">0.672</span>, -<span class="number">0.261</span>], [<span class="number">0.6</span>, -<span class="number">0.262</span>], [<span class="number">0.28</span>, <span class="number">0.001</span>], [-<span class="number">0.005</span>, -<span class="number">0.544</span>], [-<span class="number">0.666</span>, <span class="number">0.046</span>], [-<span class="number">0.457</span>, -<span class="number">0.129</span>], [-<span class="number">1.02</span>, <span class="number">1.071</span>], [<span class="number">1.191</span>, <span class="number">0.121</span>], [<span class="number">0.665</span>, -<span class="number">0.884</span>], [<span class="number">0.412</span>, <span class="number">0.665</span>], [-<span class="number">0.992</span>, -<span class="number">1.165</span>], [-<span class="number">0.726</span>, -<span class="number">1.178</span>], [-<span class="number">0.886</span>, <span class="number">1.08</span>], [<span class="number">0.263</span>, <span class="number">0.481</span>], [-<span class="number">0.051</span>, <span class="number">0.668</span>], [<span class="number">0.933</span>, -<span class="number">0.008</span>], [-<span class="number">0.896</span>, -<span class="number">0.637</span>], [-<span class="number">0.605</span>, <span class="number">0.287</span>], [<span class="number">0.03</span>, -<span class="number">0.232</span>], [<span class="number">0.749</span>, <span class="number">0.012</span>], [<span class="number">1.175</span>, <span class="number">0.632</span>], [<span class="number">0.968</span>, <span class="number">1.106</span>], [-<span class="number">1.19</span>, <span class="number">0.82</span>], [<span class="number">0.641</span>, <span class="number">0.129</span>], [-<span class="number">0.375</span>, -<span class="number">1.079</span>], [-<span class="number">0.267</span>, -<span class="number">0.442</span>], [<span class="number">0.361</span>, -<span class="number">0.741</span>], [-<span class="number">0.475</span>, <span class="number">0.473</span>], [<span class="number">0.133</span>, <span class="number">1.18</span>], [<span class="number">1.146</span>, <span class="number">1.185</span>], [-<span class="number">0.293</span>, <span class="number">0.172</span>], [<span class="number">0.78</span>, -<span class="number">0.805</span>], [<span class="number">0.186</span>, -<span class="number">0.089</span>], [-<span class="number">0.068</span>, <span class="number">0.829</span>], [-<span class="number">0.621</span>, -<span class="number">0.778</span>], [<span class="number">0.407</span>, -<span class="number">0.523</span>], [<span class="number">0.415</span>, -<span class="number">0.01</span>], [-<span class="number">0.229</span>, <span class="number">0.002</span>], [-<span class="number">0.997</span>, -<span class="number">0.891</span>], [<span class="number">1.011</span>, -<span class="number">1.186</span>], [<span class="number">0.19</span>, -<span class="number">0.437</span>], [<span class="number">0.958</span>, <span class="number">0.669</span>], [-<span class="number">0.888</span>, -<span class="number">0.217</span>], [<span class="number">0.444</span>, <span class="number">0.05</span>], [-<span class="number">0.54</span>, -<span class="number">1.041</span>], [-<span class="number">0.314</span>, <span class="number">0.296</span>], [<span class="number">0.879</span>, -<span class="number">0.898</span>], [<span class="number">0.127</span>, -<span class="number">0.008</span>], [<span class="number">0.995</span>, -<span class="number">1.11</span>], [-<span class="number">0.878</span>, -<span class="number">0.843</span>], [-<span class="number">0.109</span>, <span class="number">0.189</span>], [<span class="number">0.859</span>, <span class="number">0.564</span>], [-<span class="number">0.023</span>, <span class="number">0.945</span>], [-<span class="number">0.878</span>, <span class="number">0.899</span>], [-<span class="number">0.062</span>, -<span class="number">1.051</span>], [<span class="number">0.394</span>, <span class="number">0.519</span>], [-<span class="number">1.139</span>, <span class="number">0.282</span>], [-<span class="number">0.494</span>, -<span class="number">0.075</span>], [-<span class="number">0.922</span>, <span class="number">1.11</span>], [<span class="number">0.753</span>, -<span class="number">1.018</span>], [<span class="number">0.816</span>, -<span class="number">1.106</span>], [<span class="number">0.03</span>, <span class="number">0.569</span>], [-<span class="number">1.11</span>, -<span class="number">0.289</span>], [<span class="number">0.777</span>, <span class="number">0.025</span>], [<span class="number">0.892</span>, <span class="number">0.784</span>], [<span class="number">0.91</span>, <span class="number">0.176</span>], [<span class="number">0.692</span>, <span class="number">0.099</span>], [<span class="number">0.97</span>, <span class="number">0.58</span>], [<span class="number">0.034</span>, <span class="number">1.151</span>], [-<span class="number">0.606</span>, -<span class="number">0.775</span>], [<span class="number">0.873</span>, -<span class="number">0.579</span>], [<span class="number">0.833</span>, -<span class="number">1.042</span>], [-<span class="number">0.251</span>, <span class="number">0.102</span>], [<span class="number">0.436</span>, -<span class="number">0.585</span>], [<span class="number">0.86</span>, -<span class="number">1.06</span>], [-<span class="number">1.118</span>, <span class="number">1.094</span>], [<span class="number">0.598</span>, -<span class="number">0.129</span>], [<span class="number">0.694</span>, <span class="number">0.281</span>], [<span class="number">1.048</span>, -<span class="number">1.036</span>], [-<span class="number">0.348</span>, <span class="number">0.639</span>], [<span class="number">1.046</span>, -<span class="number">1.124</span>], [-<span class="number">0.333</span>, -<span class="number">0.463</span>], [-<span class="number">0.447</span>, -<span class="number">0.009</span>], [<span class="number">0.344</span>, -<span class="number">0.852</span>], [-<span class="number">1.174</span>, <span class="number">0.196</span>], [<span class="number">0.701</span>, <span class="number">0.695</span>], [-<span class="number">0.916</span>, -<span class="number">0.128</span>], [-<span class="number">0.597</span>, -<span class="number">0.934</span>]])</span><br><span class="line">g_y_vec = np.array([<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcLH</span>(<span class="params">id1,id2</span>):</span><br><span class="line">  <span class="keyword">if</span> g_y_vec[id1] == g_y_vec[id2]:</span><br><span class="line">    L = <span class="built_in">max</span>(<span class="number">0</span>,g_alpha[id1]+g_alpha[id2]-g_C)</span><br><span class="line">    H = <span class="built_in">min</span>(g_C,g_alpha[id1]+g_alpha[id2])</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    L = <span class="built_in">max</span>(<span class="number">0</span>,g_alpha[id2]-g_alpha[id1])</span><br><span class="line">    H = <span class="built_in">min</span>(g_C,g_C+g_alpha[id2]-g_alpha[id1])</span><br><span class="line">  <span class="keyword">return</span> (L,H)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">svmOutput</span>(<span class="params">id1</span>):</span><br><span class="line">  <span class="comment"># 这个函数是svm的实际输出，计算当前参数(w,b)下, 计算得到的y</span></span><br><span class="line">  <span class="comment"># 由于w = sum (alpha*y*x), 对于第i个分量x_i所以输出结果应该为w*x_i+b, 也就是 sum (alpha*y*&lt;x_1-m,x_i&gt;)</span></span><br><span class="line">  <span class="comment"># 注: 待会分析下alpha的变化趋势与C的关系</span></span><br><span class="line">  <span class="keyword">global</span> g_b</span><br><span class="line">  <span class="built_in">sum</span> = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">    <span class="built_in">sum</span> += g_alpha[i]*g_y_vec[i]*kernel(i,id1)</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">sum</span>+g_b</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kernel</span>(<span class="params">id1,id2</span>):</span><br><span class="line">  <span class="comment"># 这里核函数为简单的内积</span></span><br><span class="line">  <span class="comment"># id1和id2为int类型，是g_x_mat中的索引</span></span><br><span class="line">  x_1 = g_x_mat[id1]</span><br><span class="line">  x_2 = g_x_mat[id2]</span><br><span class="line">  val = np.subtract(x_1,x_2)</span><br><span class="line">  val = np.dot(val,val)</span><br><span class="line">  <span class="keyword">return</span> np.exp(-val)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compareFun</span>(<span class="params">id1,id2,L,H</span>):</span><br><span class="line">  <span class="comment"># 如果返回1，表示L处去极小值。如果返回-1，H处去极小值。如果是0，表示这次不更新</span></span><br><span class="line">  <span class="comment"># 如果两者相等，这里略过, 说明无法有强力证据证明这个样本属于wx+b&gt;1还是wx+b&lt;1，所以等待下一轮迭代。因此，与L和H相等应该设置一个阀值，判断近似相等。</span></span><br><span class="line">  <span class="keyword">global</span> g_b</span><br><span class="line">  y_1 = g_y_vec[id1]</span><br><span class="line">  y_2 = g_y_vec[id2]</span><br><span class="line">  e_1 = g_err[id1]</span><br><span class="line">  e_2 = g_err[id2]</span><br><span class="line">  alpha_1 = g_alpha[id1]</span><br><span class="line">  alpha_2 = g_alpha[id2]</span><br><span class="line">  k11 = kernel(id1,id1)</span><br><span class="line">  k12 = kernel(id1,id2)</span><br><span class="line">  k22 = kernel(id2,id2)</span><br><span class="line">  s = y_1*y_2</span><br><span class="line">  f_1 = y_1*(e_1+g_b)-alpha_1*k11-s*alpha_2*k12</span><br><span class="line">  f_2 = y_2*(e_2+g_b)-s*alpha_1*k12-alpha_2*k22</span><br><span class="line">  L_1 = alpha_1+s*(alpha_2-L)</span><br><span class="line">  H_1 = alpha_1+s*(alpha_2-H)</span><br><span class="line">  phi_l = L_1*f_1+L*f_2+<span class="number">0.5</span>*L_1*L_1*k11+<span class="number">0.5</span>*L*L*k22+s*L*L_1*k12</span><br><span class="line">  phi_h = H_1*f_1+H*f_2+<span class="number">0.5</span>*H_1*H_1*k11+<span class="number">0.5</span>*H*H*k22+s*H*H_1*k12</span><br><span class="line">  <span class="keyword">if</span> L==H:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  <span class="keyword">if</span> phi_l &lt; phi_h:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">takeStep</span>(<span class="params">id1,id2,err</span>):</span><br><span class="line">  <span class="keyword">if</span> id1==id2:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  alpha_1 = g_alpha[id1]</span><br><span class="line">  alpha_2 = g_alpha[id2]</span><br><span class="line">  y_1 = g_y_vec[id1]</span><br><span class="line">  y_2 = g_y_vec[id2]</span><br><span class="line">  e1 = g_err[id1]</span><br><span class="line">  e2 = g_err[id2]</span><br><span class="line">  s = y_1*y_2</span><br><span class="line">  L,H=calcLH(id1,id2)</span><br><span class="line">  <span class="comment">#print(&quot;id1=&quot;,id1,&quot;, id2=&quot;,id2,&quot; L=&quot;,L,&quot;, H=&quot;,H)</span></span><br><span class="line">  <span class="keyword">if</span> L==H :</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  k11 = kernel(id1,id1)</span><br><span class="line">  k12 = kernel(id1,id2)</span><br><span class="line">  k22 = kernel(id2,id2)</span><br><span class="line">  eta = k11+k22-<span class="number">2</span>*k12</span><br><span class="line">  <span class="comment">#print(&quot;kernel:&quot;,k11,k12,k22,&quot;, eta=&quot;,eta,&quot;e1=&quot;,e1,&quot;e2=&quot;,e2)</span></span><br><span class="line"></span><br><span class="line">  alpha_2_new = alpha_2</span><br><span class="line">  <span class="comment"># 如果eta大于0, 我们可知最小值在边界或极小值点上。事实上，如果极小值不在范围内，必在距离极小值近的那个边界上。</span></span><br><span class="line">  <span class="comment"># 如果eta小于0, 我们可知最小值则必在边界上。我们只需要比较两个边界点函数的大小即可。</span></span><br><span class="line">  <span class="keyword">if</span> eta&gt;<span class="number">0</span> :</span><br><span class="line">    alpha_2_new = alpha_2+y_2*(e1-e2)/eta</span><br><span class="line">    alpha_2_new = <span class="built_in">max</span>(alpha_2_new,L)</span><br><span class="line">    alpha_2_new = <span class="built_in">min</span>(alpha_2_new,H)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># 由于我们知道可以直接这是一个关于alpha的二次函数，并且自变量的取值范围是[L,H], 事实上我们只需要比较alpha_2_new离L和H哪个远即可</span></span><br><span class="line">    <span class="comment"># 但是考虑到eta=0的一次函数特殊情况，我们还是老老实实的计算函数值吧。</span></span><br><span class="line">    ret = compareFun(id1, id2, L, H)</span><br><span class="line">    <span class="keyword">if</span> ret == <span class="number">0</span>:</span><br><span class="line">      alpha_2_new = alpha_2</span><br><span class="line">    <span class="keyword">elif</span> ret == <span class="number">1</span>:</span><br><span class="line">      alpha_2_new = L</span><br><span class="line">    <span class="keyword">elif</span> ret == <span class="number">1</span>:</span><br><span class="line">      alpha_2_new = H</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;----------------eta&lt;=0----------------&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 归整化alpha_2_new</span></span><br><span class="line">  <span class="keyword">if</span> alpha_2_new &lt; err:</span><br><span class="line">    alpha_2_new = <span class="number">0</span></span><br><span class="line">  <span class="keyword">elif</span> alpha_2_new &gt; g_C - err:</span><br><span class="line">    alpha_2_new = g_C</span><br><span class="line">  <span class="keyword">if</span> <span class="built_in">abs</span>(alpha_2_new-alpha_2) &lt; err:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;alpha_2 is no need to update&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># update b</span></span><br><span class="line">  <span class="keyword">global</span> g_b</span><br><span class="line">  alpha_1_new = alpha_1 + s * (alpha_2 - alpha_2_new)      <span class="comment"># 不必担心alpha_1_new不在[0,C]范围内，之前的公式已经保证了</span></span><br><span class="line">  b1_new = -e1-y_1*k11*(alpha_1_new-alpha_1)-y_2*k12*(alpha_2_new-alpha_2) + g_b</span><br><span class="line">  b2_new = -e2-y_1*k12*(alpha_1_new-alpha_1)-y_2*k22*(alpha_2_new-alpha_2) + g_b</span><br><span class="line">  <span class="keyword">if</span> alpha_1_new&gt;<span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> alpha_1_new&lt;g_C:</span><br><span class="line">    g_b = b1_new</span><br><span class="line">  <span class="keyword">elif</span> alpha_2_new&gt;<span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> alpha_2_new&lt;g_C:</span><br><span class="line">    g_b = b2_new</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    g_b = <span class="number">0.5</span>*(b1_new+b2_new)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># update alpha</span></span><br><span class="line">  g_alpha[id1] = alpha_1_new</span><br><span class="line">  g_alpha[id2] = alpha_2_new</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;g_alpha[id1]=&quot;</span>,g_alpha[id1],<span class="string">&quot;g_alpha[id2]=&quot;</span>,g_alpha[id2],<span class="string">&quot;s=&quot;</span>,s,<span class="string">&quot;, alpha_1=&quot;</span>,alpha_1,<span class="string">&quot;, alpha_2=&quot;</span>,alpha_2)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># update err g_y_now</span></span><br><span class="line">  updateYAndErr()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updateYAndErr</span>():</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">    g_y_now[i] = svmOutput(i)</span><br><span class="line">    g_err[i] = svmOutput(i)-g_y_vec[i]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chooseBestAlphaIndex</span>(<span class="params">id2</span>):</span><br><span class="line">  maxIncr = <span class="number">0</span></span><br><span class="line">  maxIndex = -<span class="number">1</span>;</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">    incr = <span class="built_in">abs</span>(g_err[i]-g_alpha[i])</span><br><span class="line">    <span class="keyword">if</span> incr &gt;= maxIncr:</span><br><span class="line">      maxIndex = i</span><br><span class="line">      maxIncr = incr</span><br><span class="line">  <span class="keyword">return</span> maxIndex</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sizeOfNonZerorAndNonC</span>():</span><br><span class="line">  size=<span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">    <span class="keyword">if</span> g_alpha[i]!=<span class="number">0</span> <span class="keyword">and</span> g_alpha[i]!=g_C:</span><br><span class="line">      size= size+<span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> size</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chooseRandomIndex</span>(<span class="params">id2</span>):</span><br><span class="line">  ret = id2;</span><br><span class="line">  <span class="keyword">while</span> ret==id2:</span><br><span class="line">    ret = random.randint(<span class="number">0</span>,g_m-<span class="number">1</span>)</span><br><span class="line">  <span class="keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于examineExample函数,我们一次进选择id2样本对应的alpha与如下规则选择的id1对应的alpha,然后相应跟新其值。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">examineExample</span>(<span class="params">id2</span>):</span><br><span class="line">  y_2 = g_y_vec[id2]</span><br><span class="line">  tol = <span class="number">1e-2</span>        <span class="comment"># 是一个正数</span></span><br><span class="line">  alpha_2 = g_alpha[id2]</span><br><span class="line">  e_2 = svmOutput(id2)-g_y_vec[id2]</span><br><span class="line">  r_2 = e_2 * y_2</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 我们只对当前违反kkt条件的样本对应的alpha进行更新</span></span><br><span class="line">  <span class="comment"># 关于违反kkt条件的说明:</span></span><br><span class="line">  <span class="comment"># (1) r_2 &lt; -tol 表示r_2小于0，即表示输出的预测结果与样本的y符号相反，因此应该属于误差案例的。所以，根据公式，alpha = C。如果alpha &lt; C ,比违反kkt条件</span></span><br><span class="line">  <span class="comment"># (2) r_2 &lt; -tol 表示r_2大于0，即表示输出的预测结果与样本的y符号相同，当误差大于一定的</span></span><br><span class="line">  <span class="keyword">if</span> r_2 &lt; -tol <span class="keyword">and</span> alpha_2 &lt; g_C <span class="keyword">or</span> r_2 &gt; tol <span class="keyword">and</span> alpha_2 &gt; <span class="number">0</span> :</span><br><span class="line">    <span class="comment"># 下面的程序逻辑是这样的:</span></span><br><span class="line">    <span class="comment"># 先遍历alpha非0或非C, 因为我们对于alpha为0和alpha为C的情况, 认为是处于非支持向量和处于误差样本的情况。我们只有根据支持向量下，找到最优的||w||才有意义</span></span><br><span class="line">    <span class="comment"># 首先，我们找到|e1-e2|最大的alpha, 从这里优化。如果优化结果不理想，我们就随机找一个alpha一起计算。如果还不行，就在整个范围alpha范围内计算</span></span><br><span class="line">    <span class="keyword">if</span> sizeOfNonZerorAndNonC()&gt;<span class="number">0</span>:</span><br><span class="line">      id1=chooseBestAlphaIndex(id2)</span><br><span class="line">      <span class="keyword">if</span> takeStep(id1,id2,<span class="number">1e-3</span>):</span><br><span class="line">        <span class="comment">#print(&quot;takeStep1, alpha[&quot;,id1,&quot;]=&quot;,g_alpha[id1],&quot;, alphapp[&quot;,id2,&quot;]=&quot;,g_alpha[id2])</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    r=chooseRandomIndex(id2)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">      id1 = (r+i)%g_m</span><br><span class="line">      <span class="keyword">if</span> id1!=id2 <span class="keyword">and</span> g_alpha[id1]!=<span class="number">0</span> <span class="keyword">and</span> g_alpha[id1]!=g_C:</span><br><span class="line">        <span class="keyword">if</span> takeStep(id1,id2,<span class="number">1e-3</span>):</span><br><span class="line">          <span class="comment">#print(&quot;takeStep2, alpha[&quot;, id1, &quot;]=&quot;, g_alpha[id1], &quot;, alphapp[&quot;, id2, &quot;]=&quot;, g_alpha[id2])</span></span><br><span class="line">          <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    r = chooseRandomIndex(id2)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">      id1 = (r + i) % g_m</span><br><span class="line">      <span class="keyword">if</span> id1 != id2:</span><br><span class="line">        <span class="keyword">if</span> takeStep(id1,id2,<span class="number">1e-3</span>):</span><br><span class="line">          <span class="comment">#print(&quot;takeStep3, alpha[&quot;, id1, &quot;]=&quot;, g_alpha[id1], &quot;, alphapp[&quot;, id2, &quot;]=&quot;, g_alpha[id2])</span></span><br><span class="line">          <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kernel_test</span>(<span class="params">id1,x</span>):</span><br><span class="line">  <span class="comment"># 这里核函数为简单的内积</span></span><br><span class="line">  <span class="comment"># id1和id2为int类型，是g_x_mat中的索引</span></span><br><span class="line">  x_1 = g_x_mat[id1]</span><br><span class="line">  val = np.subtract(x_1,x)</span><br><span class="line">  val = np.dot(val,val)</span><br><span class="line">  <span class="keyword">return</span> np.exp(-val)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">svmOutput_test</span>(<span class="params">alpha,b,x</span>):</span><br><span class="line">  <span class="comment"># 这个函数是svm的实际输出，计算当前参数(w,b)下, 计算得到的y</span></span><br><span class="line">  <span class="comment"># 由于w = sum (alpha*y*x), 对于第i个分量x_i所以输出结果应该为w*x_i+b, 也就是 sum (alpha*y*&lt;x_1-m,x_i&gt;)</span></span><br><span class="line">  <span class="comment"># 注: 待会分析下alpha的变化趋势与C的关系</span></span><br><span class="line">  <span class="keyword">global</span> g_m</span><br><span class="line">  <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">    <span class="keyword">if</span> alpha[i]!=<span class="number">0</span>:</span><br><span class="line">      <span class="built_in">sum</span> += alpha[i]*g_y_vec[i]*kernel_test(i,x)</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">sum</span>+\</span><br><span class="line">         b</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">showPic</span>(<span class="params">alpha,b</span>):</span><br><span class="line">  figure, ax = plt.subplots()</span><br><span class="line">  ax.set_xlim(left=-<span class="number">2</span>, right=<span class="number">2</span>)</span><br><span class="line">  ax.set_ylim(bottom=-<span class="number">2</span>, top=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 生成100组测试数据</span></span><br><span class="line">  x=[]</span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    x_0 = random.randint(-<span class="number">1200</span>,<span class="number">1200</span>)/<span class="number">1000</span></span><br><span class="line">    x_1 = random.randint(-<span class="number">1200</span>,<span class="number">1200</span>)/<span class="number">1000</span></span><br><span class="line">    x.append([x_0,x_1])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">if</span> svmOutput_test(alpha,b,x[i])&gt;<span class="number">0</span>:</span><br><span class="line">      plt.plot(x[i][<span class="number">0</span>], x[i][<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;+&#x27;</span>, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      plt.plot(x[i][<span class="number">0</span>], x[i][<span class="number">1</span>], <span class="string">&#x27;b--&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, color=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># draw x_0*x_0+x_1*x_1=0</span></span><br><span class="line">  cir1 = Circle(xy=(<span class="number">0.0</span>, <span class="number">0.0</span>), radius=<span class="number">1</span>)</span><br><span class="line">  ax.add_patch(cir1)</span><br><span class="line"></span><br><span class="line">  plt.xlabel(<span class="string">&quot;x1&quot;</span>)</span><br><span class="line">  plt.ylabel(<span class="string">&quot;x2&quot;</span>)</span><br><span class="line">  plt.plot()</span><br><span class="line">  plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">  SHOW_PIC = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">  g_m = <span class="built_in">len</span>(g_x_mat)</span><br><span class="line">  <span class="keyword">if</span> SHOW_PIC == <span class="literal">False</span>:</span><br><span class="line">    <span class="comment"># 1 training</span></span><br><span class="line">    g_alpha = np.zeros(g_m)</span><br><span class="line">    g_y_now = np.zeros(g_m)</span><br><span class="line">    g_err = np.zeros(g_m)</span><br><span class="line">    <span class="keyword">global</span> g_b</span><br><span class="line">    g_b = <span class="number">0</span></span><br><span class="line">    numChanged = <span class="number">0</span></span><br><span class="line">    examineAll = <span class="number">1</span></span><br><span class="line">    g_C = <span class="number">10</span></span><br><span class="line">    err = <span class="number">0</span></span><br><span class="line">    updateYAndErr()       <span class="comment"># 实现更新下缓冲，即当前输出与误差值</span></span><br><span class="line">    <span class="keyword">while</span> numChanged&gt;<span class="number">0</span> <span class="keyword">or</span> examineAll:</span><br><span class="line">      numChanged = <span class="number">0</span></span><br><span class="line">      <span class="comment"># 循环处理，第一次对所有的样本进行一次处理。</span></span><br><span class="line">      <span class="comment"># 然后对所有非边界的数值进行处理。因为在当前参数下，非边界的样本，我们认为其是支持向量。对于优化||w||的大小，支持向量才有意义。</span></span><br><span class="line">      <span class="keyword">if</span> examineAll:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">          numChanged += examineExample(i)</span><br><span class="line">          <span class="comment">#print(&quot;examineAll=1, numChanged=&quot;,numChanged)</span></span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g_m):</span><br><span class="line">          <span class="keyword">if</span> g_alpha[i]!=<span class="number">0</span> <span class="keyword">and</span> g_alpha[i]!=g_C:</span><br><span class="line">            numChanged += examineExample(i)</span><br><span class="line">      examineAll = <span class="built_in">abs</span>(examineAll-<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(g_alpha, g_b)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># 3 show</span></span><br><span class="line">    alpha= [  <span class="number">0.00000000e+00</span>,   <span class="number">7.59410972e-01</span>,  -<span class="number">2.22044605e-16</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">1.00000000e+01</span>,   <span class="number">1.00000000e+01</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">1.00000000e+01</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">5.67018243e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">1.00000000e+01</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">1.00000000e+01</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">2.45788486e+00</span>,   <span class="number">1.33286169e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">1.11022302e-16</span>,   <span class="number">1.00000000e+01</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">7.22541284e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,  -<span class="number">1.38777878e-17</span>,   <span class="number">0.00000000e+00</span>,  -<span class="number">2.77555756e-17</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">8.66273771e-04</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">8.33442222e+00</span>,</span><br><span class="line">    <span class="number">1.00000000e+01</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">1.00000000e+01</span>,   <span class="number">1.00000000e+01</span>,  -<span class="number">2.77555756e-17</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">1.00000000e+01</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">4.39461998e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">1.00000000e+01</span>,   <span class="number">1.00000000e+01</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,  -<span class="number">2.22044605e-16</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">1.00000000e+01</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,  -<span class="number">2.77555756e-17</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    -<span class="number">5.55111512e-17</span>,  <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">6.59194921e-17</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">1.00000000e+01</span>,   <span class="number">1.00000000e+01</span>,  -<span class="number">1.38777878e-17</span>,</span><br><span class="line">    <span class="number">5.21868376e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,  -<span class="number">2.22044605e-16</span>,   <span class="number">6.56986459e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">5.19744490e+00</span>,   <span class="number">9.69510083e+00</span>,   <span class="number">1.00000000e+01</span>,</span><br><span class="line">    <span class="number">1.00000000e+01</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">0.00000000e+00</span>,</span><br><span class="line">    <span class="number">0.00000000e+00</span>,   <span class="number">8.69782724e+00</span>,   <span class="number">0.00000000e+00</span>,   <span class="number">1.00000000e+01</span>,</span><br><span class="line">    <span class="number">3.24960991e+00</span>,   <span class="number">8.23041098e+00</span>]</span><br><span class="line">    b = -<span class="number">2.68518972087</span></span><br><span class="line">    <span class="comment">#w = np.array([0, 0])</span></span><br><span class="line">    <span class="comment">#for i in range(g_m):</span></span><br><span class="line">    <span class="comment">#  if g_alpha[i] != 0:</span></span><br><span class="line">    <span class="comment">#    w = np.add(w, g_y_vec[i] * g_alpha[i] * g_x_mat[i])</span></span><br><span class="line">    showPic(alpha,b)</span><br></pre></td></tr></table></figure>

<p>经过数轮迭代之后，得到参数。然后在随机产生一些样本，通过训练集得到参数对随机测试样本进行分割，结果如下，发现分割效果还是很理想的。</p>
<img src="/images/机器学习/监督学习-支持向量机计算结果2.png" width=50% height=50% text-align=center/>

<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>手写公式和word版博客地址:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/zhengchenyu/MyBlog/tree/master/doc/mlearning/支持向量机</span><br></pre></td></tr></table></figure>

<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>cs229-notes3</li>
<li>机器学习 周志华版</li>
<li>Fast Training of Support Vector Machines Using Sequential Minimal Optimization</li>
<li><a target="_blank" rel="noopener" href="http://blog.csdn.net/luoshixian099/article/details/51227754">http://blog.csdn.net/luoshixian099/article/details/51227754</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2017/09/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-7-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" data-id="cuidlluekJvJsBA0Osrh_l-Wb" data-title="机器学习-2.7-监督学习之支持向量机" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/09/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-3-1-%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%81%9A%E7%B1%BB/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          机器学习-3.1-非监督学习之聚类.md
        
      </div>
    </a>
  
  
    <a href="/2017/08/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-6-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">机器学习-2.6-监督学习之朴素贝叶斯算法</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/02/">February 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/02/07/ErasuceCode%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/12/25/RSS-%E8%BF%9C%E7%A8%8BMerge%E7%9A%84%E8%AE%BE%E8%AE%A1/">RSS-远程Merge的设计</a>
          </li>
        
          <li>
            <a href="/2023/12/25/RSS-%E8%BF%9C%E7%A8%8BMerge%E7%9A%84%E8%AE%BE%E8%AE%A1EN/">RSS-Remote Merge Design</a>
          </li>
        
          <li>
            <a href="/2017/10/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-3-2-%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/">机器学习-3.2-非监督学习之主成分分析.md</a>
          </li>
        
          <li>
            <a href="/2017/10/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-8-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8Bk%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/">机器学习-2.8-监督学习之k近邻算法</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 zhengchenyu<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>