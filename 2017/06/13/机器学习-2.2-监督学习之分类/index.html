<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>机器学习 2.2 监督学习之逻辑回归 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="监督学习本章继续研究监督学习的问题。 1 回归问题我们随机制造一个样本空间\({x _2} \geqslant 2{x _1} + 1\)，对于对空间进行分类。具体例子如下：   对于这些样本，假定可以通过二维线性曲线进行分类。然后模拟出某条曲线，使样本得到一个很好的分类。 这个分类问题的样本的y值只有0或1(0-1分布或伯努利分布)，即表示在直线上面或直线下面。如果用之前的线性回归结果将非常">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习 2.2 监督学习之逻辑回归">
<meta property="og:url" content="http://example.com/2017/06/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2.2-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%88%86%E7%B1%BB/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="监督学习本章继续研究监督学习的问题。 1 回归问题我们随机制造一个样本空间\({x _2} \geqslant 2{x _1} + 1\)，对于对空间进行分类。具体例子如下：   对于这些样本，假定可以通过二维线性曲线进行分类。然后模拟出某条曲线，使样本得到一个很好的分类。 这个分类问题的样本的y值只有0或1(0-1分布或伯努利分布)，即表示在直线上面或直线下面。如果用之前的线性回归结果将非常">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E6%A0%B7%E6%9C%AC.png">
<meta property="og:image" content="http://example.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E8%BE%85%E5%8A%A9%E5%87%BD%E6%95%B0.png">
<meta property="og:image" content="http://example.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E8%AE%A1%E7%AE%97%E7%BB%93%E6%9E%9C1.png">
<meta property="og:image" content="http://example.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E8%AE%A1%E7%AE%97%E7%BB%93%E6%9E%9C2.png">
<meta property="og:image" content="http://example.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%9B%E9%A1%BF%E6%B3%95%E7%A4%BA%E6%84%8F%E5%9B%BE.png">
<meta property="og:image" content="http://example.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E8%AE%A1%E7%AE%97%E7%BB%93%E6%9E%9C3.png">
<meta property="article:published_time" content="2017-06-13T13:20:17.000Z">
<meta property="article:modified_time" content="2025-02-07T08:06:18.199Z">
<meta property="article:author" content="zhengchenyu">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E6%A0%B7%E6%9C%AC.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-机器学习-2.2-监督学习之分类" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2017/06/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2.2-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%88%86%E7%B1%BB/" class="article-date">
  <time class="dt-published" datetime="2017-06-13T13:20:17.000Z" itemprop="datePublished">2017-06-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      机器学习 2.2 监督学习之逻辑回归
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h1><p>本章继续研究监督学习的问题。</p>
<h2 id="1-回归问题"><a href="#1-回归问题" class="headerlink" title="1 回归问题"></a>1 回归问题</h2><p>我们随机制造一个样本空间\({x _2} \geqslant 2{x _1} + 1\)，对于对空间进行分类。具体例子如下：</p>
<img src="/images/机器学习/监督学习-分类问题样本.png" width=50% height=50% text-align=center/>

<p>对于这些样本，假定可以通过二维线性曲线进行分类。然后模拟出某条曲线，使样本得到一个很好的分类。</p>
<p>这个分类问题的样本的y值只有0或1(0-1分布或伯努利分布)，即表示在直线上面或直线下面。如果用之前的线性回归结果将非常糟糕。因为对于分类问题，线性回归得到的线性曲线，得到的\(y &gt; 1\)或\(y &lt; 0\)这些是没有意义的，肯定是错误的。因此引入了logistic函数，转换为Logistic regression，logistic函数如下：</p>

$$h(z) = \frac{1}{{1 + {e^{ - z}}}}$$


<p>函数曲线如下：</p>
<img src="/images/机器学习/监督学习-分类问题辅助函数.png" width=50% height=50% text-align=center/>

<p>如果我们设置\(z &#x3D; {\theta _1}{x _1} + {\theta _0} - {x _2}\)。因此，对于分类问题就可以转化为：对于\({x _2} \geqslant {\theta _1}{x _1} + {\theta _0}\)就可以转化为h(z)趋向于0的程度，对于\({x _2} \leqslant {\theta _1}{x _1} + {\theta _0}\)就可以转化为h(z)趋向于1的程度。我们简化为如下公式:</p>

$$z = {\theta _1}{x _1} + {\theta _0}{x _0} + {\theta _2}{x _2} = {\theta ^T}x$$


<p>其中,\({x _0} &#x3D; 1\)，\({\theta _2} &#x3D;  - 1\)。</p>
<h2 id="2-回归问题推导"><a href="#2-回归问题推导" class="headerlink" title="2 回归问题推导"></a>2 回归问题推导</h2><p>我们求辅助函数的导数，以备后来使用：</p>

$$h'(z) = (\frac{1}{{1 + {e^{ - z}}}})' =  - \frac{{ - {e^{ - z}}}}{{{{(1 + {e^{ - z}})}^2}}} = \frac{1}{{1 + {e^{ - z}}}}(1 - 1 + {e^{ - z}}) = h(z)(1 - h(z))$$


<p>接下来我们从公式角度对问题进行建模分析与推导。</p>
<p>这里我们将我们的函数\({h_\theta }(x)\)转化为如下形式:</p>

$${h_\theta }(x) = \frac{1}{{1 + {e^{ - {\theta ^T}x}}}}$$


<p>对于0-1分布，由于\({h_\theta }(x)\)只能为0或1。所以，在已知\(x\)和\({\theta}\)的情况下，我们可以认为有\({h_\theta }(x)\)的概率得到\(y&#x3D;1\)。因此有如下公式：</p>

$$p(y = 1|x,\theta ) = {h_\theta }(x)$$
$$p(y = 0|x,\theta ) = 1 - {h_\theta }(x)$$


<p>综合两个公式有如下公式：</p>

$$p(y|x,\theta ) = {h_\theta }{(x)^y}{(1 - {h _\theta }(x))^{(1 - y)}}$$

<p>即在已经样本的情况下得到准确的\(y\)，即要使用极大释然方法，得到样本概率乘积的最大值,即得到下式的最大值：</p>

$$L(\theta ) = \prod\limits _{i = 1}^m {{h _\theta }{{(x{}^i)}^{{y^i}}}{{(1 - {h _\theta }({x^i}))}^{(1 - {y^i})}}} $$


<p>取自然对数，对其中一个维度\({\theta _j}\)求偏导数，如下：</p>

$$\ell ({\theta _j}) = \frac{{\partial \ln (L(\theta ))}}{{\partial {\theta _j}}} = \frac{\partial } {{\partial {\theta _j}}}(\sum\limits _{i = 1}^m {({y^i}\ln ({h _\theta }(x{}^i)) + (1 - {y^i})} \ln (1 - {h _\theta }(x{}^i))))$$ 


<p>我们使用一个辅助函数\(z &#x3D; {\theta ^T}x\), 并且\(\ln x\)的倒数是\(\frac{1}{x}\)。<br>又由于复合函数有如下求导法则：</p>

$$\frac{{df(g(x))}}{{dx}} = \frac{{df(z)}}{{dz}}g(x)'$$


<p>其中\(z &#x3D; g(x)\)。。(这里再次强调下标识维度，上表是样本编号)。所以有：</p>

$$\ell ({\theta _j}) = \frac{{\partial L(\theta )}}{{\partial {\theta _j}}} = \frac{\partial }{{\partial {\theta _j}}}(\sum\limits _{i = 1}^m {({y^i}\ln ({h _\theta }(x{}^i)) + (1 - {y^i})} \ln (1 - {h _\theta }(x{}^i))))$$


<p>这里设\(z &#x3D; {\theta ^T}x\)，有：</p>

$$\ell ({\theta _j}) = \sum\limits _{i = 1}^m {({y^i}\frac{1}{{h(z)}}h'(z) + (1 - {y^i})} \frac{1}{{1 - h(z)}}( - h'(z)))$$


<p>将之前的倒数公式带入其中,有:</p>

$$\ell ({\theta _j}) = \sum\limits _{i = 1}^m {(x _j^i(y - {h _\theta }({x^i})))} $$


<p>因此这样就可以使用如下的式子学习样本值：</p>

$${\theta _j} = {\theta _j} + \alpha \sum\limits _{i = 1}^m {(x _j^i(y - {h _\theta }({x^i})))} $$


<h2 id="3-代码"><a href="#3-代码" class="headerlink" title="3 代码"></a>3 代码</h2><p>从这一章开始讲代码从matlab转化更为通用的python。</p>
<p>我们选取合适的步长，当\(J({\theta})\)小于合适的阈值(这里配置为4)，就认为迭代完成。分类代码如下:</p>
<pre><code>import random
import math
import numpy as np
from matplotlib import pyplot as plt
k = 2
b = 1
x1_res = [-0.711, -0.2798, 0.4258, 0.4108, 0.0492, -0.842, -0.6036, -0.8394, 0.395, -0.8612, 0.806, 0.1476, 0.5464,
          0.3992,
          0.8326, 0.76, -0.7288, 0.0718, -0.4436, 0.8448, -0.017, -0.108, -0.4032, -0.9678, -0.4322, -0.9024, -0.0146,
          -0.5346, -0.3808, 0.0498, 0.6376, -0.3946, 0.5896, -0.9504, 0.2046, -0.0994, 0.468, -0.9236, 0.0996, 0.4366,
          -0.167, 0.9618, 0.5796, -0.383, 0.0254, -0.367, 0.0734, -0.8434, -0.9948, -0.0188]
x2_res = [-0.5108, -0.1254, 0.6166, 0.5296, 0.5758, -0.5972, 0.3994, 0.9808, 0.7134, -0.9414, -0.098, -0.2778, 0.667,
          0.1916, -0.0298, -0.5124, -0.509, -0.9754, 0.261, 0.4256, -0.7104, 0.8398, 0.7094, -0.5558, -0.799, 0.666,
          0.392,
          0.6688, 0.7832, 0.6372, -0.788, -0.1138, 0.0922, -0.6254, -0.1456, 0.3608, 0.4694, -0.8946, 0.9708, 0.8736,
          -0.5254, 0.0252, 0.6554, -0.2486, 0.9064, -0.35, -0.7724, 0.131, 0.4446, 0.2468]
          
def make_samples(x0_arr,x1_arr,x2_arr,y_arr):
  # classified by x_2 = 2*x_1 + 1 . (x_1,x_2) is the point of 2-D plane
  # y is 0 or 1, means &#39;x_2 &gt;= 2*x_1 + 1 &#39;  or &#39;x_2 &lt; 2*x_1 + 1&#39;
  for i in range(len(x1_res)):
    x0_arr.append(1)
    x1_arr.append(x1_res[i])
    x2_arr.append(x2_res[i])
    y_arr.append(0 if (x2_arr[i] &gt;= k * x1_arr[i] + b ) else 1)

def h(z):
  return 1/(1 + math.exp(-1*z))

def h_xita(x0,x1,x2,xita):
  return h(transvection([x0,x1,x2], xita))

def transvection(a,b):
  return np.dot(a, b)

def J(x0_arr,x1_arr,x2_arr,y_arr,xita):
  sum = 0;
  length = len(x1_arr)
  for i in range(length):
    sum = sum + ( h_xita(x0_arr[i],x1_arr[i],x2_arr[i],xita) - y_arr[0] )**2
  return sum/2

def updateXita(x0_arr,x1_arr,x2_arr,y_arr,xita,a,update_x):
  sum = 0;
  length = len(x0_arr)
  for i in range(length):
    sum = sum + ( y_arr[i] - h_xita(x0_arr[i],x1_arr[i],x2_arr[i],xita) ) * update_x[i]
  return sum*a

def showPic(k,b,point_x1_arr,point_x2_arr):
  ## show line
  X = np.arange(-1,1,0.001)
  Y=[]
  for i in range(X.size):
    Y.append(k*X[i]+b)
  plt.plot(X, Y, &#39;b--&#39;, label=&quot;logistic&quot;)
  ## show points
  for i in range(len(point_x1_arr)):
    if(point_x2_arr[i]&gt;2*point_x1_arr[i]+1):
      plt.plot(point_x1_arr[i],point_x2_arr[i],&#39;b--&#39;,marker = &#39;x&#39;, color = &#39;g&#39;)
    else:
      plt.plot(point_x1_arr[i],point_x2_arr[i],&#39;r--&#39;,marker = &#39;+&#39;, color = &#39;r&#39;)
  plt.xlabel(&quot;x1&quot;)
  plt.ylabel(&quot;x2&quot;)
  plt.figure(figsize=(8, 4))
  plt.show()

if __name__== &quot;__main__&quot;:
  x0_arr = []
  x1_arr = []
  x2_arr = []
  y_arr = []
  make_samples(x0_arr,x1_arr,x2_arr,y_arr)
  xita = [1,20,-1]
  a = 0.01
  limit = 4
  count = 0
  while 1:
    j = J(x0_arr, x1_arr, x2_arr, y_arr, xita)
    print(&quot;j=&quot;,j)
    if j &lt; limit:
      break;
    xita[0] = xita[0] + updateXita(x0_arr, x1_arr, x2_arr, y_arr, xita, a, x0_arr)
    xita[1] = xita[1] + updateXita(x0_arr, x1_arr, x2_arr, y_arr, xita, a, x1_arr)
    count = count + 1
    print(&quot;cout=&quot;,count)
    print(&quot;xita0=&quot;,xita[0],&quot; xita1=&quot;,xita[1])
  showPic(xita[1],xita[0],x1_arr,x2_arr)
  print(xita)
</code></pre>
<p>经过数论迭代分析，得到\( \theta _0 &#x3D; 2.7841420917875297 \)，\( \theta _1 &#x3D; 4.850051144306185 \)。</p>
<p>得到如下结果:</p>
<img src="/images/机器学习/监督学习-分类问题计算结果1.png" width=50% height=50% text-align=center/>

<h2 id="4-代码的改进"><a href="#4-代码的改进" class="headerlink" title="4 代码的改进"></a>4 代码的改进</h2><p>经过试验发现上面的算法的问题是如果选取合适的阈值。这里将算法修改为两次的\(J({\theta})\)的差值小于0.00001，这样就任务函数已经几近收敛,然后结束迭代。另外，如果\(J({\theta})\)的值某一次迭代增大了,说明算法异常或者迭代参数设置过大。修改后的代码如下:</p>
<pre><code>  while 1:                                                                             
    j = J(x0_arr, x1_arr, x2_arr, y_arr, xita)                                         
    print(&quot;j=&quot;,j)                                                                      
    if j &gt; j_last:                                                                     
      print(&quot;算法异常或选取迭代系数过大&quot;)                                                           
      break;                                                                           
    if j_last - j &lt; 0.00001:                                                           
      print(&quot;out &quot;,j_last - j)                                                         
      break;                                                                           
    j_last = j                                                                         
    xita[0] = xita[0] + updateXita(x0_arr, x1_arr, x2_arr, y_arr, xita, a, x0_arr)     
    xita[1] = xita[1] + updateXita(x0_arr, x1_arr, x2_arr, y_arr, xita, a, x1_arr)    
</code></pre>
<p>得到如下结果:</p>
<img src="/images/机器学习/监督学习-分类问题计算结果2.png" width=50% height=50% text-align=center/>

<h2 id="5-牛顿法改进"><a href="#5-牛顿法改进" class="headerlink" title="5 牛顿法改进"></a>5 牛顿法改进</h2><p>这里说明另外一种快速求值的方法，牛顿法。<br><img src="/images/机器学习/牛顿法示意图.png" width=50% height=50% text-align=center/></p>
<p>如上所示为牛顿法的简易过程。我们初始值为\(x _0\)，然后在\((x _0,f(x _0))\)做切线得到\(x _0\)，依次类推会接近于得到\(x _n\),使得\(f(x _n)\)接近于0。具体算法的证明请见文献2。</p>
<p>根据上面的算法，我们可以轻易的推导出</p>

$${x _{i + 1}} = {x _i} - \frac{{f({x _i})}}{{f'({x _i})}}$$


<p>联系之前的问题，我们可以转化快速得到\({\theta _j}\)使得\(\ell ({\theta _j}) &#x3D; 0\)问题。（注: 之前的算法使找到最优下降方向使得\(J({\theta})\)最小，而牛顿法是直接得到最优的\({\theta _j}\)）</p>
<p>本问题的公式如下:</p>

$$\ell ({\theta _j}) = \sum\limits _{i = 1}^m {(x _j^i(y - {h _\theta }({x^i})))} $$



$$\ell '({\theta _j}) =  - \sum\limits _{i = 1}^m {{{(x _j^i)}^2}} $$



$${\theta _j}: = {\theta _j} - \frac{{\ell ({\theta _j})}}{{\ell '({\theta _j})}} = {\theta _j} - \frac{{\sum\limits _{i = 1}^m {(x _j^i(y - {h _\theta }({x^i})))} }}{{ - \sum\limits _{i = 1}^m {{{(x _j^i)}^2}} }}$$


<p>代码如下:</p>
<pre><code>import sys
import math
import numpy as np
from matplotlib import pyplot as plt
k = 2
b = 1
x1_res = [-0.711, -0.2798, 0.4258, 0.4108, 0.0492, -0.842, -0.6036, -0.8394, 0.395, -0.8612, 0.806, 0.1476, 0.5464,
          0.3992,
          0.8326, 0.76, -0.7288, 0.0718, -0.4436, 0.8448, -0.017, -0.108, -0.4032, -0.9678, -0.4322, -0.9024, -0.0146,
          -0.5346, -0.3808, 0.0498, 0.6376, -0.3946, 0.5896, -0.9504, 0.2046, -0.0994, 0.468, -0.9236, 0.0996, 0.4366,
          -0.167, 0.9618, 0.5796, -0.383, 0.0254, -0.367, 0.0734, -0.8434, -0.9948, -0.0188]
x2_res = [-0.5108, -0.1254, 0.6166, 0.5296, 0.5758, -0.5972, 0.3994, 0.9808, 0.7134, -0.9414, -0.098, -0.2778, 0.667,
          0.1916, -0.0298, -0.5124, -0.509, -0.9754, 0.261, 0.4256, -0.7104, 0.8398, 0.7094, -0.5558, -0.799, 0.666,
          0.392,
          0.6688, 0.7832, 0.6372, -0.788, -0.1138, 0.0922, -0.6254, -0.1456, 0.3608, 0.4694, -0.8946, 0.9708, 0.8736,
          -0.5254, 0.0252, 0.6554, -0.2486, 0.9064, -0.35, -0.7724, 0.131, 0.4446, 0.2468]

def make_samples(x0_arr,x1_arr,x2_arr,y_arr):
  # classified by x_2 = 2*x_1 + 1 . (x_1,x_2) is the point of 2-D plane
  # y is 0 or 1, means &#39;x_2 &gt;= 2*x_1 + 1 &#39;  or &#39;x_2 &lt; 2*x_1 + 1&#39;
  for i in range(len(x1_res)):
    x0_arr.append(1)
    x1_arr.append(x1_res[i])
    x2_arr.append(x2_res[i])
    y_arr.append(0 if (x2_arr[i] &gt;= k * x1_arr[i] + b ) else 1)

def h(z):
  return 1/(1 + math.exp(-1*z))

def h_xita(x0,x1,x2,xita):
  return h(transvection([x0,x1,x2], xita))

def transvection(a,b):
  return np.dot(a, b)

def l(x0_arr,x1_arr,x2_arr,y_arr,xita,a,update_x):
  sum = 0;
  for i in range(len(x0_arr)):
    sum = sum + ( y_arr[i] - h_xita(x0_arr[i],x1_arr[i],x2_arr[i],xita) ) * update_x[i]
  return sum*a

def derl(update_x):
  sum = 0;
  for i in range(len(update_x)):
    sum = sum + update_x[i]*update_x[i];
  return -1 * sum

def showPic(k,b,point_x1_arr,point_x2_arr):
  ## show line
  X = np.arange(-1,1,0.001)
  Y=[]
  for i in range(X.size):
    Y.append(k*X[i]+b)
  plt.plot(X, Y, &#39;b--&#39;, label=&quot;logistic&quot;)
  ## show points
  for i in range(len(point_x1_arr)):
    if(point_x2_arr[i]&gt;2*point_x1_arr[i]+1):
      plt.plot(point_x1_arr[i],point_x2_arr[i],&#39;b--&#39;,marker = &#39;x&#39;, color = &#39;g&#39;)
    else:
      plt.plot(point_x1_arr[i],point_x2_arr[i],&#39;r--&#39;,marker = &#39;+&#39;, color = &#39;r&#39;)
  plt.xlabel(&quot;x1&quot;)
  plt.ylabel(&quot;x2&quot;)
  plt.figure(figsize=(8, 4))
  plt.show()

if __name__== &quot;__main__&quot;:
  x0_arr = []
  x1_arr = []
  x2_arr = []
  y_arr = []
  make_samples(x0_arr,x1_arr,x2_arr,y_arr)
  xita = [1,20,-1]
  xita_bak = [10000,10000]
  count = 0
  while 1:
    xita[0] = xita[0] - l(x0_arr, x1_arr, x2_arr, y_arr, xita, 1,x0_arr)/derl(x0_arr)
    xita[1] = xita[1] - l(x0_arr, x1_arr, x2_arr, y_arr, xita, 1, x1_arr) / derl(x1_arr)
    if abs(l(x0_arr, x1_arr, x2_arr, y_arr, xita, 1,x0_arr)) &lt;= 0.001 :
      print(&quot;xita = &quot;,xita)
      break;
    xita_bak[1]=xita[1]
    xita_bak[0]=xita[0]
    count = count + 1
    print(&quot;count=&quot;, count, &quot; xita = &quot;, xita)

  showPic(xita[1],xita[0],x1_arr,x2_arr)
  print(xita)
</code></pre>
<p>得到\({\theta _0}\)和\({\theta _1}\)分别为<br>2.7762287194293407, 4.835356147838368。得到的曲线如下：</p>
<img src="/images/机器学习/监督学习-分类问题计算结果3.png" width=50% height=50% text-align=center/>

<h2 id="6-参考文献"><a href="#6-参考文献" class="headerlink" title="6 参考文献"></a>6 参考文献</h2><ul>
<li>1 &lt;&lt; cs229-notes1 &gt;&gt;</li>
<li>2 &lt;&lt; 计算机科学计算 &gt;&gt; 施吉林,张宏伟，金光日编</li>
</ul>
<blockquote>
<p>未来代码将会维护在<a target="_blank" rel="noopener" href="https://github.com/zhengchenyu/mlearning">https://github.com/zhengchenyu/mlearning</a></p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2017/06/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2.2-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%88%86%E7%B1%BB/" data-id="cm6uj1jcm000c7rrmcimk1e0d" data-title="机器学习 2.2 监督学习之逻辑回归" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/07/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2.3-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%80%9A%E7%94%A8%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          机器学习-2.3-监督学习之通用线性模型
        
      </div>
    </a>
  
  
    <a href="/2017/06/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2.1-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">机器学习 2.1 监督学习之线性回归</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/02/">February 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/02/07/2024-04-22/">(no title)</a>
          </li>
        
          <li>
            <a href="/2025/02/07/ErasuceCode%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/12/25/RSS-%E8%BF%9C%E7%A8%8BMerge%E7%9A%84%E8%AE%BE%E8%AE%A1/">RSS-远程Merge的设计</a>
          </li>
        
          <li>
            <a href="/2023/12/25/RSS-%E8%BF%9C%E7%A8%8BMerge%E7%9A%84%E8%AE%BE%E8%AE%A1EN/">RSS-Remote Merge Design</a>
          </li>
        
          <li>
            <a href="/2017/10/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-3-2-%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/">机器学习-3.2-非监督学习之主成分分析.md</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 zhengchenyu<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>