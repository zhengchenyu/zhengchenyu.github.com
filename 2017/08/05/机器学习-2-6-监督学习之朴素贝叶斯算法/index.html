<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>机器学习-2.6-监督学习之朴素贝叶斯算法 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="朴素贝叶斯算法1 朴素贝叶斯算法1.1 垃圾邮件处理模型以预测垃圾邮件为例子，介绍贝叶斯算法。词库中有5000个单词。我们有输入\(x\)，\(x \in {\{ 0,1\} ^{50000}}\)，是一个50000维的向量，其中\(x _i\)表示词库中第\(i\)个单次在该邮件中出现，为\(0\)表示词库中第\(i\)个单次在该邮件中没有出现。 已经邮件是否为邮件,样本\(({x _1},">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习-2.6-监督学习之朴素贝叶斯算法">
<meta property="og:url" content="http://example.com/2017/08/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-6-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="朴素贝叶斯算法1 朴素贝叶斯算法1.1 垃圾邮件处理模型以预测垃圾邮件为例子，介绍贝叶斯算法。词库中有5000个单词。我们有输入\(x\)，\(x \in {\{ 0,1\} ^{50000}}\)，是一个50000维的向量，其中\(x _i\)表示词库中第\(i\)个单次在该邮件中出现，为\(0\)表示词库中第\(i\)个单次在该邮件中没有出现。 已经邮件是否为邮件,样本\(({x _1},">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2017-08-05T13:11:31.000Z">
<meta property="article:modified_time" content="2025-02-07T08:06:18.199Z">
<meta property="article:author" content="zhengchenyu">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<meta name="generator" content="Hexo 8.1.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-机器学习-2-6-监督学习之朴素贝叶斯算法" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2017/08/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-6-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/" class="article-date">
  <time class="dt-published" datetime="2017-08-05T13:11:31.000Z" itemprop="datePublished">2017-08-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      机器学习-2.6-监督学习之朴素贝叶斯算法
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <script type="text/javascript" src="/Users/zcy/Desktop/study/git/MathJax-master/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="朴素贝叶斯算法"><a href="#朴素贝叶斯算法" class="headerlink" title="朴素贝叶斯算法"></a>朴素贝叶斯算法</h1><h2 id="1-朴素贝叶斯算法"><a href="#1-朴素贝叶斯算法" class="headerlink" title="1 朴素贝叶斯算法"></a>1 朴素贝叶斯算法</h2><h3 id="1-1-垃圾邮件处理模型"><a href="#1-1-垃圾邮件处理模型" class="headerlink" title="1.1 垃圾邮件处理模型"></a>1.1 垃圾邮件处理模型</h3><p>以预测垃圾邮件为例子，介绍贝叶斯算法。词库中有5000个单词。我们有输入\(x\)，\(x \in {\{ 0,1\} ^{50000}}\)，是一个50000维的向量，其中\(x _i\)表示词库中第\(i\)个单次在该邮件中出现，为\(0\)表示词库中第\(i\)个单次在该邮件中没有出现。</p>
<p>已经邮件是否为邮件,样本\(({x _1},{x _2},…,{x _i})\)出现的概率为\(p({x _1},{x _2},…,{x _i}|y)\)，可以定义为下式:</p>

$$p({x _1},{x _2},...,{x _i}|y) = p({x _1}|y)p({x _2}|y,{x _1})...p({x _{50000}}|y,{x _1},...,{x _{49999}})$$


<p>注: 上式很好理解。右侧第一个式子为已知\(y\)，\(x _1\)出现的概率。第二个为一直\(y\)和\(x _1\)出现\(x _2\)的概率。因此两个式子相乘表示一直\(y\)初选\((x1,x2)\)的概率。依次类推，可以得到这个公式。</p>

$$p({x _1},{x _2},...,{x _i}|y) = p({x _1}|y)p({x _2}|y)...p({x _{50000}}|y) = \prod\limits _{i = 1}^{50000} {p({x _i}|y)}$$



<h3 id="1-2-公式推导"><a href="#1-2-公式推导" class="headerlink" title="1.2 公式推导"></a>1.2 公式推导</h3><p>关于最大释然函数如何设置，我们需要知道我们的目标是找到参数\(\theta\)，使得训练时候在给定\(x\)的情况下，得到最准确\(y\)的概率最大，最大释然函数就是这些概率的连乘，具体是需要保证下面的式子最大:</p>

$$\ln \prod\limits _{i = 1}^m {p(y = {y^i}|x = {x^i};\theta )p(x = {x^i})} $$


<p>在很多教材中都介绍最大释然函数是如下的表达，实际上是一样的。下面的式子的直接意义是在\(\theta\)已知情况下，出现\((x^i,y^i)\)的概率，实际上面的分析过程是一样的。</p>

$$\ln \prod\limits _{i = 1}^m {p(y = {y^i},x = {x^i};\theta )} $$


<blockquote>
<p>后面为了简写后面省略了\(\theta\)。</p>
</blockquote>
<p>我们做如下设置:</p>

$${\phi _y} = p(y = 1)$$
$${\phi _{i|y = 1}} = p({x _i} = 1|y = 1)$$
$${\phi _{i|y = 0}} = p({x _i} = 1|y = 0)$$


<p>然后得到最大释然函数:</p>

$$\ell ({\phi _y},{\phi _{i|y = 1}},{\phi _{i|y = 0}}) = \ln \prod\limits _{i = 1}^m {p(y = {y^i},x = {x^i})}  = \ln \prod\limits _{i = 1}^m {p({x^i}|{y^i})p({y^i})}$$
$$\ell ({\phi _y},{\phi _{i|y = 1}},{\phi _{i|y = 0}}) = \ln \prod\limits _{i = 1}^m {[[1\{ {y^i} = 1\} \ln ((\prod\limits _{j = 1}^n {p(x _j^i|{y^i})p({y^i} = 1)} ){\phi _y})][1\{ {y^i} = 1\} \ln ((\prod\limits _{j = 1}^n {p(x _j^i|{y^i})p({y^i} = 0)} )(1 - {\phi _y}))]} ]$$
$$\ell ({\phi _y},{\phi _{i|y = 1}},{\phi _{i|y = 0}}) = \sum\limits _{i = 1}^m {[[1\{ {y^i} = 1\} (\ln {\phi _y} + \ln \prod\limits _{j = 1}^n {({{({\phi _{j|y = 1}})}^{1\{ {x _j} = 1\} }}{{(1 - {\phi _{j|y = 1}})}^{1\{ {x _j} = 0\} }})} )][1\{ {y^i} = 0\} (\ln (1 - {\phi _y}) + \ln \prod\limits _{j = 1}^n {({{({\phi _{j|y = 0}})}^{1\{ {x _j} = 1\} }}{{(1 - {\phi _{j|y = 0}})}^{1\{ {x _j} = 0\} }})} )]]}$$
$$\ell ({\phi _y},{\phi _{i|y = 1}},{\phi _{i|y = 0}}) = \sum\limits  _{i = 1}^m {[[1\{ {y^i} = 1\} (\ln {\phi _y} + \sum\limits _{j = 1}^n {(1\{ x _j^i = 1\} \ln {\phi _{j|y = 1}} + 1\{ x _j^i = 0\} \ln (1 - {\phi _{j|y = 1}}))} )][1\{ {y^i} = 0\} (\ln (1 - {\phi _y}) + \sum\limits _{j = 1}^n {(1\{ x _j^i = 1\} \ln {\phi _{j|y = 0}} + 1\{ x _j^i = 0\} \ln (1 - {\phi _{j|y = 0}}))} )]]}$$


<p>然后我们对\(\phi _y\)求偏导数：</p>

$$\frac{{\partial \ell ({\phi _y},{\phi _{i|y = 1}},{\phi _{i|y = 0}})}}{\partial {\phi _y}} = \sum\limits _{i = 1}^m {(1\{ {y^i} = 1\} \frac{1}{\phi _y} + 1\{ {y^i} = 0\} \frac{ - 1}{1 - {\phi _y}})}  = \sum\limits _{i = 1}^m {\frac{1\{ {y^i} = 1\}  - {\phi _y}}{\phi _y}(1 - {\phi _y})} = 0$$


<p>所以有:</p>

$${\phi _y} = \frac{1}{m}\sum\limits _{i = 1}^m {1\{ {y^i} = 1\} }$$


<p>然后继续求偏导数:</p>

$$\frac{{\partial \ell ({\phi _y},{\phi _{i|y = 1}},{\phi _{i|y = 0}})}}{{\partial {\phi _{i|y = 1}}}} = \sum\limits _{i = 1}^m {1\{ {y^i} = 1\} (1\{ x _j^i = 1\} \frac{1}{{{\phi _{x _j^i = 1|y = 1}}}} + 1\{ x _j^i = 0\} \frac{{ - 1}}{{1 - {\phi _{x _j^i = 1|y = 1}}}})}=0$$

$$\frac{{\partial \ell ({\phi _y},{\phi _{i|y = 1}},{\phi _{i|y = 0}})}}{{\partial {\phi _{i|y = 1}}}} = \sum\limits _{i = 1}^m {1\{ {y^i} = 1\} (\frac{{1\{ x _j^i = 1\}  - {\phi _{i|y = 1}}}}{{{\phi _{i|y = 1}}(1 - {\phi _{i|y = 1}})}})}=0$$


<p>如果一个单词不再训练样本中，就会出现0&#x2F;0的现象。避免这个事件发生，引入拉普拉斯平滑。所以有:</p>

$${\phi _{i|y = 1}} = \frac{{\sum\limits _{i = 1}^m {1\{ x _j^i = 1,{y^i} = 1\} }  + 1}}{{\sum\limits _{i = 1}^m {1\{ {y^i} = 1\} }  + 2}}$$


<p>同理有:</p>

$${\phi _{i|y = 0}} = \frac{{\sum\limits _{i = 1}^m {1\{ x _j^i = 1,{y^i} = 0\} }  + 1}}{{\sum\limits _{i = 1}^m {1\{ {y^i} = 1\} }  + 2}}$$


<p>注:试分析这样的公式，\(\phi _y\)就是总的垃圾邮件数目除以从样本数。\({\phi _{i|y &#x3D; 1}}\)就是所有垃圾邮件中出现\(x _j^i\)对应的单词的数目除以所有垃圾邮件的数目。实际上这是个很容理解的道理。当然这就是数学的魅力，即便很简单的公司都是有着其理论依据的。</p>
<h3 id="1-3-算法实现"><a href="#1-3-算法实现" class="headerlink" title="1.3 算法实现"></a>1.3 算法实现</h3><p>我们使用样本得到各个单词的概率分布，然后预测邮件是否为垃圾邮件。运行下面的程序，可以得到了正确的垃圾邮件分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">global</span> DEBUG</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">constructDic</span>():</span><br><span class="line">  file = <span class="built_in">open</span>(<span class="string">&quot;../res/NaiveBayes/dict.txt&quot;</span>)</span><br><span class="line">  <span class="comment"># key is the word, value is the sequence number</span></span><br><span class="line">  <span class="built_in">dict</span> = &#123;&#125;           <span class="comment"># In fact, treeMap is better than dict</span></span><br><span class="line">  count = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">    line = file.readline()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> line.split():</span><br><span class="line">      <span class="keyword">if</span> word.lower() <span class="keyword">not</span> <span class="keyword">in</span> <span class="built_in">dict</span>:</span><br><span class="line">        <span class="built_in">dict</span>[word.lower()]=count</span><br><span class="line">        count = count+<span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">dict</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">findIndexInDict</span>(<span class="params">word,<span class="built_in">dict</span></span>):</span><br><span class="line">  <span class="keyword">if</span> word <span class="keyword">in</span> <span class="built_in">dict</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>[word]</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parseSpamOrHam</span>(<span class="params">words</span>):</span><br><span class="line">  <span class="keyword">if</span> words[<span class="number">0</span>] == <span class="string">&quot;spam&quot;</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">  <span class="keyword">elif</span> words[<span class="number">0</span>] == <span class="string">&quot;ham&quot;</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">if</span> DEBUG:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&quot;error email type in training sample!&quot;</span>)</span><br><span class="line">  <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">splitEmail</span>(<span class="params">line</span>):</span><br><span class="line">  regEx = re.<span class="built_in">compile</span>(<span class="string">r&#x27;[^a-zA-Z]|\d&#x27;</span>)</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> word: word!=<span class="string">&quot;&quot;</span>, regEx.split(line)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># spam is 1, ham is 0</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parseEmails</span>(<span class="params">x_arr,y_arr,<span class="built_in">dict</span>,dictLen</span>):</span><br><span class="line">  file = <span class="built_in">open</span>(<span class="string">&quot;../res/NaiveBayes/emails.txt&quot;</span>)</span><br><span class="line">  <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">    line = file.readline()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">    words = splitEmail(line)</span><br><span class="line">    y = parseSpamOrHam(words)</span><br><span class="line">    x = np.zeros(dictLen)</span><br><span class="line">    <span class="keyword">if</span> y == -<span class="number">1</span>:</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words[<span class="number">1</span>:]:</span><br><span class="line">      <span class="keyword">if</span> word.lower() <span class="keyword">in</span> <span class="built_in">dict</span>:</span><br><span class="line">        index = <span class="built_in">dict</span>[word.lower()]</span><br><span class="line">        x[index]=<span class="number">1</span></span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> DEBUG:</span><br><span class="line">          <span class="built_in">print</span>(word,<span class="string">&quot; is not in dic!&quot;</span>)</span><br><span class="line">    x_arr.append(x)</span><br><span class="line">    y_arr.append(y)</span><br><span class="line">    <span class="comment">#if DE<span class="doctag">BUG:</span></span></span><br><span class="line">    <span class="comment"># for i in range(len(x)):</span></span><br><span class="line">    <span class="comment">#  if x[i] == 1:</span></span><br><span class="line">    <span class="comment">#    print(i)</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">dict</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calPhiY</span>(<span class="params">y_arr</span>):</span><br><span class="line">  <span class="comment"># p(y=1)</span></span><br><span class="line">  <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> y_arr:</span><br><span class="line">    <span class="built_in">sum</span> = <span class="built_in">sum</span> + i</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">sum</span>/<span class="built_in">len</span>(y_arr)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calPhiXY</span>(<span class="params">y_arr,x_arr,knownY,dictLen</span>):</span><br><span class="line">  <span class="comment"># return a vector</span></span><br><span class="line">  <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">  ret = np.zeros(dictLen)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(y_arr)):</span><br><span class="line">    <span class="keyword">if</span> y_arr[i]!=knownY:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    <span class="built_in">sum</span>=<span class="built_in">sum</span>+<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(dictLen):</span><br><span class="line">      <span class="keyword">if</span> x_arr[i][j]==<span class="number">1</span>:</span><br><span class="line">       ret[j]=ret[j]+<span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> (ret+<span class="number">1</span>)/(<span class="built_in">sum</span>+<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">classify</span>(<span class="params">phi,phi_y0,phi_y1,dictLen</span>):</span><br><span class="line">  file = <span class="built_in">open</span>(<span class="string">&quot;../res/NaiveBayes/testEmails.txt&quot;</span>)</span><br><span class="line">  <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">    line = file.readline()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">    words = splitEmail(line)[<span class="number">1</span>:]</span><br><span class="line">    x = np.zeros(dictLen)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">      <span class="keyword">if</span> word.lower() <span class="keyword">in</span> <span class="built_in">dict</span>:</span><br><span class="line">        index = <span class="built_in">dict</span>[word.lower()]</span><br><span class="line">        x[index]=<span class="number">1</span></span><br><span class="line">    res = calcPro(phi,phi_y1,x)/(calcPro(phi,phi_y1,x)+calcPro(phi,phi_y0,x))</span><br><span class="line">    <span class="keyword">if</span> res &gt; <span class="number">0.5</span>:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&quot;spam :&quot;</span> + line)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&quot;ham :&quot;</span> + line)</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">dict</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcPro</span>(<span class="params">phi,phi_y,x</span>):</span><br><span class="line">  <span class="comment"># p(x|y=phi_y)</span></span><br><span class="line">  ret = <span class="number">1</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)):</span><br><span class="line">    <span class="keyword">if</span> x[i] == <span class="number">1</span>:</span><br><span class="line">      ret = ret * phi_y[i]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      ret = ret * (<span class="number">1</span>-phi_y[i])</span><br><span class="line">  <span class="keyword">return</span> ret*phi</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">  <span class="comment"># 0. debug options</span></span><br><span class="line">  DEBUG = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 1. construct Dict</span></span><br><span class="line">  <span class="built_in">dict</span> = constructDic()</span><br><span class="line">  dictLen = <span class="built_in">len</span>(<span class="built_in">dict</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 2. parse the email to generate the sample, x and y</span></span><br><span class="line">  x_arr=[]</span><br><span class="line">  y_arr=[]</span><br><span class="line">  parseEmails(x_arr,y_arr,<span class="built_in">dict</span>,dictLen)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 3. learn from the sample</span></span><br><span class="line">  phi = calPhiY(y_arr)            <span class="comment"># p(y)=1</span></span><br><span class="line">  phi_y1 = calPhiXY(y_arr, x_arr, <span class="number">1</span>, dictLen)</span><br><span class="line">  phi_y0 = calPhiXY(y_arr, x_arr, <span class="number">0</span>, dictLen)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 4. test classify</span></span><br><span class="line">  <span class="comment">#p(y=1|x)=p(x|y=1)p(y=1)/(p(x|y=1)p(y=1)+p(x|y=0)p(y=0))</span></span><br><span class="line">  <span class="comment">#p(x|y=1) = Pe(p(x=x^i|y=1))</span></span><br><span class="line">  classify(phi,phi_y0,phi_y1,dictLen)</span><br></pre></td></tr></table></figure>

<h2 id="2-多元伯努利事件模型"><a href="#2-多元伯努利事件模型" class="headerlink" title="2 多元伯努利事件模型"></a>2 多元伯努利事件模型</h2><h3 id="2-1-公式推导"><a href="#2-1-公式推导" class="headerlink" title="2.1 公式推导"></a>2.1 公式推导</h3><p>下面采用另外一种方式进行建模。对于\({x^T} &#x3D; [{x _1},{x _2},…,{x _n}]\)，其中\({x _1} &#x3D; 1\)代表邮件的第一个单词在词库中的索引为1。仍然有如下公式：</p>

$$p({x _1},{x _2},...,{x _i}|y) = \prod\limits _{i = 1}^n {p({x _i}|y)}$$


<p>我们设置\({\phi _y} &#x3D; p(y &#x3D; 1)\)，\({\phi _{i &#x3D; k|y &#x3D; 1}} &#x3D; p({x _i} &#x3D; k|y &#x3D; 1)\)。</p>
<p>然后求最大释然函数：</p>

$$\ell ({\phi _y},{\phi _{i = k|y = 1}},{\phi _{i = k|y = 0}}) = \ln \prod\limits _{i = 1}^m {p({y^i}|{x^i})}  = \ln \prod\limits _{i = 1}^m {p({x^i}|{y^i})p({y^i})}$$
$$\ell ({\phi _y},{\phi _{i = k|y = 1}},{\phi _{i = k|y = 0}}) = \ln \prod\limits _{i = 1}^m {{{(p({x^i}|{y^i} = 1){\phi _y})}^{1\{ {y^i} = 1\} }}p({x^i}|{y^i} = 0)(1 - {\phi _y}){)^{1\{ {y^i} = 0\} }})}$$
$$\ell ({\phi _y},{\phi _{i = k|y = 1}},{\phi _{i = k|y = 0}}) = \ln \prod\limits _{i = 1}^m {{{((\ln (\prod\limits _{j = 1}^n {p(} x _j^i|{y^i} = 1)){\phi _y})}^{1\{ {y^i} = 1\} }}(\ln (\prod\limits _{j = 1}^n {p(} x _j^i|{y^i} = 0))(1 - {\phi _y}){)^{1\{ {y^i} = 0\} }})}$$
$$\ell ({\phi _y},{\phi _{i = k|y = 1}},{\phi _{i = k|y = 0}}) = \sum\limits _{i = 1}^m {[1\{ {y^i} = 1\} (\ln {\phi _y} + \sum\limits _{j = 1}^n {\ln p(x _j^i|{y^i} = 1)} ) + 1\{ {y^i} = 0\} (\ln (1 - {\phi _y}) + \sum\limits _{j = 1}^n {\ln p(x _j^i|{y^i} = 0)} )]}$$


<p>我们对\({\phi _y}\)求偏导数，与之前相同，这里直接写出：</p>

$${\phi _y} = \frac{1}{m}\sum\limits _{i = 1}^m {1\{ {y^i} = 1\} }$$


<p>我们对\({\phi _{i &#x3D; k|y &#x3D; 0}}\)求偏导数，如下：</p>

$$\frac{{\partial \ell ({\phi _y},{\phi _{i = k|y = 1}},{\phi _{i = k|y = 0}})}}{{\partial {\phi _{i = k|y = 1}}}} = \frac{{\partial \sum\limits _{i = 1}^m {1\{ {y^i} = 1\} \sum\limits _{j = 1}^n {\ln p({x^i}|{y^i} = 1)} } }}{{\partial {\phi _{i|y = 1}}}}$$


<p>其中有：</p>

$$\ln p({x^i}|{y^i} = 1) = \ln [p{(x _j^i = k|{y^i} = 1)^{1\{ x _j^i = k\} }}p{(x _j^i \ne k|{y^i} = 1)^{1\{ x _j^i \ne k\} }}]$$
$$\ln p({x^i}|{y^i} = 1) = 1\{ x _j^i = k\} \ln {\phi _{i = k|y = 1}} + (1 - 1\{ x _j^i = k\} )\ln (1 - {\phi _{i = k|y = 1}})$$


<p>所以有：</p>

$$\frac{{\partial \ell ({\phi _y},{\phi _{i = k|y = 1}},{\phi _{i = k|y = 0}})}}{{\partial {\phi _{i = k|y = 1}}}} = \sum\limits _{i = 1}^m {1\{ {y^i} = 1\} \sum\limits _{j = 1}^n {\frac{{1\{ x _j^i = k\}  - {\phi _{i = k|y = 1}}}}{{{\phi _{i = k|y = 1}}(1 - {\phi _{i = k|y = 1}})}}} }  = 0$$


<p>根据拉普拉斯平滑，所以最后得到下面的公式，其中V为词库中单词的数目。</p>

$${\phi _{i = k|y = 1}} = \frac{{\sum\limits _{i = 1}^m {\sum\limits _{j = 1}^n {1\{ {y^i} = 1,x _j^i = k\} } }  + 1}}{{n\sum\limits _{i = 1}^m {1\{ {y^i} = 1\} }  + V}}$$


<p>同理有:</p>

$${\phi _{i = k|y = 0}} = \frac{{\sum\limits _{i = 1}^m {\sum\limits _{j = 1}^n {1\{ {y^i} = 0,x _j^i = k\} } }  + 1}}{{n\sum\limits _{i = 1}^m {1\{ {y^i} = 0\} }  + V}}$$


<h3 id="2-2-算法的实现"><a href="#2-2-算法的实现" class="headerlink" title="2.2 算法的实现"></a>2.2 算法的实现</h3><p>我们使用样本得到各个单词的概率分布，然后预测邮件是否为垃圾邮件。运行下面的程序，可以得到了正确的垃圾邮件分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">global</span> DEBUG</span><br><span class="line"><span class="keyword">global</span> SCALE</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">constructDic</span>():</span><br><span class="line">  file = <span class="built_in">open</span>(<span class="string">&quot;../res/NaiveBayes/dict.txt&quot;</span>)</span><br><span class="line">  <span class="comment"># key is the word, value is the sequence number</span></span><br><span class="line">  <span class="built_in">dict</span> = &#123;&#125;           <span class="comment"># In fact, treeMap is better than dict</span></span><br><span class="line">  count = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">    line = file.readline()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> line.split():</span><br><span class="line">      <span class="keyword">if</span> word.lower() <span class="keyword">not</span> <span class="keyword">in</span> <span class="built_in">dict</span>:</span><br><span class="line">        <span class="built_in">dict</span>[word.lower()]=count</span><br><span class="line">        count = count+<span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">dict</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">findIndexInDict</span>(<span class="params">word,<span class="built_in">dict</span></span>):</span><br><span class="line">  <span class="keyword">if</span> word <span class="keyword">in</span> <span class="built_in">dict</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>[word]</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parseSpamOrHam</span>(<span class="params">words</span>):</span><br><span class="line">  <span class="keyword">if</span> words[<span class="number">0</span>] == <span class="string">&quot;spam&quot;</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">  <span class="keyword">elif</span> words[<span class="number">0</span>] == <span class="string">&quot;ham&quot;</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">if</span> DEBUG:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&quot;error email type in training sample!&quot;</span>)</span><br><span class="line">  <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">splitEmail</span>(<span class="params">line</span>):</span><br><span class="line">  regEx = re.<span class="built_in">compile</span>(<span class="string">r&#x27;[^a-zA-Z]|\d&#x27;</span>)</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> word: word!=<span class="string">&quot;&quot;</span>, regEx.split(line)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># spam is 1, ham is 0</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parseEmails</span>(<span class="params">x_arr,y_arr,<span class="built_in">dict</span>,dictLen</span>):</span><br><span class="line">  file = <span class="built_in">open</span>(<span class="string">&quot;../res/NaiveBayes/emails.txt&quot;</span>)</span><br><span class="line">  <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">    line = file.readline()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">    words = splitEmail(line)</span><br><span class="line">    y = <span class="built_in">int</span>(parseSpamOrHam(words))</span><br><span class="line">    words = words[<span class="number">1</span>:]</span><br><span class="line">    x = np.zeros(<span class="built_in">len</span>(words),<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">if</span> y == -<span class="number">1</span>:</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(words)):</span><br><span class="line">      <span class="keyword">if</span> words[i].lower() <span class="keyword">in</span> <span class="built_in">dict</span>:</span><br><span class="line">        index = <span class="built_in">dict</span>[words[i].lower()]</span><br><span class="line">        x[i]=index</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        x[i]=dictLen+<span class="number">1</span></span><br><span class="line">    x_arr.append(x)</span><br><span class="line">    y_arr.append(y)</span><br><span class="line">    <span class="comment">#if DE<span class="doctag">BUG:</span></span></span><br><span class="line">    <span class="comment"># for i in range(len(x)):</span></span><br><span class="line">    <span class="comment">#    print(x[i])</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">dict</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calPhiY</span>(<span class="params">y_arr</span>):</span><br><span class="line">  <span class="comment"># p(y=1)</span></span><br><span class="line">  <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> y_arr:</span><br><span class="line">    <span class="built_in">sum</span> = <span class="built_in">sum</span> + i</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">sum</span>/<span class="built_in">len</span>(y_arr)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calPhiXY</span>(<span class="params">y_arr,x_arr,knownY,dictLen</span>):</span><br><span class="line">  <span class="comment"># return a vector</span></span><br><span class="line">  <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">  ret = np.zeros(dictLen)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(y_arr)):</span><br><span class="line">    <span class="keyword">if</span> y_arr[i]!=knownY:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    <span class="built_in">sum</span>=<span class="built_in">sum</span>+<span class="built_in">len</span>(x_arr[i])</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_arr[i])):</span><br><span class="line">      index = x_arr[i][j]</span><br><span class="line">      ret[index]=ret[index]+<span class="number">1</span></span><br><span class="line">  <span class="keyword">return</span> SCALE*(ret+<span class="number">1</span>)/(<span class="built_in">sum</span>+dictLen)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">classify</span>(<span class="params">phi,phi_y0,phi_y1,dictLen</span>):</span><br><span class="line">  file = <span class="built_in">open</span>(<span class="string">&quot;../res/NaiveBayes/testEmails.txt&quot;</span>)</span><br><span class="line">  <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">    line = file.readline()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">    words = splitEmail(line)[<span class="number">1</span>:]</span><br><span class="line">    x = np.zeros(<span class="built_in">len</span>(words),<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(words)):</span><br><span class="line">      <span class="keyword">if</span> words[i].lower() <span class="keyword">in</span> <span class="built_in">dict</span>:</span><br><span class="line">        index = <span class="built_in">dict</span>[words[i].lower()]</span><br><span class="line">        x[i]=index</span><br><span class="line">    res = calcPro(phi,phi_y1,x)/(calcPro(phi,phi_y1,x)+calcPro(phi,phi_y0,x))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> res &gt; <span class="number">0.5</span>:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&quot;spam :&quot;</span> + line)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&quot;ham :&quot;</span> + line)</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">dict</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcPro</span>(<span class="params">phi,phi_y,x</span>):</span><br><span class="line">  <span class="comment"># p(x|y=phi_y)</span></span><br><span class="line">  ret = <span class="number">1</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)):</span><br><span class="line">    ret = ret*phi_y[x[i]]</span><br><span class="line">    <span class="comment">#print(ret)</span></span><br><span class="line">  <span class="keyword">return</span> ret*phi</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">  <span class="comment"># 0. debug options</span></span><br><span class="line">  DEBUG = <span class="literal">True</span></span><br><span class="line">  SCALE = <span class="number">10e3</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 1. construct Dict</span></span><br><span class="line">  <span class="built_in">dict</span> = constructDic()</span><br><span class="line">  dictLen = <span class="built_in">len</span>(<span class="built_in">dict</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 2. parse the email to generate the sample, x and y</span></span><br><span class="line">  x_arr=[]</span><br><span class="line">  y_arr=[]</span><br><span class="line">  parseEmails(x_arr,y_arr,<span class="built_in">dict</span>,dictLen)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 3. learn from the sample</span></span><br><span class="line">  phi = calPhiY(y_arr)            <span class="comment"># p(y)=1</span></span><br><span class="line">  phi_y1 = calPhiXY(y_arr, x_arr, <span class="number">1</span>, dictLen)</span><br><span class="line">  phi_y0 = calPhiXY(y_arr, x_arr, <span class="number">0</span>, dictLen)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 4. test classify</span></span><br><span class="line">  <span class="comment">#p(y=1|x)=p(x|y=1)p(y=1)/(p(x|y=1)p(y=1)+p(x|y=0)p(y=0))</span></span><br><span class="line">  <span class="comment">#p(x|y=1) = Pe(p(x=x^i|y=1))</span></span><br><span class="line">  classify(phi,phi_y0,phi_y1,dictLen)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>由于概率值过小，多次乘积会超过浮点数精度范围，所以程序这里乘以一个固定的比例系数</p>
</blockquote>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>手写公式和word版博客地址:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/zhengchenyu/MyBlog/tree/master/doc/mlearning/贝叶斯算法</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2017/08/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-6-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/" data-id="cuidcIB_RMSTeApCb3T37F2Fs" data-title="机器学习-2.6-监督学习之朴素贝叶斯算法" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/09/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-7-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          机器学习-2.7-监督学习之支持向量机
        
      </div>
    </a>
  
  
    <a href="/2017/08/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-5-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%94%9F%E6%88%90%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">机器学习-2.5-监督学习之生成型学习算法</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/02/">February 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/02/07/ErasuceCode%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/12/25/RSS-%E8%BF%9C%E7%A8%8BMerge%E7%9A%84%E8%AE%BE%E8%AE%A1/">RSS-远程Merge的设计</a>
          </li>
        
          <li>
            <a href="/2023/12/25/RSS-%E8%BF%9C%E7%A8%8BMerge%E7%9A%84%E8%AE%BE%E8%AE%A1EN/">RSS-Remote Merge Design</a>
          </li>
        
          <li>
            <a href="/2017/10/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-3-2-%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/">机器学习-3.2-非监督学习之主成分分析.md</a>
          </li>
        
          <li>
            <a href="/2017/10/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-8-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B9%8Bk%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/">机器学习-2.8-监督学习之k近邻算法</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 zhengchenyu<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>